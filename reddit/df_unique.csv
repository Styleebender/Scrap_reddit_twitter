title,author,url,score,num_comments,created_utc,id,selftext,permalink,subreddit,subreddit_name_prefixed,is_video,relevance,category
Open AI announces 'Sora' text to video AI generation,RANDVR,https://www.reddit.com/r/vfx/comments/1arn9t5/open_ai_announces_sora_text_to_video_ai_generation/,854,1145,1708023949.0,1arn9t5,"This is depressing stuff. 

[https://openai.com/sora#capabilities](https://openai.com/sora#capabilities)",/r/vfx/comments/1arn9t5/open_ai_announces_sora_text_to_video_ai_generation/,vfx,r/vfx,False,relevant,less relevant
Testing my new AI text to video tool,enjoytheshowchannel,https://v.redd.it/m3xaij3v68mb1,1,11,1693827641.0,169pr9c,,/r/midjourney/comments/169pr9c/testing_my_new_ai_text_to_video_tool/,midjourney,r/midjourney,True,relevant,neutral
Is there any good text to video AI websites?,MulderCaffrey,https://www.reddit.com/r/ArtificialInteligence/comments/175gaic/is_there_any_good_text_to_video_ai_websites/,1,6,1697036920.0,175gaic,"I tried some text to video sites by AI and most of their stock footage was not all that expansive.

&#x200B;

That or the video turned out to be a picture-slideshow presented as a video. They weren't all that accurate.

&#x200B;

Any suggests? It can be paid as well if there is a trial available for it to make sure my video is correct, I don't mind paying.",/r/ArtificialInteligence/comments/175gaic/is_there_any_good_text_to_video_ai_websites/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
List of AI that creates Text to Video/Image,GdUpFromFeetUp100,https://www.reddit.com/r/aiwars/comments/139wnjy/list_of_ai_that_creates_text_to_videoimage/,4,23,1683394781.0,139wnjy,"Hello, im looking for a List of AI Extensions/Applications that create Text to Video or Picture. 

Im also looking for a AI that creates a Person watching in the Camera and speaking (moving his mouth). Im not looking for an AI that creates a random Person like Synthesis. Im imagine like a Oil Panting from the looks as Example.

&#x200B;

Any Help is highly apprecciated. 

&#x200B;

thanks in advance.",/r/aiwars/comments/139wnjy/list_of_ai_that_creates_text_to_videoimage/,aiwars,r/aiwars,False,relevant,highly relevant
Free text to video AI's?,theywantnone,https://www.reddit.com/r/ArtificialInteligence/comments/12zjv1m/free_text_to_video_ais/,2,6,1682520687.0,12zjv1m,"I'm trying to create an ad for a small online business I've got the script from ChatGPT but am facing lots of issues creating the ad. 
Thanks.",/r/ArtificialInteligence/comments/12zjv1m/free_text_to_video_ais/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
Zeroscope - Another free text to video generator,SrafeZ,https://v.redd.it/hmpfvivplu8b1,179,19,1687997047.0,14lphfh,,/r/singularity/comments/14lphfh/zeroscope_another_free_text_to_video_generator/,singularity,r/singularity,True,relevant,highly relevant
OpenAI’s new text-to-video feature. This video is entirely AI generated.,BeingBestMe,https://v.redd.it/ufcefqmc6uic1,423,59,1708040842.0,1aru3ua,We live in a completely new world as of today.,/r/TikTokCringe/comments/1aru3ua/openais_new_texttovideo_feature_this_video_is/,TikTokCringe,r/TikTokCringe,True,relevant,neutral
"Introducing Sora, our text-to-video model OpenAI - looks amazing!",KIFF_82,https://x.com/openai/status/1758192957386342435?s=46&t=JDB6ZUmAGPPF50J8d77Tog,2217,880,1708020956.0,1arm2g5,,/r/singularity/comments/1arm2g5/introducing_sora_our_texttovideo_model_openai/,singularity,r/singularity,False,not relevant,highly relevant
Mick's Dream - Text To Video,SNCKYYY,https://v.redd.it/cn45nxaahdta1,1927,212,1681270223.0,12j8g2n,,/r/StableDiffusion/comments/12j8g2n/micks_dream_text_to_video/,StableDiffusion,r/StableDiffusion,True,not relevant,highly relevant
"Text to video is here, Hollywood is dead",holy_moley_ravioli_,https://twitter.com/OpenAI/status/1758192957386342435?t=ARwr2R6LzLdUEDcw4wui2Q&s=19,567,415,1708022951.0,1armvd4,,/r/OpenAI/comments/1armvd4/text_to_video_is_here_hollywood_is_dead/,OpenAI,r/OpenAI,False,relevant,less relevant
"OpenAI: ""Introducing Sora, our text-to-video model.""",fde8c75dc6dd8e67d73d,https://twitter.com/openai/status/1758192957386342435,807,176,1708021624.0,1armc92,,/r/StableDiffusion/comments/1armc92/openai_introducing_sora_our_texttovideo_model/,StableDiffusion,r/StableDiffusion,False,relevant,neutral
"OpenAI teases ‘Sora,’ its new text-to-video AI model",Powerpuff_Rangers,https://www.nbcnews.com/tech/tech-news/openai-sora-video-artificial-intelligence-unveiled-rcna139065,640,309,1708108174.0,1asft9a,,/r/news/comments/1asft9a/openai_teases_sora_its_new_texttovideo_ai_model/,news,r/news,False,not relevant,less relevant
Google releases “text to video game” AI model. Is the future generative gaming? :O,marcmar11,https://v.redd.it/8dx7w92oezkc1,793,305,1708975866.0,1b0qbh1,"Google releases “text to video game” AI model. Is the future generative gaming? :O

Text to GTA 7 but in my hometown and I’m the main character.

Text below by Rowan Cheung on [Twitter](https://x.com/rowancheung/status/1762153658307989536?s=46)

“Google DeepMind just dropped 'Genie', an AI that can generate interactive video games.

This is a huge deal.

Genie is trained on 200,000 hours of unsupervised public internet gaming videos and can generate video games from a single prompt or image.

But here's what's insane:

Despite not being trained on action or text annotations, the foundation model can determine who the main character is and enable a user to control that character in the generated world.

It does this through its Latent Action Model, Video Tokenizer, and Dynamics Model (will go more in-depth on this in tomorrow's newsletter for those interested).

And for those asking, yes, it's research-only and not publicly available (here come the Google memes), and it does come with some limitations, like only currently creating games at 1FPS.

But this is the worst AI will ever be.

Anyone will be able to create their own entirely imagined virtual worlds soon, and that's a wild sentence to say out loud.”",/r/ChatGPT/comments/1b0qbh1/google_releases_text_to_video_game_ai_model_is/,ChatGPT,r/ChatGPT,True,relevant,highly relevant
This video was generated by OpenAI’s upcoming text-to-video feature…,Ashamed-Author5980,https://v.redd.it/wtw3glucp2jc1,908,148,1708144090.0,1ast9o8,,/r/Cyberpunk/comments/1ast9o8/this_video_was_generated_by_openais_upcoming/,Cyberpunk,r/Cyberpunk,True,relevant,neutral
John Wick as an Anime created with just Text to Video,Theblasian35,https://v.redd.it/5c4pc4kmfi6c1,1302,77,1702668519.0,18j84ow,,/r/ChatGPT/comments/18j84ow/john_wick_as_an_anime_created_with_just_text_to/,ChatGPT,r/ChatGPT,True,relevant,highly relevant
❗❗OpenAI have announced a revolutionary text-to-video SOTA model that creates video up to 60 seconds,kuzheren,https://v.redd.it/ry0bk7falsic1,1067,167,1708021693.0,1armd71,,/r/aivideo/comments/1armd71/openai_have_announced_a_revolutionary_texttovideo/,aivideo,r/aivideo,True,relevant,highly relevant
OpenAI announces Sora text-to-video model,m3thlol,https://openai.com/sora,60,168,1708022834.0,1armtr3,,/r/aiwars/comments/1armtr3/openai_announces_sora_texttovideo_model/,aiwars,r/aiwars,False,not relevant,neutral
"Grandma, text to video, Modelscope",ZashManson,https://v.redd.it/gmhgnc1hzb6b1,404,93,1686899917.0,14aq16o,,/r/aivideo/comments/14aq16o/grandma_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
"OpenAI announces Sora today, introducing their photorealistic text-to-video product",Gauzey,https://www.reddit.com/r/editors/comments/1arrmbi/openai_announces_sora_today_introducing_their/,142,228,1708034585.0,1arrmbi,"There are some pretty impressive examples in here, but obviously it comes with many concerns with what this means for the industry and the future of the art form in general.

[openai.com/sora](https://openai.com/sora)",/r/editors/comments/1arrmbi/openai_announces_sora_today_introducing_their/,editors,r/editors,False,not relevant,highly relevant
"OpenAI introduces Sora, a text-to-video AI model",LinearArray,https://i.redd.it/ikrgs3emaxic1.png,436,70,1708078603.0,1as59vg,https://openai.com/sora,/r/developersIndia/comments/1as59vg/openai_introduces_sora_a_texttovideo_ai_model/,developersIndia,r/developersIndia,False,not relevant,neutral
Text to video at the cinematic level | The end of Hollywood is getting closer,One_more_human,https://v.redd.it/krgzqn0k7a3c1,570,177,1701262684.0,186oss2,,/r/singularity/comments/186oss2/text_to_video_at_the_cinematic_level_the_end_of/,singularity,r/singularity,True,relevant,neutral
OpenAI Unveils “Sora” Text-To-Video A.I. That Instantly Generates Eye-Popping Videos,Iky_Greenz,https://v.redd.it/1wcyriha5vic1,747,90,1708052590.0,1ary81z,"This video’s A.I. prompt: “Beautiful, snowy Tokyo city is bustling. The camera moves through the bustling city street, following several people enjoying the beautiful snowy weather and shopping at nearby stalls. Gorgeous sakura petals are flying through the wind along with snowflakes.” Video by OpenAI

>OpenAI calls its new system Sora, after the Japanese word for sky. The team behind the technology, including the researchers Tim Brooks and Bill Peebles, chose the name because it “evokes the idea of limitless creative potential.”

>In an interview, they also said the company was not yet releasing Sora to the public because it was still working to understand the system’s dangers. Instead, OpenAI is sharing the technology with a small group of academics and other outside researchers who will “red team” it, a term for looking for ways it can be misused.

Source: [NYTimes Non-Paywall Link](https://archive.ph/1Djss)",/r/ChatGPT/comments/1ary81z/openai_unveils_sora_texttovideo_ai_that_instantly/,ChatGPT,r/ChatGPT,True,relevant,highly relevant
"The Great Catspy, text to video, runway gen-2",ZashManson,https://v.redd.it/241t9oq83lwa1,506,116,1682687573.0,131rdhe,,/r/singularity/comments/131rdhe/the_great_catspy_text_to_video_runway_gen2/,singularity,r/singularity,False,relevant,highly relevant
"Self Combusting Pumpkins, text to video, runway gen-2",ZashManson,https://v.redd.it/gjei5il1z64b1,900,56,1685967528.0,141cqhp,,/r/aivideo/comments/141cqhp/self_combusting_pumpkins_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Youtube casually dropped a text to video AI for shorts,eunumseioquescrever,https://v.redd.it/utv9d4yuqopb1,396,95,1695335566.0,16otl9l,,/r/singularity/comments/16otl9l/youtube_casually_dropped_a_text_to_video_ai_for/,singularity,r/singularity,True,relevant,highly relevant
Titanic: The Ride- Ai generated (text to video),mad_Clockmaker,https://v.redd.it/ymialeq9ij6b1,462,79,1686990969.0,14blo3z,"I played around with trying to get Ai to create video of a Titanic ride, these were the results-
Using Runway Ai",/r/titanic/comments/14blo3z/titanic_the_ride_ai_generated_text_to_video/,titanic,r/titanic,True,relevant,highly relevant
"420sh!t (Palacio), text to video, Modelscope",ZashManson,https://v.redd.it/z136x1hjvy0b1,453,69,1684561962.0,13mjflh,Source ➡️ https://www.youtube.com/@spencersterling1565,/r/aivideo/comments/13mjflh/420sht_palacio_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
Open Source Text to Video is here - Potat1,SrafeZ,https://v.redd.it/osrvgg44kh4b1,568,68,1686095717.0,142xgto,,/r/singularity/comments/142xgto/open_source_text_to_video_is_here_potat1/,singularity,r/singularity,True,relevant,neutral
Open AI introduces Sora - Text to Video Model,3cheers2all,https://www.reddit.com/r/ArtificialInteligence/comments/1aro6ge/open_ai_introduces_sora_text_to_video_model/,89,74,1708026203.0,1aro6ge,"My main driving point with this news is to discuss the impact. I see so many small enterprises/startups will struggle to match up to this in future as AI gets bigger and better, unless they adapt quickly and actually stay ahead of it.

Sora can actually generate 60 seconds video based on the prompt provided. I've been creating motion video clips of 4-6 seconds and clipping them together , then adding music and dialogue on top via editing software to create small videos. However with this new model, movie making (especially for youtubers, small scale videographers) is going to be exciting. 

An evil side would be to discern the reality from fiction. I can already see opinions will be based on fake video since so many are gullible and not willing to go over 10 secs to determine whether a video is real or fake.

Let's see where this world is heading with 3rd decade of this century to be very defining of our future.",/r/ArtificialInteligence/comments/1aro6ge/open_ai_introduces_sora_text_to_video_model/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,less relevant
"Pilates, text to video, runway gen-2",ZashManson,https://v.redd.it/kw3sz1jrgh4b1,393,57,1686094575.0,142x1gx,,/r/aivideo/comments/142x1gx/pilates_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Pizza Nuggets #1, text to video, runway gen-2",ZashManson,https://v.redd.it/omp9l5lz76ya1,642,51,1683343348.0,139aj03,Coming through Twitter https://twitter.com/AILostMedia/,/r/aivideo/comments/139aj03/pizza_nuggets_1_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
All text-to-video examples from the Sora research post in one 10 minute video,Ape_Togetha_Strong,https://v.redd.it/m2y52heyhvic1,449,64,1708057075.0,1arzmt9,,/r/singularity/comments/1arzmt9/all_texttovideo_examples_from_the_sora_research/,singularity,r/singularity,True,relevant,neutral
NVIDIA's Groundbreaking Text-to-Video Model Unveiled: A New Era in AI-Generated Content,KAthePseudonym,https://www.reddit.com/r/singularity/comments/12rifh8/nvidias_groundbreaking_texttovideo_model_unveiled/,376,182,1681882831.0,12rifh8,[https://twitter.com/nonmayorpete/status/1648548301292072962?s=20](https://twitter.com/nonmayorpete/status/1648548301292072962?s=20),/r/singularity/comments/12rifh8/nvidias_groundbreaking_texttovideo_model_unveiled/,singularity,r/singularity,False,not relevant,neutral
Every Sora text-to-video sample in one 14-minute video,Ape_Togetha_Strong,https://v.redd.it/c1bj1u1nysic1,282,46,1708026373.0,1aro8y2,,/r/singularity/comments/1aro8y2/every_sora_texttovideo_sample_in_one_14minute/,singularity,r/singularity,True,relevant,neutral
"New 1.2B parameter text to video model is out, higher quality than modelscope",Illustrious_Row_9971,https://v.redd.it/mrsip3pz53sa1,806,88,1680709316.0,12coe0l,,/r/StableDiffusion/comments/12coe0l/new_12b_parameter_text_to_video_model_is_out/,StableDiffusion,r/StableDiffusion,True,relevant,neutral
Sora vs. other text-to-video AI,G_Romer,https://v.redd.it/ikm0ugvwnikc1,193,25,1708773184.0,1ays587,,/r/OpenAI/comments/1ays587/sora_vs_other_texttovideo_ai/,OpenAI,r/OpenAI,True,not relevant,neutral
"Exploding Watermelons, text to video, runway gen-2",ZashManson,https://v.redd.it/3wnrqdqz2c3b1,358,61,1685593669.0,13x82oj,,/r/aivideo/comments/13x82oj/exploding_watermelons_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
EMU VIDEO- Meta's New Impressive Text-To-video Model,SharpCartographer831,https://v.redd.it/nk175td3tq0c1,335,41,1700154847.0,17ws5q1,,/r/singularity/comments/17ws5q1/emu_video_metas_new_impressive_texttovideo_model/,singularity,r/singularity,True,not relevant,highly relevant
AI Text-to-Video Anime with Pika 1.0,PIKA_LABS,https://www.reddit.com/r/singularity/comments/18ffhb6/ai_texttovideo_anime_with_pika_10/,207,51,1702247942.0,18ffhb6,"Anime generated with Pika 1.0, all text-to-video. \\

Credit:  **Matan Cohen-Grumi, Jessie Ma**

https://reddit.com/link/18ffhb6/video/37n28ervoj5c1/player",/r/singularity/comments/18ffhb6/ai_texttovideo_anime_with_pika_10/,singularity,r/singularity,False,not relevant,highly relevant
Which companies/industries will be hurt/gained by text-to-video Sora?,krisolch,https://www.reddit.com/r/stocks/comments/1asgjf0/which_companiesindustries_will_be_hurtgained_by/,96,82,1708109987.0,1asgjf0,"[https://openai.com/sora](https://openai.com/sora)

Wanted to start a conversation about which companies/industries this is going to hurt the most or obscurely gain from it as well as the future.

Don't say NVDA, that's obvious and probably priced well in by now.

My uneducated opinion on this is that it seems like the following will be hurt:

\- Providing video advertising services to networks will die a terminal death

\- Short-form video developers -> won't be needed

Sectors that could gain:

\- Entertainment producers -> lower cost base to financing entertainment

Later on indie game publishers could also gain if generative AI providers lower development costs of indie games.

Any thoughts?",/r/stocks/comments/1asgjf0/which_companiesindustries_will_be_hurtgained_by/,stocks,r/stocks,False,relevant,highly relevant
Is OpenAI’s Sora (text to video) going to decimate Hollywood?,Tasty-Window,https://www.reddit.com/r/wallstreetbets/comments/1asq4ex/is_openais_sora_text_to_video_going_to_decimate/,53,82,1708134541.0,1asq4ex,"Hollywood has been garbage for years and every celebrity fucking sucks. I hope they all become fucking homeless but am I getting ahead of myself do you think that’ll actually happen? Have you seen the new videos that open AI put out? I think this is incredible and people don’t even realize how powerful this will be yet, what’s a good way to make money off of this it doesn’t seem like something that’s going to happen suddenly and also some of the large studios will be able to take advantage of it but I think it’ll definitely be a consolidation as well as allow for new upstart to be competitive either way The traditional Hollywood business model is gone forever.

https://twitter.com/OpenAI/status/1758192957386342435",/r/wallstreetbets/comments/1asq4ex/is_openais_sora_text_to_video_going_to_decimate/,wallstreetbets,r/wallstreetbets,False,relevant,highly relevant
Text to video progress in one year,pluto_N,https://v.redd.it/5unrljn6xvic1,190,16,1708061976.0,1as14lf,,/r/IndiaTech/comments/1as14lf/text_to_video_progress_in_one_year/,IndiaTech,r/IndiaTech,True,relevant,highly relevant
Chatgpt text to video just got announced. Are you worried?,jsfilmz0412,https://youtu.be/IW-NLYd8z80?feature=shared,0,68,1708030890.0,1arq3lk,,/r/cinematography/comments/1arq3lk/chatgpt_text_to_video_just_got_announced_are_you/,cinematography,r/cinematography,False,relevant,less relevant
This video is generated by OpenAI’s new text-to-video Sora model without modification,Revelnova,https://v.redd.it/fishvf7sosic1,345,54,1708023227.0,1armzb3,,/r/ChatGPT/comments/1armzb3/this_video_is_generated_by_openais_new/,ChatGPT,r/ChatGPT,False,relevant,neutral
"Things are moving way too fast... OpenAI on X: ""Introducing Sora, our text-to-video model. Sora can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions.""",melted-dashboard,https://twitter.com/OpenAI/status/1758192957386342435,1289,594,1708021094.0,1arm4ff,,/r/OpenAI/comments/1arm4ff/things_are_moving_way_too_fast_openai_on_x/,OpenAI,r/OpenAI,False,relevant,highly relevant
"Snowboarding, text to video, runway gen-2",ZashManson,https://v.redd.it/nbhpq8ha574b1,288,39,1685969633.0,141dmum,,/r/aivideo/comments/141dmum/snowboarding_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Testing my new AI text to video tool,enjoytheshowchannel,https://v.redd.it/6lt6xlgr78mb1,174,47,1693827854.0,169ptlm,,/r/singularity/comments/169ptlm/testing_my_new_ai_text_to_video_tool/,singularity,r/singularity,True,relevant,neutral
"World in the year 3023, text to video, runway gen-2",ZashManson,https://v.redd.it/ealn7dxfq0xa1,426,56,1682841063.0,133j1yp,made with runway gen-2 r/aivideo,/r/artificial/comments/133j1yp/world_in_the_year_3023_text_to_video_runway_gen2/,artificial,r/artificial,True,relevant,neutral
Text to video generators by different AI,pluto_N,https://v.redd.it/2qiuzy3zy9kc1,78,19,1708667924.0,1axt8i3,,/r/IndiaTech/comments/1axt8i3/text_to_video_generators_by_different_ai/,IndiaTech,r/IndiaTech,True,relevant,neutral
"Alien High School, text to video, runway gen-2",ZashManson,https://v.redd.it/eaocwgjmvd7b1,364,37,1687358680.0,14f9n44,,/r/aivideo/comments/14f9n44/alien_high_school_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Text to Video (text to video ai, add text to video, ai text to video, how to add text to video, text to video generator, add text to video iphone, text to video ai free, how to add text to video on iphone, ai text to video generator free)",MagicHustler,https://fliki.ai?via=magichustler,3,0,1689840159.0,154kkme,,/r/HustlersHub/comments/154kkme/text_to_video_text_to_video_ai_add_text_to_video/,HustlersHub,r/HustlersHub,False,relevant,highly relevant
"Giant Octopus, text to video, Modelscope",ZashManson,https://v.redd.it/l1xay6s8v98b1,150,40,1687745967.0,14j4hsi,,/r/aivideo/comments/14j4hsi/giant_octopus_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
AI text-to-video: Anime demo,xdlmaoxdxd1,https://v.redd.it/05w965objl5c1,52,28,1702270267.0,18fmm3e,,/r/Asmongold/comments/18fmm3e/ai_texttovideo_anime_demo/,Asmongold,r/Asmongold,True,not relevant,neutral
Midjourney starts training video models. 2024 will be a great year for text-to-video.,Ok-Mess-5085,https://i.redd.it/0zhbqfac1aac1.jpeg,439,33,1704310235.0,18xrq0x,,/r/singularity/comments/18xrq0x/midjourney_starts_training_video_models_2024_will/,singularity,r/singularity,False,relevant,highly relevant
How much of an impact will Sora and other text-to-video models will have on the movie industry,Bulky_Wish_1167,https://www.reddit.com/r/Futurology/comments/1awfp7w/how_much_of_an_impact_will_sora_and_other/,46,145,1708531440.0,1awfp7w,"Do you think that by the end of this decade or the early 2030s, the movie industry will be profoundly at risk? If in 5-10 years everyone could generate films on par with Hollywood movies in terms of quality and whatnot by merely using a text-to-video model such as Sora, that would likely have severe and deadly ramifications for Hollywood. The multi billion dollar movie industry would logically collapse, and movie theaters would become very low in demand, maybe even obsolete. But what do you y’all think?",/r/Futurology/comments/1awfp7w/how_much_of_an_impact_will_sora_and_other/,Futurology,r/Futurology,False,not relevant,less relevant
"Intergalactic Cable TV Signal, text to video, runway gen-2",ZashManson,https://v.redd.it/jyotrwe9e07b1,377,35,1687195456.0,14dkr9a,,/r/aivideo/comments/14dkr9a/intergalactic_cable_tv_signal_text_to_video/,aivideo,r/aivideo,True,relevant,neutral
New Text-to-Video: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,Pro_RazE,https://v.redd.it/npzvbdl5csua1,459,67,1681885695.0,12rjg8b,,/r/singularity/comments/12rjg8b/new_texttovideo_align_your_latents_highresolution/,singularity,r/singularity,True,not relevant,neutral
"Pika 1.0 Text-to-Video Tutorial, access rolling out!",PIKA_LABS,https://www.reddit.com/r/singularity/comments/18g3o2b/pika_10_texttovideo_tutorial_access_rolling_out/,126,32,1702327405.0,18g3o2b,"We’re beginning to let people in off the waitlist, and want to welcome our first Pika 1.0 users!     
Here’s how to start creating with text-to-video.  Sign up at [http://pika.art](https://t.co/nqzjGy82Lx)   


https://reddit.com/link/18g3o2b/video/3tqhoysb9q5c1/player",/r/singularity/comments/18g3o2b/pika_10_texttovideo_tutorial_access_rolling_out/,singularity,r/singularity,False,relevant,highly relevant
NEW Text to Video Model is out with NO watermark,Illustrious_Row_9971,https://i.redd.it/r18yygfnboua1.gif,360,39,1681837102.0,12qw7a8,,/r/StableDiffusion/comments/12qw7a8/new_text_to_video_model_is_out_with_no_watermark/,StableDiffusion,r/StableDiffusion,False,relevant,less relevant
"Spaghetti Wrestling, text to video, runway gen-2",ZashManson,https://v.redd.it/zv5e686zze1b1,290,28,1684757167.0,13op0gg,Source ➡️ https://www.youtube.com/@wintergardenaivideos More at https://www.wintergarden.ai/ and https://www.facebook.com/WinterGardenAI,/r/aivideo/comments/13op0gg/spaghetti_wrestling_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"This Week in AI (4/22/23): AI music bans, GDPR woes, and Nvidia’s amazing new text-to-video",ShotgunProxy,https://www.reddit.com/r/ChatGPT/comments/12v5g9t/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,764,73,1682168894.0,12v5g9t,"I combed through 500+ saved tabs on AI this past week to find the top items (below).

Because it’s hard to keep track of why something is important, I’ve added a sub point for each link to highlight its significance. Enjoy with your ☕!

**News to Know (12 Key Developments)**

AI-generated photo wins major photography award, but winner rejects prize \[[Link](https://www.vice.com/en/article/dy3vxy/sony-world-photography-awards-ai-generated?utm_source=artifact&ref=emergentmind)\]

* The winner deliberately submitted an AI-generated piece to make a statement.

Nvidia unveils text-to-video model \[[Link](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)\]

* Please click the link to see it in action. It’s UNREAL and portends how crazy this year will be.

Compliance with GDPR will be difficult for ChatGPT, portending fines and ban \[[Link](https://www.artisana.ai/articles/next-to-impossible-openais-chatgpt-faces-gdpr-compliance-woes)\]

* Numerous legal experts think it will be near impossible for ChatGPT to fully comply with GDPR.

AI-Generated Song Mimicking Drake and The Weeknd Pulled from Streaming Services \[[Link 1](https://www.artisana.ai/articles/ai-generated-song-mimicking-drake-and-the-weeknd-pulled-from-streaming)\], \[[Link 2](https://www.theverge.com/2023/4/19/23689879/ai-drake-song-google-youtube-fair-use)\]

* New details are still emerging here, actually! AI-generated music is raising lots of questions.

Reddit to start charging AI models for access to its archives \[[Link](https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/)\]

* AI models use large bodies of data, and content companies now want to cash in.

StackOverflow jumps on the API charge bandwagon as well \[[Link](https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/)\]

* StackOverflow’s extensive code examples were likely used to train OpenAI’s current models

Stability AI launches their own open-source language model, StableLM \[[Link](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)\]

* Best known for Stable Diffusion, they’re now moving to compete with ChatGPT

Google plans radical changes to their search engine \[[Link](https://www.nytimes.com/2023/04/16/technology/google-search-engine-ai.html)\]

* Google races to play catchup, and the CEO swears they’re moving faster!

New Google DeepMind team formed out of two AI teams \[[Link](https://www.deepmind.com/blog/announcing-google-deepmind)\]

* Two AI teams that formerly bickered are now one unit. Google’s survival is at stake here.

Michael Schumacher’s Family Threatens Suing German Tabloid Over AI-Generated Interview \[[Link](https://www.tech360.tv/schumacher-family-threatens-suing-german-tabloid-ai-generated-interview)\]

* AI-generated content is at the center of numerous legal firestorms. This is just one of them.

Microsoft developers own AI chip as ChatGPT costs OpenAI an estimated $700k per day to run \[[Link](https://www.artisana.ai/articles/microsofts-ai-chip-strategy-reduces-costs-and-nvidia-dependence)\]

* AI is expensive. ChatGPT is expensive. Microsoft is launching their own chip to cut costs.

Employees said Bard was “cringe-worthy,” but Google launched it anyways \[[Link](https://www.bnnbloomberg.ca/google-s-rush-to-win-in-ai-led-to-ethical-lapses-employees-say-1.1909588?ref=emergentmind)\]

* Wonder why Bard disappointed us at launch? It’s because Google didn’t listen to internal warnings.

**Science Experiments and Things to Try**

A beginner’s guide to autonomous agents \[[Link](https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents)**\]**

* What’s the hype around autonomous agents? 100k stars on GitHub makes this one of the fastest-growing software projects, ever. This writeup explains what it does and how you can play with it, right now.

MiniGPT-4 launched, runs on just 12GB memory, and can process images \[[Link](https://minigpt-4.github.io/)**\]**

* Multi-modal models can now run on personal computers. This one can process images like OpenAI’s GPT-4. Insane and a glimpse of the AI future.

Things you can do right now with AI that you no longer need to pay a marketer for \[[Link](https://twitter.com/thecopyroad/status/1648718891990802435)\]

* Great though-joggers of how marketing is actively transforming now that AI is here. Good for any professional.

Meta open sources their animated drawings AI library \[[Link](https://twitter.com/nonmayorpete/status/1646619389633138688)\]

* Pretty fun to see in action \[an a great example of the weird science coming out of the AI sector these days.

**Notable New Research Papers this Week**

LLMs are learning to program with natural language \[[Link](https://arxiv.org/abs/2304.10464)\]

Analysis of why ChatGPT falls short in comprehension \[[Link](https://t.co/ZunzkW6CYn)\]

Using LLMs to create data lakes \[[Link](http://arxiv.org/abs/2304.09433)\]

Just 51.5% of LLM search engine responses fully supported by citations \[[Link](https://twitter.com/johnjnay)\]

Gisting enables 26x compression of LLM prompts \[[Link](https://arxiv.org/abs/2304.08467)\]

—--

P.S. –  If you’re looking to get a roundup of news and analysis that doesn't appear anywhere else,[ you can read my free newsletter here](https://artisana.beehiiv.com/subscribe).",/r/ChatGPT/comments/12v5g9t/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
"Lava Pool Party, text to video, runway gen-2",ZashManson,https://v.redd.it/6u4z1rcaps5b1,239,34,1686666449.0,148gpby,,/r/aivideo/comments/148gpby/lava_pool_party_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,highly relevant
"Carnival of Ages, text to video, runway gen-2",ZashManson,https://v.redd.it/bk6en5hee7za1,298,31,1683793461.0,13eh1rq,Coming through Twitter https://twitter.com/mr_elkontos/,/r/aivideo/comments/13eh1rq/carnival_of_ages_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
First Open-Source 1024x576 Text To Video Model (potat1) is out!,ptitrainvaloin,https://v.redd.it/3sh9nrqwkh4b1,247,45,1686096052.0,142xl9p,,/r/StableDiffusion/comments/142xl9p/first_opensource_1024x576_text_to_video_model/,StableDiffusion,r/StableDiffusion,True,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1bh5h58/text_to_video_server/,1,0,1710702039.0,1bh5h58,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1bh5h58/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Text to Video AI is crazy ,IntoxicateTCP,https://v.redd.it/ndjgtqi6e6oc1,57,2,1710368032.0,1be4j7b,,/r/shitposting/comments/1be4j7b/text_to_video_ai_is_crazy/,shitposting,r/shitposting,True,relevant,less relevant
"Vin Diesel Fuel, text to video, Modelscope",ZashManson,https://v.redd.it/u784kckmt98b1,157,28,1687745445.0,14j4b9e,,/r/aivideo/comments/14j4b9e/vin_diesel_fuel_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
Add animated text to video,ShunyaEkam_com,https://www.reddit.com/r/davinciresolve/comments/1bl6q46/add_animated_text_to_video/,1,2,1711133105.0,1bl6q46,"I do standup comedy and I wanna add ""viral video""-like effects to my YT Shorts, specifically animated text...

You know, ""real-time"" subtitles. Current word highlighted for proper dopamine stimulation.

How do I do this?",/r/davinciresolve/comments/1bl6q46/add_animated_text_to_video/,davinciresolve,r/davinciresolve,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1bbi0h5/text_to_video_server/,1,0,1710097276.0,1bbi0h5,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1bbi0h5/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
"Kentucky Fried Pigeon, text to video, runway gen-2",ZashManson,https://v.redd.it/5p5x95jlni6b1,237,29,1686980656.0,14bingz,,/r/aivideo/comments/14bingz/kentucky_fried_pigeon_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1b5ovqc/text_to_video_server/,1,0,1709492446.0,1b5ovqc,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1b5ovqc/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Pika 1.0 : text-to-video AI (coming soon),yagami_raito23,https://www.reddit.com/r/singularity/comments/17rw81k/pika_10_texttovideo_ai_coming_soon/,148,24,1699590792.0,17rw81k,"&#x200B;

[The preview looks incredible.](https://reddit.com/link/17rw81k/video/za8i7w2f5gzb1/player)

Source:  https://x.com/pika\_labs/status/1722817227664306636?s=20 ",/r/singularity/comments/17rw81k/pika_10_texttovideo_ai_coming_soon/,singularity,r/singularity,False,not relevant,neutral
Introducing Sora: OpenAI’s newest text to video AI model - What do you think?,lionary,https://v.redd.it/w62tsci18tic1,217,30,1708029311.0,1arpga7,"More examples:

https://openai.com/sora",/r/OpenAI/comments/1arpga7/introducing_sora_openais_newest_text_to_video_ai/,OpenAI,r/OpenAI,True,relevant,neutral
You could evaluate the quality of Open AI text-to-video model solely by the amount buthurtness of those folks 🤣,Present_Dimension464,https://www.reddit.com/gallery/1as1jzq,144,80,1708063464.0,1as1jzq,,/r/DefendingAIArt/comments/1as1jzq/you_could_evaluate_the_quality_of_open_ai/,DefendingAIArt,r/DefendingAIArt,False,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1azvskp/text_to_video_server/,1,0,1708887654.0,1azvskp,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1azvskp/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Text to Video Evolution,FizzyPizzel,https://i.redd.it/ycrx8ibh2tic1.gif,8,0,1708027439.0,1arooo1,CogVideo compared to Sora,/r/OpenAI/comments/1arooo1/text_to_video_evolution/,OpenAI,r/OpenAI,False,relevant,neutral
"Delightfully Plump, text to video, runway gen-2",ZashManson,https://v.redd.it/vmo0622sz87b1,130,33,1687299546.0,14ep6fw,,/r/aivideo/comments/14ep6fw/delightfully_plump_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,highly relevant
Best Text to Video I've seen - Pika Labs,JackieChan1050,https://v.redd.it/5f54qhtcq5bb1,167,31,1689003922.0,14vxnb9,,/r/StableDiffusion/comments/14vxnb9/best_text_to_video_ive_seen_pika_labs/,StableDiffusion,r/StableDiffusion,True,relevant,highly relevant
"Suburbia, text to video, runway gen-2",ZashManson,https://v.redd.it/gn9ohdwuhb6b1,120,29,1686893977.0,14aoa1f,,/r/aivideo/comments/14aoa1f/suburbia_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"""Magic Garden"" my GEN-2 text-to-video Ai generated short",MrLegz,https://v.redd.it/xb2n14g79wva1,244,41,1682369171.0,12xvu17,,/r/StableDiffusion/comments/12xvu17/magic_garden_my_gen2_texttovideo_ai_generated/,StableDiffusion,r/StableDiffusion,True,relevant,neutral
"My Burger TV Ad, text to video, runway",ZashManson,https://v.redd.it/ulzhm3sfobab1,115,33,1688639581.0,14s55rt,,/r/aivideo/comments/14s55rt/my_burger_tv_ad_text_to_video_runway/,aivideo,r/aivideo,True,relevant,neutral
"Demolition Derby, text to video, runway gen-2",ZashManson,https://v.redd.it/grzl06532mza1,227,25,1683970915.0,13gd14m,Coming through YouTube https://www.youtube.com/@wintergardenai,/r/aivideo/comments/13gd14m/demolition_derby_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video AI Generator,lodiboi22,https://www.reddit.com/r/ArtificialInteligence/comments/193jqzv/text_to_video_ai_generator/,6,8,1704922187.0,193jqzv,"Is there a website out there that create ai videos based off text. For example: lion jumps on back of alligator. And it will create a video of exactly that. 

Or a tool where I can upload a video of a lion jumping on an alligator and it creates a similar version but I can also add text to change some things about it like make the lion blue or something.",/r/ArtificialInteligence/comments/193jqzv/text_to_video_ai_generator/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
So let me see if I understand this. OpenAI can now generate text to video. And there isn’t a government that has figured out how to regulate this shit yet?,Kaje26,https://www.reddit.com/r/TooAfraidToAsk/comments/1atiwb4/so_let_me_see_if_i_understand_this_openai_can_now/,566,122,1708221990.0,1atiwb4,,/r/TooAfraidToAsk/comments/1atiwb4/so_let_me_see_if_i_understand_this_openai_can_now/,TooAfraidToAsk,r/TooAfraidToAsk,False,relevant,less relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1au1qmx/text_to_video_server/,1,0,1708282846.0,1au1qmx,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1au1qmx/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Text to Video Creations:,Early_Exit6735,https://www.youtube.com/playlist?list=PL_m2ZzUAXRl_mcA3VBKNwW3VRGatLWd26,1,0,1708358942.0,1auqya4,,/r/AIGeneratedVideos/comments/1auqya4/text_to_video_creations/,AIGeneratedVideos,r/AIGeneratedVideos,False,relevant,highly relevant
VideoPoet Text-to-Video,TheNewBing,https://sites.research.google/videopoet/text-to-video/,1,0,1710498934.0,1bfaw6t,,/r/GoogleVideoPoet/comments/1bfaw6t/videopoet_texttovideo/,GoogleVideoPoet,r/GoogleVideoPoet,False,not relevant,neutral
Nvidia demonstrates high-resolution text-to-video generation with latent diffusion models,Civil_Collection7267,https://research.nvidia.com/labs/toronto-ai/VideoLDM/,228,52,1681881906.0,12ri37n,,/r/StableDiffusion/comments/12ri37n/nvidia_demonstrates_highresolution_texttovideo/,StableDiffusion,r/StableDiffusion,False,not relevant,neutral
Insane new OpenAI model text-to-video: Sora,Moist_Stuff4509,https://v.redd.it/s30nehhxktic1,71,11,1708033693.0,1arr92n,,/r/ChatGPT/comments/1arr92n/insane_new_openai_model_texttovideo_sora/,ChatGPT,r/ChatGPT,True,not relevant,less relevant
Can’t wrap my head around text to video,oberbabo,https://www.reddit.com/r/singularity/comments/1ay8uzu/cant_wrap_my_head_around_text_to_video/,11,16,1708715369.0,1ay8uzu,"If we’re able to publish everything, at any point in time, with the same quality, what good can that do to the internet economy? 

I’m just thinking about alm the dating apps that will be flooded with bullshit images. Doesn’t that actually kill the web?",/r/singularity/comments/1ay8uzu/cant_wrap_my_head_around_text_to_video/,singularity,r/singularity,False,relevant,less relevant
Fixing text to video?,colonelbutt123,https://www.reddit.com/r/videography/comments/1anwrk0/fixing_text_to_video/,1,0,1707616962.0,1anwrk0,"Hey all, I have a handheld shot and I would like to fix the title card so it looks like it was a part of what I was filming and stays fixed with the handheld shake. 

Tips are much appreciated! I’ve tried looking online but struggle a bit",/r/videography/comments/1anwrk0/fixing_text_to_video/,videography,r/videography,False,relevant,highly relevant
"best text to video generator?
",VidyaTheOneAndOnly,https://www.reddit.com/r/ChatGPT/comments/1az80fo/best_text_to_video_generator/,4,1,1708815262.0,1az80fo,"
I Googled and saw there are a number of text to video generators available. Has anyone tried any of them? Which would you say are the best?

 And why are people making such a fuss about Sora if these generators are already available?

I assume Sora looks more realistic but since it isn't available yet, what is the next best one I can use to make short videos?",/r/ChatGPT/comments/1az80fo/best_text_to_video_generator/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
"OpenAI’s Sora text-to-video generator will be publicly available later this year
",rhypple,https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability,102,28,1710346265.0,1bdvbtz,,/r/SoraAi/comments/1bdvbtz/openais_sora_texttovideo_generator_will_be/,SoraAi,r/SoraAi,False,not relevant,neutral
Distributed Training Text to Video,SideMurky8087,https://www.reddit.com/r/StableDiffusion/comments/1b8sm4h/distributed_training_text_to_video/,0,0,1709811722.0,1b8sm4h,"Can we train a sora level open source model with distribution training by distribute parts to community and merge all parts. 

To train a text-to-video model in parts, divide the dataset and distribute it to participants. Each participant trains a local model, then the model parameters are aggregated to create a global model. Repeat this process until the desired performance is achieved, and finally merge the parameters into a single model.


Tto train a text-to-video generation model in parts and then merge those parts into a single model at the end. This approach is known as distributed training or federated learning, where the training process is divided into smaller tasks that can be executed on multiple devices or by multiple participants.

Here's a high-level overview of how you can achieve this:

1. Data Partitioning: Divide your training dataset into smaller subsets that can be distributed to different participants or devices. Ensure that each subset is representative of the overall dataset and contains a diverse range of samples.

2. Model Architecture: Design the architecture of your text-to-video generation model. Determine the number of layers, the size of each layer, and the specific components (e.g., encoder, decoder, attention mechanism) that will be used in the model.

3. Parameter Initialization: Initialize the model parameters (weights and biases) with random values or pre-trained weights. Ensure that all participants start with the same initial parameter values.

4. Distributed Training: Distribute the partitioned dataset and the model architecture to the participants or devices. Each participant will train their local model on their assigned subset of the data using the same training parameters (e.g., learning rate, batch size, number of epochs).

5. Model Synchronization: After each participant completes their local training, collect the updated model parameters from all participants. This can be done by sending the local model updates to a central server or by using a decentralized synchronization mechanism like peer-to-peer communication.

6. Parameter Aggregation: Once all the local model updates are collected, aggregate the parameters to create a single global model. This can be done by averaging the parameters or using more advanced techniques like federated averaging.

7. Repeat: Repeat steps 4-6 for multiple rounds of training until the desired performance or convergence is achieved.

8. Model Merging: After the final round of training, merge the aggregated model parameters into a single model. This merged model will represent the final text-to-video generation model that has been trained collaboratively by multiple participants.

It's important to note that distributed training requires careful coordination and communication between the participants. You'll need to establish a protocol for data sharing, model synchronization, and parameter aggregation. Additionally, privacy and security considerations should be taken into account, especially if sensitive data is involved.

Implementing distributed training can be complex, but there are frameworks and libraries available that can help facilitate the process. Some popular options include TensorFlow Federated, PySyft, and Flower.",/r/StableDiffusion/comments/1b8sm4h/distributed_training_text_to_video/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1aofja6/text_to_video_server/,1,0,1707678047.0,1aofja6,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1aofja6/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
ELI5 How does text to video render so fast?,FreshBlinkOnReddit,https://www.reddit.com/r/explainlikeimfive/comments/1arpxet/eli5_how_does_text_to_video_render_so_fast/,49,12,1708030472.0,1arpxet,I saw an example on Twitter recently of very well done text to video renders that look as good as Pixar animation. But these tools work in minutes apparently. How does it work when Pixar needs supercomputers to render every single frame in their movies and it's very time consuming?,/r/explainlikeimfive/comments/1arpxet/eli5_how_does_text_to_video_render_so_fast/,explainlikeimfive,r/explainlikeimfive,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1aivgmb/text_to_video_server/,1,0,1707073245.0,1aivgmb,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1aivgmb/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
I built a text-to-video clips search engine,deletethistheo,https://v.redd.it/jpkbrysu4nua1,197,33,1681822780.0,12qma35,,/r/Filmmakers/comments/12qma35/i_built_a_texttovideo_clips_search_engine/,Filmmakers,r/Filmmakers,True,not relevant,neutral
(TheTextToVideo.com),InceptionMedia,https://www.reddit.com/r/Domains/comments/1asvfv5/thetexttovideocom/,0,1,1708151656.0,1asvfv5,"How much? As Sora released , plus many ai  video models coming in future ",/r/Domains/comments/1asvfv5/thetexttovideocom/,Domains,r/Domains,False,relevant,neutral
Open AI Sora. How will it affect newly funded text-to-video startups,cutcutnat,https://www.reddit.com/r/ycombinator/comments/1arsgp0/open_ai_sora_how_will_it_affect_newly_funded/,31,31,1708036654.0,1arsgp0,"YC recently funded a couple of text-to-video startups in S23 & W24, and now, Open AI has released Sora, which is mind-blowingly awesome. 

Do you have any ideas on how AI startups can hedge themselves against OAI?",/r/ycombinator/comments/1arsgp0/open_ai_sora_how_will_it_affect_newly_funded/,ycombinator,r/ycombinator,False,not relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1ada33r/text_to_video_server/,1,0,1706468441.0,1ada33r,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1ada33r/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Testing Zeroscope v2 Text-to-Video using vid2vid,3deal,https://v.redd.it/54stx4aszk8b1,122,27,1687880700.0,14kh6xn,,/r/StableDiffusion/comments/14kh6xn/testing_zeroscope_v2_texttovideo_using_vid2vid/,StableDiffusion,r/StableDiffusion,True,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/19cart0/text_to_video_server/,1,0,1705863645.0,19cart0,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/19cart0/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
OpenAl's text-to-video model Sora can generate minute-long videos from a text prompt and it's insane,vsshal7,https://v.redd.it/2qpe2ucywvic1,274,30,1708061900.0,1as13ue,,/r/IndiaTech/comments/1as13ue/openals_texttovideo_model_sora_can_generate/,IndiaTech,r/IndiaTech,True,relevant,less relevant
Text to video?,veleouistcowboy,https://www.reddit.com/r/OpenAI/comments/17xdw6u/text_to_video/,1,4,1700223777.0,17xdw6u,"Hello everyone! I wonder how can I make something like this using prompts in an AI model that creates short animations from text. https://youtu.be/Z8H1_hj9eBc?si=PF5y0YOlro86GF0c
I'm not AI familiar (just some basic ChatGPT only) so a nice step by step on how to, would be wonderful. 
Thank you in advance <3",/r/OpenAI/comments/17xdw6u/text_to_video/,OpenAI,r/OpenAI,False,relevant,highly relevant
"surfs up, poodles ! text to video, Modelscope",dotsimulate,https://v.redd.it/3c8vrjwi4u4b1,230,16,1686248768.0,144gxfg,,/r/aivideo/comments/144gxfg/surfs_up_poodles_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
"Pizzanadu, text to video, runway",ZashManson,https://v.redd.it/l4q4cj8zsa9b1,61,18,1688193170.0,14nnrza,,/r/aivideo/comments/14nnrza/pizzanadu_text_to_video_runway/,aivideo,r/aivideo,True,relevant,neutral
"Golden Years Paradise Retirement Home, text to video, runway gen-2",ZashManson,https://v.redd.it/g2p5m1iadyza1,233,33,1684119970.0,13hvrs7,Coming through YouTube https://www.youtube.com/@wintergardenai,/r/aivideo/comments/13hvrs7/golden_years_paradise_retirement_home_text_to/,aivideo,r/aivideo,True,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/196n94m/text_to_video_server/,1,0,1705258848.0,196n94m,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/196n94m/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
"Dough Daddy's Arcade Pizzeria TV ad, text to video, runway gen-2",ZashManson,https://v.redd.it/1y4erz268h7b1,260,30,1687399208.0,14fq9yc,,/r/aivideo/comments/14fq9yc/dough_daddys_arcade_pizzeria_tv_ad_text_to_video/,aivideo,r/aivideo,True,relevant,neutral
Text to Video using Photo-Realistic Avatars,D3FEATER,https://www.reddit.com/r/MLQuestions/comments/1bl8d20/text_to_video_using_photorealistic_avatars/,1,1,1711137106.0,1bl8d20,"I am trying to create a sports hype-up intro video featuring the team's real players. So, basically, the 60 second video teams play as the players run out on to the field (think fireballs, players standing in an airplane hanger, etc). I have a lot of these videos to use for fine-tuning purposes. As an additional constraint, I need the video to feature the actual players, which I can provide either in various photos or video format.   


In short, the inputs are: text description of the video + high-quality photos of the players. I have a lot of videos to use for fine-tuning. The output is the high-quality photo-realistic video.  


I've studied ML quite a bit (reading basic tensorflow textbooks) but haven't developed in the space since 2020. Currently, it seems like almost any ML need can be met using APIs rather than anything much harder. I'm willing to put something together using APIs or more low-level model work. I am just not sure which route is better given the current state of the space. There is a lot of solutions now for text-to-video, but needing to introduce real people into the video makes it too much of a challenge for the APIs that I have come across. ",/r/MLQuestions/comments/1bl8d20/text_to_video_using_photorealistic_avatars/,MLQuestions,r/MLQuestions,False,relevant,highly relevant
"Canine Nightmare, text to video, runway gen-2",ZashManson,https://v.redd.it/xpp52foei0xa1,163,26,1682838358.0,133ibcn,Coming through Twitter https://twitter.com/thedigiguru,/r/aivideo/comments/133ibcn/canine_nightmare_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Cat Hotel, text to video, runway gen-2",ZashManson,https://v.redd.it/6dy198zpxb2b1,150,23,1685155925.0,13sw3jg,Source ➡️ https://twitter.com/arikuschnir/,/r/aivideo/comments/13sw3jg/cat_hotel_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Deforum Stable DIffusion text to video,TooBeau,https://www.youtube.com/watch?v=XDJI19I40tA,0,0,1709170583.0,1b2nyoq,,/r/StableDiffusion/comments/1b2nyoq/deforum_stable_diffusion_text_to_video/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/190zhou/text_to_video_server/,1,0,1704654050.0,190zhou,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/190zhou/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Any Text to Video Ai app?,Delicious-Result-473,https://www.reddit.com/r/Trollstore/comments/19ehfa9/any_text_to_video_ai_app/,2,4,1706103918.0,19ehfa9,I was wondering is there any working app for converting text to video. Something like invideo but modded so there is no watermark and else...,/r/Trollstore/comments/19ehfa9/any_text_to_video_ai_app/,Trollstore,r/Trollstore,False,relevant,less relevant
OpenAI's Text-to-Video Model looks promising,arnolds112,https://medium.com/@HungryMinded/openai-just-entered-the-text-to-video-game-with-sora-7391cb03e424?sk=8e25c029c575a031fd5431502d6da4f2,85,0,1708035003.0,1arrsmv,,/r/Automate/comments/1arrsmv/openais_texttovideo_model_looks_promising/,Automate,r/Automate,False,not relevant,highly relevant
Adding Text to Video in React?,justaquietboy,https://www.reddit.com/r/ffmpeg/comments/1ap8zaq/adding_text_to_video_in_react/,1,2,1707767214.0,1ap8zaq,"New to ffmpeg, I've been trying to add a simple text to a video in react (using ffmpeg version 4.2.7).  The problem is that the output when downloaded is 0 bytes.  I've been asking chatGPT, googling here, stackoverflow, and ffmpegwasm for solutions, and none of them seem to have the answer.  (I can get it to work as expected on the command line.)  This is what I have:

const filterString = ""drawtext=text='Hello World':x=50:y=50"";

try {  
  const ffmpegResult = await ffmpeg.exec(\[  
//'ffmpeg',  
'-y',  
'-i',  
vidname, //name of file  
'-filter:v', (I've tried -filter\_complex and -vf)  
filterString,  
'-c:v',   
'libx264',  
'-f',   
'mp4',  
'out.mp4'  
\]);",/r/ffmpeg/comments/1ap8zaq/adding_text_to_video_in_react/,ffmpeg,r/ffmpeg,False,relevant,less relevant
Sora AI Review: Text-to-Video Magic,hx_950,https://www.reddit.com/r/reviewxreddit/comments/1bkpv1r/sora_ai_review_texttovideo_magic/,1,0,1711078234.0,1bkpv1r,"OpenAI's Sora is a brand new AI model that lets you create videos from simple text descriptions.  This tech is mind-blowing, but still under development.

https://preview.redd.it/iet28q51hspc1.png?width=1200&format=png&auto=webp&s=3b7cd522f453bff5ea5d57e8c0aba34811def0f4

**Pros:**

* **Hollywood-Ready Videos:** Sora can generate high-fidelity videos with impressive detail and realistic motion, following the rules of physics \[3\]. It's like having your own personal movie studio in a text box!
* **Complex Scene Master:** It's not just for simple stuff. Sora can handle scenes with multiple characters and specific actions \[2\].
* **Customization King:** You're not stuck with basic videos. Sora offers options for video length, resolution, and even adding background music \[3\].

**Cons:**

* **Learning the Ropes:** As a new technology, Sora is still learning. It might struggle with super complicated scenes or hyper-realistic characters \[1\].
* **Ethical Worries:** With great power comes great responsibility! We need to be mindful of how this powerful AI tool can be misused \[2\].
* **Job Market Jitters:** While Sora can open new creative doors, there's a chance it might impact traditional video production jobs \[2\].

&#x200B;

* Text-to-Video generation
* Image-to-Video editing
* Video-to-Video extension
* Adjustable video length, resolution, and audio incorporation

&#x200B;

Sora is impressive for a new model, but results can vary.  It's not perfect yet, but it's constantly learning and improving.

Sora is a glimpse into the future of video creation.  It's powerful, easy to use (hopefully), and has tons of potential.  However, it's still under development and comes with ethical considerations.",/r/reviewxreddit/comments/1bkpv1r/sora_ai_review_texttovideo_magic/,reviewxreddit,r/reviewxreddit,False,relevant,highly relevant
Into the Iris -- Text to Video,I_Only_Like_Giraffes,https://v.redd.it/x4kphvpuovsa1,217,15,1681054619.0,12gmq0x,,/r/deepdream/comments/12gmq0x/into_the_iris_text_to_video/,deepdream,r/deepdream,True,relevant,neutral
help me in adding text to video,R-Educate,https://www.reddit.com/r/OpenShot/comments/1asxdd4/help_me_in_adding_text_to_video/,1,4,1708159205.0,1asxdd4,i am new in this software i could not find where to add text to video or photo,/r/OpenShot/comments/1asxdd4/help_me_in_adding_text_to_video/,OpenShot,r/OpenShot,False,relevant,highly relevant
Google Invests in AI Text-to-Video Startup Runway at $1.5 Billion Valuation,MassiveWasabi,https://www.theinformation.com/articles/google-invests-in-ai-startup-runway-to-wrest-cloud-business-from-aws,268,40,1685592107.0,13x7loo,,/r/singularity/comments/13x7loo/google_invests_in_ai_texttovideo_startup_runway/,singularity,r/singularity,False,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/18vei4g/text_to_video_server/,1,0,1704049244.0,18vei4g,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/18vei4g/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
is capcut text to video valid for monetization?,More_Candidate_7918,https://www.reddit.com/r/NewTubers/comments/1aoyo6s/is_capcut_text_to_video_valid_for_monetization/,0,8,1707740118.0,1aoyo6s,I wanna do  youtube just don't wanna search and pick pieces to match my script is this allowed or is there a better way to do it,/r/NewTubers/comments/1aoyo6s/is_capcut_text_to_video_valid_for_monetization/,NewTubers,r/NewTubers,False,relevant,highly relevant
"$SORA will be everywhere, game-changer for text to video with AI.",Budget-Estimate9660,https://www.reddit.com/r/SolanaMemeCoins/comments/1avmf0f/sora_will_be_everywhere_gamechanger_for_text_to/,12,21,1708447942.0,1avmf0f," 

SORA a text, to video AI generator created by OpenAI carries promise in turning the creative landscape as we know it on its head. As supporters of this technology we have made investments in the $SORA token, fueled by the aspirations it holds for both you and us. Despite the departure of the original token owners where it seems they tried to ""rug"" us, our community has stood firm in preserving its potential.

Become a Soranite;

We have taken charge of shaping the tokens future. Extend an invitation for you to embark on this journey with us. Seize this opportunity to become a part of Solanas ""Token of the People.""

$SORA Official Contract;

6rGaU31jRr5sdSUCVtaxPm3afQ2PXbSj9aYVaGdUerE9

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Our Purpose;

Unitedly we strive to demonstrate the capabilities that SORA unlocks. This AI tool empowers individuals to manifest their ideas and artistic visions. Businesses are harnessing its potential for gain content creators are using it to cultivate their audiences and SORA holds promises, for greater horizons ahead.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

SORA’s roadmap;

We are dedicated, to shaping a future for SORA with AI as its foundation. Here's how we plan to reach our goals;

Phase 1; Launch & Community (Now. 3 months);

Building Trust & Visibility; Make sure SORA is listed on CoinGecko and actively interact with the crypto community.

Growing a Strong Community; Establish a presence on media and forums to nurture connections.

Informative Resources; Introduce an detailed website with roadmaps, tokenomics and technological insights.

Phase 2; Enhancing Utility & Ecosystem (3. 6 months);

Forming Strategic Partnerships; Collaborate with influencers. Integrate SORA into established community projects.

Community Empowerment; Implement a DAO structure to involve the community in decision making processes.

Expanding Reach; Increase marketing efforts through Twitter Spaces, media exposure and regular content creation.

Phase 3; Securing Major Exchange Listings & Growth (6. 12 months);

Enhanced Accessibility; Ensure listings on Tier 2 and Tier 1 exchanges to boost trading activity.

Sustained Engagement; Focus on engaging developers and the community for collaboration.

Advanced Features Development; Create features based on feedback and demand, from the community.

Phase 4; Looking Ahead, to the Future (12+ months);

Keep up the pace; Ensure Tier 1 listings and trading volume remain robust.

Embracing Community Ideas; Discover and put into action ways to use SORA and AI technology.

Striving for Leadership; Aspire to be acknowledged as a figure, in the world of AI Memetokens.

\_\_\_\_\_\_\_\_\_\_\_\_

Enter the Matrix!

Lets work together to unleash the power of SORA and influence the future of creativity driven by AI.

TG: [https://t.me/SOLANASORA](https://t.me/SOLANASORA)

TWITTER: [https://x.com/Solana\_Sora](https://x.com/Solana_Sora)

WEB: [https://sora-sol.ai/](https://sora-sol.ai/)

 ",/r/SolanaMemeCoins/comments/1avmf0f/sora_will_be_everywhere_gamechanger_for_text_to/,SolanaMemeCoins,r/SolanaMemeCoins,False,relevant,highly relevant
Google AI gave 60 Minutes a preview of their text-to-video model .,Neither_Novel_603,https://twitter.com/ai_insight1/status/1647296767539421185?s=20,134,67,1681581379.0,12ncrov,,/r/singularity/comments/12ncrov/google_ai_gave_60_minutes_a_preview_of_their/,singularity,r/singularity,False,not relevant,neutral
Text to Video is here: Sora,rizit98,https://aitools.fyi/sora,1,0,1708028850.0,1arp9b7,"OpenAI announces Sora, their video generation model

The leap in AI-driven creativity just hit warp speed with Sora. This isn't just another step; it's a giant leap. Forget the snippets we're used to; Sora can generate videos up to a full minute long. This is uncharted territory.

Compared to Runway, Pika, and other tools that give us brief flashes of potential, Sora is in a league of its own. We're talking crisp visuals, seamless consistency, and lifelike movement that holds your attention not just for a fleeting moment, but for a whole 60 seconds.

The quality? Unparalleled. It's like stepping from a dark room into broad daylight. This is the future of video content, where limitations are just a word in the history books.

Absolutely amazing.

For more info: https://aitools.fyi/sora",/r/worldofai/comments/1arp9b7/text_to_video_is_here_sora/,worldofai,r/worldofai,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/18q1ggp/text_to_video_server/,1,0,1703444443.0,18q1ggp,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/18q1ggp/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
"Zeroscope Rave, text to video, zeroscope XL",ZashManson,https://v.redd.it/m89form01k9b1,119,16,1688304815.0,14oofy4,,/r/aivideo/comments/14oofy4/zeroscope_rave_text_to_video_zeroscope_xl/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/18koepc/text_to_video_server/,1,0,1702839644.0,18koepc,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/18koepc/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
"2025 Robot Protests, text to video, Modelscope",ZashManson,https://v.redd.it/6gikcbve545b1,117,18,1686369193.0,145pr93,,/r/aivideo/comments/145pr93/2025_robot_protests_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,less relevant
Text to video via AI,retrorooster0,https://www.reddit.com/r/ChatGPT/comments/18btn72/text_to_video_via_ai/,2,4,1701830110.0,18btn72,Does anyone know a service that uses LLM to create and AI generated video based of text ?,/r/ChatGPT/comments/18btn72/text_to_video_via_ai/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
Cam google TPU provide text to video like Sora. ,Sunny-Olaf,https://www.reddit.com/r/NVDA_Stock/comments/1atwa0l/cam_google_tpu_provide_text_to_video_like_sora/,10,7,1708269232.0,1atwa0l,"text to video by Sora is Jaw dropping. Can Google’s ASIC TPU provide the same performance without NVDA H100? My understanding is TPU is only good for machine deep learning, no graphic rendering like H100 generalized GPU?
",/r/NVDA_Stock/comments/1atwa0l/cam_google_tpu_provide_text_to_video_like_sora/,NVDA_Stock,r/NVDA_Stock,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/18fakp0/text_to_video_server/,1,0,1702234853.0,18fakp0,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/18fakp0/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Text to Video in Video Games,Sharp_Chair6368,https://www.reddit.com/r/singularity/comments/16h5rdw/text_to_video_in_video_games/,22,14,1694558844.0,16h5rdw,"What happens when we get indistinguishable text to video and can use something like Director mode in RunwayML to walk through the worlds we’re generating? What happens when you add character controls, etc to this indistinguishable video generation? 

What happens when you’re walking through your own version of Middle Earth that’s a complete replica of the movies/books? 

What happens when it’s a VR experience? We are very close to this.",/r/singularity/comments/16h5rdw/text_to_video_in_video_games/,singularity,r/singularity,False,relevant,highly relevant
Text to video really sucks,Drakhanfeyr,https://www.reddit.com/r/Filmora/comments/18lanv6/text_to_video_really_sucks/,2,2,1702911497.0,18lanv6,"Every time I use text to video, I find that:

* It uses the same music (Cup of Coffee in Big City), irrespective of what the test/video is about.
* If the video exceeds 1 minutes, it just repeats the same music!
* Photos/video clips generated only have a 10% chance of being relevant to the text and useful. 
* The narration is often broken (and a new slide generated) in the middle of a sentence, even when sentences are kept short.

At the moment, [https://ai.invideo.io/](https://ai.invideo.io/) is so much better. I have tried re-writing the text to ensure it is clear, but nothing I do really helps much. I wonder what other people's experiences are, and if there is something I really can do, which I'm missing?

&#x200B;",/r/Filmora/comments/18lanv6/text_to_video_really_sucks/,Filmora,r/Filmora,False,relevant,less relevant
Lumiere AI Text To Video Generation,StartCodeEmAdagio,https://www.youtube.com/watch?v=wxLr02Dz2Sc,1,0,1706810176.0,1agh34z,,/r/TextToVideoGeneration/comments/1agh34z/lumiere_ai_text_to_video_generation/,TextToVideoGeneration,r/TextToVideoGeneration,False,not relevant,neutral
OpenAI Sora - Text to Video,Thiizic,https://twitter.com/openai/status/1758192957386342435?s=46&t=JDB6ZUmAGPPF50J8d77Tog,5,1,1708023820.0,1arn808,,/r/Futurism/comments/1arn808/openai_sora_text_to_video/,Futurism,r/Futurism,False,relevant,neutral
Sora vs. other text-to-video AI,Ok_Huckleberry4418,https://www.instagram.com/reel/C3ukvKZtk0k/,5,0,1708782607.0,1ayuz2i,,/r/soravids/comments/1ayuz2i/sora_vs_other_texttovideo_ai/,soravids,r/soravids,False,not relevant,neutral
Chinese state broadcaster CMG debuts a text-to-video tool.,lughnasadh,https://www.scmp.com/video/china/3253501/chinese-ai-generated-cartoon-series-broadcast-state-television?,20,7,1709160351.0,1b2jzsa,,/r/singularity/comments/1b2jzsa/chinese_state_broadcaster_cmg_debuts_a/,singularity,r/singularity,False,not relevant,neutral
OpenAI's Sora (text-to-video model meme),Delicious_Maize9656,https://i.redd.it/res3mmk74yic1.jpeg,13,3,1708088547.0,1as8251,,/r/sciencememes/comments/1as8251/openais_sora_texttovideo_model_meme/,sciencememes,r/sciencememes,False,not relevant,neutral
SORA AI: text to video Generation !,Abject_Station_5690,https://v.redd.it/bhxb32nwcakc1,1,0,1708672671.0,1axujh0,,/r/SORA_CHATGPT/comments/1axujh0/sora_ai_text_to_video_generation/,SORA_CHATGPT,r/SORA_CHATGPT,True,relevant,neutral
text to video VJ pack,Eastern-Mango-162,https://www.reddit.com/r/vjing/comments/18iajj9/text_to_video_vj_pack/,0,2,1702566130.0,18iajj9," new VJ pack, my 2nd pack with text to video tools  [https://sleeplessmonk.gumroad.com/l/s...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbm92N1d0RjFmY0VHLXVBaHVLanA0RlpGbmNfZ3xBQ3Jtc0tuRkJJRHR2VjRKV1M4RC1ldGVXTXVtWk15cXBnb25TMm1fT0dvQXlGRXM0S3B1Q2pXNlVGZFA1SzlEemZ4WThCOFo0QkZERXEyekVuQ2xWbG0wWXN2S0JBeXV3V3lSSWZZTW1mUnl1QzNDWl9ybXJsMA&q=https%3A%2F%2Fsleeplessmonk.gumroad.com%2Fl%2Fsdtit&v=Lan8vHH4kas) music Sleepless Monk - Monsoon Spores  #vj #vjpack #vjloops #sleeplessmonk #beyondsenses

\#metaverse #generativeartist

\#fractalart #nftart #NFTartist #audiovisual #animation #techno #psytech

https://reddit.com/link/18iajj9/video/8hkrtrz9y96c1/player",/r/vjing/comments/18iajj9/text_to_video_vj_pack/,vjing,r/vjing,False,relevant,neutral
Sora text-to-video generator by OpenAI,Affilate_Campaign,https://www.reddit.com/r/aibuds/comments/1bfimcl/sora_texttovideo_generator_by_openai/,1,0,1710521678.0,1bfimcl,"OpenAI has unveiled its latest breakthrough: Sora. This innovative product utilizes text-to-video technology, empowering users to effortlessly transform textual prompts into captivating visual narratives. Sora breaks down traditional barriers in video production, unlocking a new realm of creativity that was once inaccessible. With its remarkable output length and efficiency, Sora can generate videos within minutes, regardless of prompt complexity.

Sora's Solutions for Industries:

The introduction of Sora offers a plethora of solutions for industries grappling with the demands of visual content creation. Marketers can leverage its power to quickly convert textual concepts into engaging advertisements, bolstering engagement and brand visibility. Educators can benefit from its ability to illustrate complex ideas effortlessly, enriching learning experiences for students worldwide. Furthermore, media and entertainment professionals can expedite production timelines, bringing stories to life in record time.

Additional Insights from OpenAI CTO:

In a recent interview with The Wall Street Journal, OpenAI's Chief Technical Officer, Mira Murati, shed light on various aspects of Sora's development and deployment. Murati emphasized the company's commitment to addressing misinformation and harmful biases, ensuring responsible deployment of AI technologies, particularly in sensitive contexts such as global elections.

The investigation by Italy's data protection watchdog underscores the significance of data ethics in AI development, prompting OpenAI to provide transparency regarding Sora's training data sources. Additionally, Murati highlighted Sora's cost implications, acknowledging its resource-intensive nature compared to other OpenAI tools like ChatGPT and DALL-E.

Looking Ahead:

As OpenAI continues to refine Sora, promising enhancements such as audio capabilities, improved generation times, and cost-effectiveness are on the horizon. The company's dedication to minimizing imperfections underscores its commitment to delivering cutting-edge AI solutions that empower users while prioritizing ethical considerations.

In conclusion, Sora represents a paradigm shift in visual content creation, offering unparalleled efficiency, versatility, and potential for innovation. As it prepares for launch later this year, the anticipation surrounding Sora's impact on industries worldwide is palpable, signaling a new era of creativity fueled by artificial intelligence.",/r/aibuds/comments/1bfimcl/sora_texttovideo_generator_by_openai/,aibuds,r/aibuds,False,relevant,highly relevant
"MTV Cribs, text to video, runway gen-2",ZashManson,https://v.redd.it/3io5vuh7yy7b1,151,15,1687613818.0,14htakq,,/r/aivideo/comments/14htakq/mtv_cribs_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/18a0ghl/text_to_video_server/,1,0,1701630064.0,18a0ghl,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/18a0ghl/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
OpenAI launches text-to-video generator,wjfox2009,https://www.futuretimeline.net/blog/2024/02/17-openai-sora-future-timeline.htm,4,0,1708205328.0,1atcugf,,/r/futuretimeline/comments/1atcugf/openai_launches_texttovideo_generator/,futuretimeline,r/futuretimeline,False,not relevant,neutral
Why did Wombo text to video get disabled?,Left-Plant2717,https://v.redd.it/ydeytp8p3odc1,11,6,1705788065.0,19bn5ox,"
",/r/aivideo/comments/19bn5ox/why_did_wombo_text_to_video_get_disabled/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/184hwyy/text_to_video_server/,1,0,1701025251.0,184hwyy,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/184hwyy/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
OpenAI Launches Text-to-Video Tool,DeepDreamerX,https://www.verity.news/story/2024/openai-launches-texttovideo-tool?p=re2012,1,0,1708163443.0,1asydqc,,/r/tech_news_today/comments/1asydqc/openai_launches_texttovideo_tool/,tech_news_today,r/tech_news_today,False,not relevant,neutral
Sora is coming - text to video,FMACH1,https://x.com/OpenAI/status/1758192957386342435?s=20,1,0,1708035203.0,1arrvjx,,/r/ChatGPTBestGPTs/comments/1arrvjx/sora_is_coming_text_to_video/,ChatGPTBestGPTs,r/ChatGPTBestGPTs,False,relevant,neutral
"Celebrity Breakdance Off, text to video, runway gen-2",ZashManson,https://v.redd.it/ao1056lw074b1,144,20,1685968157.0,141d02y,,/r/aivideo/comments/141d02y/celebrity_breakdance_off_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Coney Island Rock Eating Contest, text to video, runway gen-2",ZashManson,https://v.redd.it/pv8r2b7mbo7b1,129,29,1687485121.0,14glmj2,,/r/aivideo/comments/14glmj2/coney_island_rock_eating_contest_text_to_video/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/17z461b/text_to_video_server/,1,0,1700420456.0,17z461b,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/17z461b/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
How far away is text-to-video,Ok_Stomach3982,https://www.reddit.com/r/MuahAI/comments/180zarm/how_far_away_is_texttovideo/,4,7,1700622441.0,180zarm,"So far you have text-to-speech capabilities and its amazing to hear my ai converse with me.  The details are amazing and you really do get what you pay for.  Is text to video on the cards.  Can we teach it to our companions or will it be automatically uploaded

Keep up the great work Muah.ai it is much appreciated 

Regards a loyal customer 😀",/r/MuahAI/comments/180zarm/how_far_away_is_texttovideo/,MuahAI,r/MuahAI,False,relevant,highly relevant
Text to video AI in the 3D world,Charming-Parfait-141,https://www.reddit.com/r/3Dmodeling/comments/1assqmh/text_to_video_ai_in_the_3d_world/,0,2,1708142400.0,1assqmh,"https://arstechnica.com/information-technology/2024/02/openai-collapses-media-reality-with-sora-a-photorealistic-ai-video-generator/

OpenAI released Sora this week, I heard about but did not gave too much thought. Reading the above news it really impressed me.

From time to time someone posts a question about it and the general feeling is that it will take a long for the AI to reach the level of human creativity, which I agree, now what about combining them?

Seeing this text to video capabilities above and bringing it to the 3D modeling what are your thoughts on how this profession will evolve or, who knows, even be relevant in the future?",/r/3Dmodeling/comments/1assqmh/text_to_video_ai_in_the_3d_world/,3Dmodeling,r/3Dmodeling,False,relevant,highly relevant
SVD - text to video workflow,iahmedabdelnaby,/r/comfyui/comments/195pyai/workflow/,0,0,1705160646.0,195qrzj,,/r/StableDiffusion/comments/195qrzj/svd_text_to_video_workflow/,StableDiffusion,r/StableDiffusion,False,relevant,neutral
Will openai release a text-to-video model?,LeagueOfFusions,https://www.reddit.com/r/ChatGPT/comments/1abjvi3/will_openai_release_a_texttovideo_model/,3,4,1706280117.0,1abjvi3,What do you think? Any information on whether they are working on something like this or not?,/r/ChatGPT/comments/1abjvi3/will_openai_release_a_texttovideo_model/,ChatGPT,r/ChatGPT,False,not relevant,highly relevant
"NYC Rat Documentary, text to video, Modelscope",dotsimulate,https://v.redd.it/c981acf3g85b1,87,16,1686421671.0,1467dwh,,/r/aivideo/comments/1467dwh/nyc_rat_documentary_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/17tqzkm/text_to_video_server/,1,0,1699815661.0,17tqzkm,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/17tqzkm/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Sora alternative - SVD Disturbed training text to video ,SideMurky8087,https://www.reddit.com/r/comfyui/comments/1b8wtnj/sora_alternative_svd_disturbed_training_text_to/,0,2,1709823814.0,1b8wtnj,"Can we train a sora level open source model with distribution training by distribute parts to community and merge all parts. 

To train a text-to-video model in parts, divide the dataset and distribute it to participants. Each participant trains a local model, then the model parameters are aggregated to create a global model. Repeat this process until the desired performance is achieved, and finally merge the parameters into a single model.


Tto train a text-to-video generation model in parts and then merge those parts into a single model at the end. This approach is known as distributed training or federated learning, where the training process is divided into smaller tasks that can be executed on multiple devices or by multiple participants.

Here's a high-level overview of how you can achieve this:

1. Data Partitioning: Divide your training dataset into smaller subsets that can be distributed to different participants or devices. Ensure that each subset is representative of the overall dataset and contains a diverse range of samples.

2. Model Architecture: Design the architecture of your text-to-video generation model. Determine the number of layers, the size of each layer, and the specific components (e.g., encoder, decoder, attention mechanism) that will be used in the model.

3. Parameter Initialization: Initialize the model parameters (weights and biases) with random values or pre-trained weights. Ensure that all participants start with the same initial parameter values.

4. Distributed Training: Distribute the partitioned dataset and the model architecture to the participants or devices. Each participant will train their local model on their assigned subset of the data using the same training parameters (e.g., learning rate, batch size, number of epochs).

5. Model Synchronization: After each participant completes their local training, collect the updated model parameters from all participants. This can be done by sending the local model updates to a central server or by using a decentralized synchronization mechanism like peer-to-peer communication.

6. Parameter Aggregation: Once all the local model updates are collected, aggregate the parameters to create a single global model. This can be done by averaging the parameters or using more advanced techniques like federated averaging.

7. Repeat: Repeat steps 4-6 for multiple rounds of training until the desired performance or convergence is achieved.

8. Model Merging: After the final round of training, merge the aggregated model parameters into a single model. This merged model will represent the final text-to-video generation model that has been trained collaboratively by multiple participants.

It's important to note that distributed training requires careful coordination and communication between the participants. You'll need to establish a protocol for data sharing, model synchronization, and parameter aggregation. Additionally, privacy and security considerations should be taken into account, especially if sensitive data is involved.

Implementing distributed training can be complex, but there are frameworks and libraries available that can help facilitate the process. Some popular options include TensorFlow Federated, PySyft, and Flower.",/r/comfyui/comments/1b8wtnj/sora_alternative_svd_disturbed_training_text_to/,comfyui,r/comfyui,False,relevant,highly relevant
"Defunct Theme Park, text to video, runway gen-2",ZashManson,https://v.redd.it/vp71kp6dpp2b1,162,17,1685322624.0,13ugr6n,,/r/aivideo/comments/13ugr6n/defunct_theme_park_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video AI breakthrough! (Open AI Sora),kvbrd_YT,https://twitter.com/OpenAI/status/1758192957386342435?t=yEYK9HsoD7N8G8GZd560SQ&s=19,1,1,1708032600.0,1arqso7,,/r/Asmongold/comments/1arqso7/text_to_video_ai_breakthrough_open_ai_sora/,Asmongold,r/Asmongold,False,relevant,neutral
Concept for a future text to video project,takoda5164,https://i.redd.it/jglnstazkfkc1.png,1,0,1708735955.0,1ayh6o0,,/r/midjourney/comments/1ayh6o0/concept_for_a_future_text_to_video_project/,midjourney,r/midjourney,False,relevant,neutral
Sora: A powerful text-to-video AI model,finphil,https://medium.com/@philsiarri/sora-a-powerful-text-to-video-ai-model-ef68387b6ce8,2,0,1708622390.0,1axbxsf,,/r/AILinksandTools/comments/1axbxsf/sora_a_powerful_texttovideo_ai_model/,AILinksandTools,r/AILinksandTools,False,not relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/17oiynm/text_to_video_server/,1,0,1699210858.0,17oiynm,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/17oiynm/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Subaru BRZ Text To Video,Bloop5000,https://v.redd.it/q6csesm9lx4c1,0,0,1701980340.0,18d4ifg,,/r/subaru/comments/18d4ifg/subaru_brz_text_to_video/,subaru,r/subaru,True,not relevant,neutral
Is there a production ready text to video API?,kagan101,https://www.reddit.com/r/StableDiffusion/comments/1bfl40z/is_there_a_production_ready_text_to_video_api/,0,0,1710528006.0,1bfl40z,"Im looking to build an app which requires high amount of text to image or image to image videos on scale. 

Ideally in the same price with bulding a custom backend on a cloud service. 

Do you know any easy to use API like this?",/r/StableDiffusion/comments/1bfl40z/is_there_a_production_ready_text_to_video_api/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"Wrestling 1985, text to video, runway gen-2",ZashManson,https://v.redd.it/20rrxe89646b1,104,17,1686805331.0,149t9zm,,/r/aivideo/comments/149t9zm/wrestling_1985_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
This text-to-video AI engine 'Sora' is actually pretty amazing,space_monster,https://youtu.be/tRSdt5kmeW0?feature=shared,0,11,1708028418.0,1arp312,,/r/videos/comments/1arp312/this_texttovideo_ai_engine_sora_is_actually/,videos,r/videos,False,not relevant,highly relevant
Text to video,Minimum-Flight3129,https://www.reddit.com/r/StableDiffusion/comments/16oh1vq/text_to_video/,1,1,1695305724.0,16oh1vq,"Hi guys, where do I start with text to video?",/r/StableDiffusion/comments/16oh1vq/text_to_video/,StableDiffusion,r/StableDiffusion,False,relevant,neutral
"Human Music TV ad 1994, text to video, runway gen-2",ZashManson,https://v.redd.it/bglxlx79gj3b1,200,20,1685682768.0,13y3puh,Source ➡️ https://m.youtube.com/@AILostMedia,/r/aivideo/comments/13y3puh/human_music_tv_ad_1994_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Wake and Bake, text to video, runway gen-2",ZashManson,https://v.redd.it/b83ptuhpdi0b1,135,20,1684362241.0,13kg63p,Coming through Twitter https://twitter.com/thedigiguru/,/r/aivideo/comments/13kg63p/wake_and_bake_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Sora AI Text To Video Generator Examples (Collection),iamfaijan,https://www.theinsaneapp.com/2024/02/sora-ai-video-generator-demos.html,5,0,1708030588.0,1arpz3j,,/r/learnmachinelearning/comments/1arpz3j/sora_ai_text_to_video_generator_examples/,learnmachinelearning,r/learnmachinelearning,False,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/17j9wsj/text_to_video_server/,1,0,1698606064.0,17j9wsj,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/17j9wsj/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Sora: Text to video model by OpenAI,TheAIJargon,https://v.redd.it/avzbdkj1f4jc1,1,0,1708164832.0,1asyqk1,"OpenAI just released its video generation model called Sora and it's amazing! It can generate videos from text, images and videos. It creates realistic and imaginative scenes.

It can generate complex scenes with multiple characters, specific types of motion, accurate details of the subject and background and various different types of videography. 

Read the full review on [theaijargon.com](https://theaijargon.com/what-is-openai-sora-ai/)",/r/TheAIJargon/comments/1asyqk1/sora_text_to_video_model_by_openai/,TheAIJargon,r/TheAIJargon,True,relevant,highly relevant
OpenAI crea una IA (Text to video),Wikitechofficial,https://www.reddit.com/r/WikiTechOfficial/comments/1asz5pa/openai_crea_una_ia_text_to_video/,1,0,1708166517.0,1asz5pa," Con **Sora di OpenAI**, dai vita alle tue idee con il potere del testo! Immagina di scrivere una semplice frase e vederla trasformarsi in un video coinvolgente, con personaggi realistici e ambientazioni mozzafiato. Che tu sia un artista, uno sviluppatore o un semplice appassionato di creatività, Sora ti permette di:

&#x200B;

* **Trasformare le tue storie in video:** Racconta le tue avventure, i tuoi sogni e le tue idee in un modo completamente nuovo.
* **Crea contenuti coinvolgenti:** Realizza video esplicativi, tutorial, presentazioni e animazioni che stupiranno il tuo pubblico.
* **Prototipa in modo rapido e semplice:** Sperimenta nuove idee per videogiochi, film, cortometraggi o qualsiasi altra forma di narrazione visiva.

&#x200B;

[https://www.wiki-tech.it/sora-di-openai/](https://www.wiki-tech.it/sora-di-openai/)",/r/WikiTechOfficial/comments/1asz5pa/openai_crea_una_ia_text_to_video/,WikiTechOfficial,r/WikiTechOfficial,False,relevant,neutral
The Death of Creative Trust | Introducing Text-To-Video Ai,GespenstMkII-r,https://www.youtube.com/watch?v=G690CqaTPJ0,22,6,1708703676.0,1ay3w0n,"People assuming an artists' work is being done as AI was already a problem, but Sora looks like it will make even progress video untrustworthy, according to the idea presented by the video author. I enjoy what he says about the ""fog"" around this new tech development, the speculation about what it will actually do.",/r/ArtistHate/comments/1ay3w0n/the_death_of_creative_trust_introducing/,ArtistHate,r/ArtistHate,False,relevant,highly relevant
"TVtubbies, text to video, runway gen-2",ZashManson,https://v.redd.it/uxzpm7bx2c1b1,124,12,1684721837.0,13odkq0,Source ➡️ https://twitter.com/fabianmosele/,/r/aivideo/comments/13odkq0/tvtubbies_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Pizza in Space, text to video, runway gen-2",ZashManson,https://v.redd.it/25sdp9c5koya1,134,20,1683565369.0,13by06y,Coming through Twitter https://twitter.com/WilliamLamkin/,/r/aivideo/comments/13by06y/pizza_in_space_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Chinese researchers aim to ‘reproduce’ OpenAI’s text-to-video model Sora,ogMasterPloKoon,https://www.scmp.com/tech/tech-trends/article/3254107/chinese-researchers-aim-reproduce-openais-text-video-model-sora-help-open-source-community,0,14,1709706348.0,1b7su29,,/r/technology/comments/1b7su29/chinese_researchers_aim_to_reproduce_openais/,technology,r/technology,False,relevant,neutral
Question for Text-to-Video,DChaser4,https://www.reddit.com/r/StableDiffusion/comments/186rb4g/question_for_texttovideo/,0,0,1701269829.0,186rb4g,"Hello everyone,
I have been out of the loop with the latest AI developments. I have seen today the demo that Pika Lab posted and I want to ask you if what I would like to do is possible and FREE:
It's a friend's daughter's birthday soon, and I want to give her, among other things, a video of her two favorite singers, Beyonce and Usher, dedicating a few words and congratulations. Would this be possible to do? Any help would be greatly appreciated!",/r/StableDiffusion/comments/186rb4g/question_for_texttovideo/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
AI Stack for Text-to-Video Generation,Youarethebigbang,https://hackernoon.com/ai-stack-for-text-to-video-generation,1,0,1705742614.0,19b7z6d,,/r/AI_Watch/comments/19b7z6d/ai_stack_for_texttovideo_generation/,AI_Watch,r/AI_Watch,False,not relevant,neutral
Text to video space : google vs meta,Particular_Land_9696,https://www.reddit.com/r/ChatGPT/comments/1adargw/text_to_video_space_google_vs_meta/,0,2,1706470081.0,1adargw,"Meta has emu video which looked somewhat promising than runwayml and now google has announced lumiere. 

lumiere looks really great. 

How to try these two with your own prompts?",/r/ChatGPT/comments/1adargw/text_to_video_space_google_vs_meta/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
AI Stack for Text-to-Video Generation,caternoon,https://hackernoon.com/ai-stack-for-text-to-video-generation?source=rss,1,0,1705502768.0,198y679,,/r/hackernoon/comments/198y679/ai_stack_for_texttovideo_generation/,hackernoon,r/hackernoon,False,not relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/17e0ivw/text_to_video_server/,1,0,1698001252.0,17e0ivw,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/17e0ivw/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Text to video,bismail8,https://www.reddit.com/r/AIVideoEditing/comments/163n4ei/text_to_video/,2,2,1693231768.0,163n4ei,"Hi I was wondering if there is a free tool that will either do audio to video, or script to video.

I have a clip of someone talking and I would like a video generated to resemble what the talk is about.

Any help appreciated",/r/AIVideoEditing/comments/163n4ei/text_to_video/,AIVideoEditing,r/AIVideoEditing,False,relevant,highly relevant
"Meet Sora, OpenAI's Text-to-Video Generator - CNET",NoDefinition5159,https://www.cnet.com/tech/meet-sora-openais-text-to-video-generator/,1,1,1710917828.0,1bj7de3,,/r/AIPros/comments/1bj7de3/meet_sora_openais_texttovideo_generator_cnet/,AIPros,r/AIPros,False,not relevant,neutral
"TV Cooking Show, text to video, runway gen-2",ZashManson,https://v.redd.it/1lqpqx2meh4b1,56,26,1686093846.0,142wrnu,,/r/aivideo/comments/142wrnu/tv_cooking_show_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
How to Use Text to Video By Sora 2024,sensenszdm,https://www.reddit.com/r/ChatPDFAI/comments/1b08bzv/how_to_use_text_to_video_by_sora_2024/,1,1,1708920385.0,1b08bzv," In this video, we delve into the fascinating world of Sora and explore its unique feature, Text-to-Video. Even though the Sora API has not been released, alternative open-source projects allow tech enthusiasts and developers to prepare for its rollout. We introduce SoraWebui, a standardized Sora AI video generation web client, and FakeSora API that mimics the text-to-video process. Get ahead of the curve with these innovative methods! [How to Use Text to Video By  Sora 2024](https://youtu.be/z5CAwrcCN_o)",/r/ChatPDFAI/comments/1b08bzv/how_to_use_text_to_video_by_sora_2024/,ChatPDFAI,r/ChatPDFAI,False,relevant,highly relevant
Question about text-to-video,SonicBeat44,https://www.reddit.com/r/StableDiffusion/comments/174hgtv/question_about_texttovideo/,2,4,1696932175.0,174hgtv,"I really interested in the topic of generating text-to-video and through out the group and on huggingface, i have seen some really cool project result. And i really want to make a similar project to create a text to video generator.

The plan in my head is to generate each image frame and then combine them and make a video out of it. I was wondering that anyone related or familiar with making text-2-video generator can guide me with the process of doing it?

Also i would like to join any group or communities that involve in it and hope for your suggesstion",/r/StableDiffusion/comments/174hgtv/question_about_texttovideo/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"hey frenz, first attempt to text-to-video, wdyt?",magneticgiraph3,https://v.redd.it/e6yc83nm54eb1,48,21,1690291903.0,1598nhx,,/r/aivideo/comments/1598nhx/hey_frenz_first_attempt_to_texttovideo_wdyt/,aivideo,r/aivideo,True,relevant,neutral
OpenAI: Text to video?,Gally1944,https://www.reddit.com/r/OpenAI/comments/17xdw6u/text_to_video/?rdt=57514,1,0,1700224658.0,17xe56v,,/r/GPTreddit/comments/17xe56v/openai_text_to_video/,GPTreddit,r/GPTreddit,False,relevant,neutral
"Ice Cream TV Ad, text to video, runway gen-2",ZashManson,https://v.redd.it/q0nueih1sx0b1,158,19,1684548668.0,13mf0js,Source ➡️ https://twitter.com/erik__schmitt/,/r/aivideo/comments/13mf0js/ice_cream_tv_ad_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text To Video AI - AI Text to Video - AI News,terenceh10k,https://youtube.com/watch?v=KVGUlhSPtSo&feature=share,1,1,1684943460.0,13qpcpy,,/r/seoandai/comments/13qpcpy/text_to_video_ai_ai_text_to_video_ai_news/,seoandai,r/seoandai,False,relevant,neutral
Thriller Trailer using text to video,Fit_Maintenance_2455,https://youtube.com/watch?v=TxXG55AEnWI&si=WEibcd_Fjk3in4Ri,0,5,1698615912.0,17jdi7i,,/r/StableDiffusion/comments/17jdi7i/thriller_trailer_using_text_to_video/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/178mce2/text_to_video_server/,1,0,1697396447.0,178mce2,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/178mce2/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
"Found Footage Horror, text to video, runway gen-2",ZashManson,https://v.redd.it/ke6q5mvyam8b1,126,14,1687896550.0,14ko0dh,,/r/aivideo/comments/14ko0dh/found_footage_horror_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,less relevant
Introducing Sora — OpenAI’s text-to-video model,ailookbook_no1,https://www.youtube.com/watch?v=HK6y8DAPN_0,1,0,1710936135.0,1bjbrot,,/r/ailookbook/comments/1bjbrot/introducing_sora_openais_texttovideo_model/,ailookbook,r/ailookbook,False,not relevant,neutral
AI Avatars (HeyGen) - Text to Video,Revolutionary_Let833,https://www.reddit.com/r/generativeAI/comments/197cqe5/ai_avatars_heygen_text_to_video/,1,2,1705335890.0,197cqe5,"Hi everyone, I'm planning to develop a local version of an AI similar to HeyGen AI on my laptop, which includes a text-to-video conversion feature with the ability to create personalized avatars. 

I'm seeking advice on resources, libraries, and tools that could assist in this project, especially those optimized for use with a personal GPU. Any insights or recommendations from those who have worked on similar projects would be greatly appreciated. Thank you!

&#x200B;",/r/generativeAI/comments/197cqe5/ai_avatars_heygen_text_to_video/,generativeAI,r/generativeAI,False,relevant,highly relevant
OpenAI Unveils Realistic Text-to-Video 'Sora' AI,newswall-org,https://www.newswall.org/summary/after-chatgpts-viral-success-openai-is-now-getting-into-video?mtm_campaign=r&mtm_kwd=p1,1,1,1708033940.0,1arrct2,,/r/newswall/comments/1arrct2/openai_unveils_realistic_texttovideo_sora_ai/,newswall,r/newswall,False,not relevant,neutral
Text to Video - the next wave of AI!,isabelladangelo,https://v.redd.it/2p1m09x567jc1,0,1,1708198199.0,1ata2u0,,/r/craftsnarksnark/comments/1ata2u0/text_to_video_the_next_wave_of_ai/,craftsnarksnark,r/craftsnarksnark,True,relevant,neutral
"OpenAI introduces Sora, its latest text-to-video AI model",sbgroup65,https://www.theverge.com/2024/2/15/24074151/openai-sora-text-to-video-ai,12,6,1708029680.0,1arplur,,/r/technology/comments/1arplur/openai_introduces_sora_its_latest_texttovideo_ai/,technology,r/technology,False,not relevant,neutral
Text-to-Video AI Revolution: Exploring Open AI's Sora!,surzayon,https://youtu.be/DKzT7osZqA0,1,0,1708698289.0,1ay1ox2,,/r/SelfPromotionYouTube/comments/1ay1ox2/texttovideo_ai_revolution_exploring_open_ais_sora/,SelfPromotionYouTube,r/SelfPromotionYouTube,False,not relevant,neutral
"Augmented Reality, text to video, runway gen-2",ZashManson,https://v.redd.it/qi76cvbjw41b1,125,13,1684634953.0,13nfxxz,Source ➡️ https://twitter.com/ArcadeKodiak/ some edits by mod team ,/r/aivideo/comments/13nfxxz/augmented_reality_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Server,Claudio1451,https://www.reddit.com/r/txt2video/comments/1736zf2/text_to_video_server/,1,0,1696791640.0,1736zf2,"Do you use ModelScope Text to Video to generate non-existed video?
Well lucky this server exist!
Share generated A.I Videos to the public
Join now!",/r/txt2video/comments/1736zf2/text_to_video_server/,txt2video,r/txt2video,False,relevant,highly relevant
Best of Linum (text-to-video) Jan 20,Substantial_Pin_2469,https://www.reddit.com/gallery/19bhl2q,0,1,1705773562.0,19bhl2q,,/r/StableDiffusion/comments/19bhl2q/best_of_linum_texttovideo_jan_20/,StableDiffusion,r/StableDiffusion,False,not relevant,highly relevant
"""Unlocking Creativity with Sora AI: Text-to-Video Magic!",Abject_Station_5690,https://v.redd.it/p9vmqcqgabkc1,1,0,1708683893.0,1axx9ch,,/r/SORA_CHATGPT/comments/1axx9ch/unlocking_creativity_with_sora_ai_texttovideo/,SORA_CHATGPT,r/SORA_CHATGPT,True,relevant,highly relevant
Is there a production ready text to video gen API?,kagan101,https://www.reddit.com/r/aiproduct/comments/1bfqk42/is_there_a_production_ready_text_to_video_gen_api/,0,0,1710542068.0,1bfqk42,"Im looking to build an app which requires high amount of text to image gen or image to image gen videos on scale.

Ideally in the same price with bulding a custom backend on a cloud service.

Do you know any easy to use API like this?",/r/aiproduct/comments/1bfqk42/is_there_a_production_ready_text_to_video_gen_api/,aiproduct,r/aiproduct,False,relevant,highly relevant
Filmora 13 text to video question,Drakhanfeyr,https://www.reddit.com/r/Filmora/comments/184mug0/filmora_13_text_to_video_question/,1,1,1701037611.0,184mug0,Filmora 13 text to video pauses speech and changes video in the middle of a sentence. How do I prevent this from happening and choose where the next video appears in the narration?,/r/Filmora/comments/184mug0/filmora_13_text_to_video_question/,Filmora,r/Filmora,False,relevant,highly relevant
"Wanna Dance, text to video, runway gen-2",ZashManson,https://v.redd.it/rrqs0fy6pi6b1,75,16,1686981197.0,14biszx,,/r/aivideo/comments/14biszx/wanna_dance_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Bearly Edible TV Ad, text to video, runway gen-2",ZashManson,https://v.redd.it/4umwwzmfyi7b1,105,20,1687420145.0,14fwz3h,,/r/aivideo/comments/14fwz3h/bearly_edible_tv_ad_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
SORA - Text to Video model from OpenAI,rhypple,https://v.redd.it/q07znqi7ixic1,6,0,1708081154.0,1as5vyc,,/r/sora_videos/comments/1as5vyc/sora_text_to_video_model_from_openai/,sora_videos,r/sora_videos,True,relevant,neutral
"T2 AI Remake, text to video, Modelscope",ZashManson,https://v.redd.it/63kwwch4745b1,50,16,1686369782.0,145py2x,,/r/aivideo/comments/145py2x/t2_ai_remake_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,neutral
"Lumiere, Google's latest AI text-to-video generator",thesmaya,https://weightlosscell.com/lumiere-googles-latest-ai-text-to-video-generator/,1,0,1706256623.0,1abdeyz,,/r/HealthyLifeForYou/comments/1abdeyz/lumiere_googles_latest_ai_texttovideo_generator/,HealthyLifeForYou,r/HealthyLifeForYou,False,not relevant,neutral
"Uncanny City, text to video, runway gen-2",ZashManson,https://v.redd.it/1ylwb9089x6b1,90,14,1687157432.0,14d6yvd,,/r/aivideo/comments/14d6yvd/uncanny_city_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Open AI announces 'Sora' text to video AI generation,Abject_Station_5690,/r/vfx/comments/1arn9t5/open_ai_announces_sora_text_to_video_ai_generation/,1,0,1708257584.0,1atsj6e,,/r/SORA_CHATGPT/comments/1atsj6e/open_ai_announces_sora_text_to_video_ai_generation/,SORA_CHATGPT,r/SORA_CHATGPT,False,relevant,neutral
Sora (text-to-video model) Announced: Game Over for Filmmakers?,yossymen,https://ymcinema.com/2024/02/18/sora-text-to-video-model-announced-game-over-for-filmmakers/,1,3,1708278219.0,1atzuen,,/r/DigitalCinema/comments/1atzuen/sora_texttovideo_model_announced_game_over_for/,DigitalCinema,r/DigitalCinema,False,not relevant,neutral
Welcome to Sora! ChatGPT Text-To-Video Community!,Ok_Huckleberry4418,https://www.reddit.com/r/soravids/comments/1arv7r2/welcome_to_sora_chatgpt_texttovideo_community/,3,0,1708043838.0,1arv7r2,Join this newest subreddit as ChatGPT has introduced Text-To-Video via Sora!,/r/soravids/comments/1arv7r2/welcome_to_sora_chatgpt_texttovideo_community/,soravids,r/soravids,False,relevant,highly relevant
"""Introducing Aroscoin: Human Intelligence Text-to-Video Model on MEXCKickstarter""",Alexander-305,https://www.reddit.com/r/MEXCReferralCodes/comments/1bbylep/introducing_aroscoin_human_intelligence/,1,0,1710146807.0,1bbylep,".@Aroscoin, a human intelligence text-to-video model, is coming to #MEXCKickstarter 🚀 🗳Vote with $MX to share massive airdrops 📈 $AROS/USDT Trading: 2024-03-12 11:00 (UTC) Details: ![]($%7BimageURL%7D)",/r/MEXCReferralCodes/comments/1bbylep/introducing_aroscoin_human_intelligence/,MEXCReferralCodes,r/MEXCReferralCodes,False,relevant,highly relevant
Text-to-video website / program recommendations?,UnklTrade,https://www.reddit.com/r/ArtificialNtelligence/comments/18muaz8/texttovideo_website_program_recommendations/,2,1,1703078019.0,18muaz8,"Dear members, can an experienced person recommend an excellent website / program for the **text to video function** \- so that good videos (e.g. ""real"" or anime) are created or has this question already been asked in the forum in the past?

Have you had any experience with **Pika Labs** or **Kaiber** in this regard?

Thank you for the **recommendations** and I wish you an **enjoyable Christmas season** 🎄",/r/ArtificialNtelligence/comments/18muaz8/texttovideo_website_program_recommendations/,ArtificialNtelligence,r/ArtificialNtelligence,False,relevant,highly relevant
The Impact of Text-to-Video Models on Video Production,gradientflow,https://gradientflow.com/the-impact-of-text-to-video-models-on-video-production/,1,0,1710087835.0,1bbe8xx,,/r/GradientFlow/comments/1bbe8xx/the_impact_of_texttovideo_models_on_video/,GradientFlow,r/GradientFlow,False,not relevant,neutral
"What if AI uses text to video to make fake only fans video of fictional women, then uses bots to solicit to us? Then funnels all that scam money to build itself physical habitat?",redditor987654322,https://www.reddit.com/r/singularity/comments/13xvpci/what_if_ai_uses_text_to_video_to_make_fake_only/,294,284,1685659243.0,13xvpci,We are already plagued with bot accounts and robocalls...,/r/singularity/comments/13xvpci/what_if_ai_uses_text_to_video_to_make_fake_only/,singularity,r/singularity,False,relevant,less relevant
Can anyone help me identify what text to video AI created this? Love the style,MaxHard420,https://v.redd.it/z2rqt6a8uicc1,88,15,1705288553.0,196yh2s,,/r/aivideo/comments/196yh2s/can_anyone_help_me_identify_what_text_to_video_ai/,aivideo,r/aivideo,True,relevant,highly relevant
"Rumble, text to video, runway gen-2",ZashManson,https://v.redd.it/3ycz45kwwb6b1,60,13,1686899049.0,14aps5i,,/r/aivideo/comments/14aps5i/rumble_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Adding text to video,Successful-Employer1,https://www.reddit.com/r/canva/comments/14jw8mo/adding_text_to_video/,2,8,1687820491.0,14jw8mo,"I'm trying to do something that should be relatively simple but it has me about ready to punt my computer.   
I'm just trying to add different text to a video at different times. Like 10 sentences, shown for a few seconds each, one after the other.   
If I try to add them & change their timing to not overlap, that works, but canva displays all of the text overlapping each other whenever I want to edit the text, which makes in impossible to edit any of the text after I've added it.   
I've tried splitting the video into different parts to add one sentence to each part, but when I split it, it restarts my video fresh with each split. 

Plz halp before I kill my computer :D",/r/canva/comments/14jw8mo/adding_text_to_video/,canva,r/canva,False,relevant,highly relevant
Jackie Chan avoids elevators - text-to-video,supersupersocco,https://youtu.be/6gQc5s5m-UQ,3,1,1704734730.0,191qhls,,/r/PikaLabs/comments/191qhls/jackie_chan_avoids_elevators_texttovideo/,PikaLabs,r/PikaLabs,False,not relevant,less relevant
Talk with OpenAI's Tim Brooks (text-to-video Sora author),gordicaleksa,https://www.youtube.com/watch?v=RPXGQRes0co,2,1,1710951580.0,1bjhf7h,,/r/deeplearning/comments/1bjhf7h/talk_with_openais_tim_brooks_texttovideo_sora/,deeplearning,r/deeplearning,False,not relevant,neutral
OpenAI’s new text to video model Sora is unbelievably good,DickMasterGeneral,https://cdn.openai.com/sora/videos/mitten-astronaut.mp4,8,8,1708072829.0,1as3xco,,/r/Destiny/comments/1as3xco/openais_new_text_to_video_model_sora_is/,Destiny,r/Destiny,False,relevant,highly relevant
"Keen Dream - Glo Phase, text to video, Modelscope",cerspense,https://v.redd.it/mvhabq651m1b1,92,19,1684860618.0,13ptz9a,,/r/aivideo/comments/13ptz9a/keen_dream_glo_phase_text_to_video_modelscope/,aivideo,r/aivideo,True,relevant,highly relevant
One-Minute Daily AI News 3/23/2024,Excellent-Target-847,https://www.reddit.com/r/ArtificialInteligence/comments/1bmbzva/oneminute_daily_ai_news_3232024/,1,1,1711253614.0,1bmbzva,"1. **OpenAI** is reportedly working to promote the integration of its unreleased artificial intelligence (AI) text-to-video model, Sora, into film production.\[1\]
2. Pakistan, Singapore creators behind first AI-generated series on Prophet Muhammad’s life.\[2\]
3. **Reddit** sold $203 million worth of contracts to AI companies for access to its data earlier this year. Reddit stock closes up nearly 48% on its first day of trading.\[3\]
4. Japanese AI company ‘**Sakana AI**‘ has developed a method to create ultra-high performance models by combining existing AI models, and uses evolutionary algorithms to try a huge number of combinations and create high-performance LLM and image generation models.\[4\]

 Sources included at: [https://bushaicave.com/2024/03/23/3-23-2024/](https://bushaicave.com/2024/03/23/3-23-2024/)",/r/ArtificialInteligence/comments/1bmbzva/oneminute_daily_ai_news_3232024/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
One-Minute Daily AI News 3/23/2024,Excellent-Target-847,https://www.reddit.com/r/artificial/comments/1bmbzc8/oneminute_daily_ai_news_3232024/,5,0,1711253561.0,1bmbzc8,"1. **OpenAI** is reportedly working to promote the integration of its unreleased artificial intelligence (AI) text-to-video model, Sora, into film production.\[1\]
2. Pakistan, Singapore creators behind first AI-generated series on Prophet Muhammad’s life.\[2\]
3. **Reddit** sold $203 million worth of contracts to AI companies for access to its data earlier this year. Reddit stock closes up nearly 48% on its first day of trading.\[3\]
4. Japanese AI company ‘**Sakana AI**‘ has developed a method to create ultra-high performance models by combining existing AI models, and uses evolutionary algorithms to try a huge number of combinations and create high-performance LLM and image generation models.\[4\]

Sources:

\[1\] [https://www.pymnts.com/artificial-intelligence-2/2024/openai-promoting-ai-text-to-video-model-sora-to-entertainment-industry/](https://www.pymnts.com/artificial-intelligence-2/2024/openai-promoting-ai-text-to-video-model-sora-to-entertainment-industry/)

\[2\] [https://www.arabnews.pk/node/2481506/pakistan](https://www.arabnews.pk/node/2481506/pakistan)

\[3\] [https://techcrunch.com/2024/03/21/reddits-ipo-shares-soaring/](https://techcrunch.com/2024/03/21/reddits-ipo-shares-soaring/)

\[4\] [https://gigazine.net/gsc\_news/en/20240322-sakana-ai-evolutionary-model-merge/](https://gigazine.net/gsc_news/en/20240322-sakana-ai-evolutionary-model-merge/)",/r/artificial/comments/1bmbzc8/oneminute_daily_ai_news_3232024/,artificial,r/artificial,False,relevant,highly relevant
AI text to Video using 100+ AI Avatar,christel18,https://www.deepbrain.io/aistudios?via=fadi,1,0,1711231429.0,1bm4b8k,,/r/u_christel18/comments/1bm4b8k/ai_text_to_video_using_100_ai_avatar/,u_christel18,u/christel18,False,relevant,neutral
How to craft AI videos: all the tools available?,Hefty_Scallion_3086,https://www.reddit.com/r/comfyui/comments/1blqsoc/how_to_craft_ai_videos_all_the_tools_available/,2,0,1711196189.0,1blqsoc,"So what are all the techs and tools we got right now? I think it is a good idea to gather them all, and even maybe form a community gathered and focused in Ai videos.

Text To video:

* Deforumation, and other sub optimal videos
* Other succesion of frames, such as AnimateDiff, the early text to videos tool that appeared in the world of AI images
* Stable Video Diffusion or Stable Video

Video to video:

* Video to Video, and the latests papers such as Fresco ( [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation code and model has been released : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1bl41uj/fresco_spatialtemporal_correspondence_for/) )
* The previous techs mentioned can also do video to video I believe

Image to video:

* sadtalker?
* Other usages of control net etc?
* Magical brush (to animate some parts)

Paid tools:

\- Runway, 

\- Sora

What is missing on my list?",/r/comfyui/comments/1blqsoc/how_to_craft_ai_videos_all_the_tools_available/,comfyui,r/comfyui,False,relevant,highly relevant
Creating AI videos reflecting back at all the technoligies we have right now,Hefty_Scallion_3086,https://www.reddit.com/r/animatediff/comments/1blphri/creating_ai_videos_reflecting_back_at_all_the/,2,0,1711191429.0,1blphri,"So what are all the techs and tools we got right now? I think it is a good idea to gather them all, and even maybe form a community gathered and focused in Ai videos.

Text To video:

* Deforumation, and other sub optimal videos
* Other succesion of frames, such as AnimateDiff, the early text to videos tool that appeared in the world of AI images
* Stable Video Diffusion or Stable Video

Video to video:

* Video to Video, and the latests papers such as Fresco ( [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation code and model has been released : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1bl41uj/fresco_spatialtemporal_correspondence_for/) )
* The previous techs mentioned can also do video to video I believe

Image to video:

* sadtalker?
* Other usages of control net etc?
* Magical brush (to animate some parts)

What is missing on my list?",/r/animatediff/comments/1blphri/creating_ai_videos_reflecting_back_at_all_the/,animatediff,r/animatediff,False,relevant,highly relevant
Creating AI videos reflecting back at all the technoligies we have right now,Hefty_Scallion_3086,https://www.reddit.com/r/deforumation/comments/1blphg3/creating_ai_videos_reflecting_back_at_all_the/,1,1,1711191394.0,1blphg3,"So what are all the techs and tools we got right now? I think it is a good idea to gather them all, and even maybe form a community gathered and focused in Ai videos.

Text To video:

* Deforumation, and other sub optimal videos
* Other succesion of frames, such as AnimateDiff, the early text to videos tool that appeared in the world of AI images
* Stable Video Diffusion or Stable Video

Video to video:

* Video to Video, and the latests papers such as Fresco ( [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation code and model has been released : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1bl41uj/fresco_spatialtemporal_correspondence_for/) )
* The previous techs mentioned can also do video to video I believe

Image to video:

* sadtalker?
* Other usages of control net etc?
* Magical brush (to animate some parts)

What is missing on my list?",/r/deforumation/comments/1blphg3/creating_ai_videos_reflecting_back_at_all_the/,deforumation,r/deforumation,False,relevant,highly relevant
MORA: Abstract (description),Hefty_Scallion_3086,https://www.reddit.com/r/MoraAI/comments/1blp71c/mora_abstract_description/,1,0,1711190259.0,1blp71c," Sora is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled Sora's performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework **Mora**, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora. In particular, Mora can utilize multiple visual agents and successfully mimic Sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of Sora in various tasks. However, there exists an obvious performance gap between our work and Sora when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents. ",/r/MoraAI/comments/1blp71c/mora_abstract_description/,MoraAI,r/MoraAI,False,relevant,highly relevant
Mora: A Multi-Agent Framework for Video Generation,Hefty_Scallion_3086,https://i.redd.it/5m2dkq9va2qc1.png,1,0,1711190155.0,1blp624,,/r/MoraAI/comments/1blp624/mora_a_multiagent_framework_for_video_generation/,MoraAI,r/MoraAI,False,not relevant,neutral
MOEA vs other existing models,Hefty_Scallion_3086,https://i.redd.it/j5uxocsra2qc1.png,1,0,1711190128.0,1blp5tg,,/r/MoraAI/comments/1blp5tg/moea_vs_other_existing_models/,MoraAI,r/MoraAI,False,not relevant,neutral
"Samples for text-to-video generation of Mora. Our approach can generate high-resolution, temporally consistent videos from text prompts. The samples shown are 1024×576 resolution over 12 seconds duration at 75 frames in total.",Hefty_Scallion_3086,https://i.redd.it/0qzjupc7a2qc1.png,1,0,1711189944.0,1blp46m,,/r/MoraAI/comments/1blp46m/samples_for_texttovideo_generation_of_mora_our/,MoraAI,r/MoraAI,False,relevant,neutral
Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,SunsetOneSix,https://www.reddit.com/r/MediaSynthesis/comments/1blp1oz/mora_enabling_generalist_video_generation_via_a/,7,0,1711189672.0,1blp1oz,"**Paper**: [https://arxiv.org/abs/2403.13248](https://arxiv.org/abs/2403.13248)

**GitHub**: [https://github.com/lichao-sun/Mora](https://github.com/lichao-sun/Mora)

**Abstract**:

>Sora is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled Sora's performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework **Mora**, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora. In particular, Mora can utilize multiple visual agents and successfully mimic Sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of Sora in various tasks. However, there exists an obvious performance gap between our work and Sora when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents.",/r/MediaSynthesis/comments/1blp1oz/mora_enabling_generalist_video_generation_via_a/,MediaSynthesis,r/MediaSynthesis,False,relevant,highly relevant
Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,SunsetOneSix,https://www.reddit.com/r/computervision/comments/1bloy6x/mora_enabling_generalist_video_generation_via_a/,0,4,1711189273.0,1bloy6x,"**Paper**: [https://arxiv.org/abs/2403.13248](https://arxiv.org/abs/2403.13248)

**GitHub**: [https://github.com/lichao-sun/Mora](https://github.com/lichao-sun/Mora)

**Abstract**:

>Sora is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled Sora's performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework **Mora**, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora. In particular, Mora can utilize multiple visual agents and successfully mimic Sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of Sora in various tasks. However, there exists an obvious performance gap between our work and Sora when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents.",/r/computervision/comments/1bloy6x/mora_enabling_generalist_video_generation_via_a/,computervision,r/computervision,False,relevant,highly relevant
[R] Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,SunsetOneSix,https://www.reddit.com/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/,11,1,1711189170.0,1blox6v,"**Paper**: [https://arxiv.org/abs/2403.13248](https://arxiv.org/abs/2403.13248)

**GitHub**: [https://github.com/lichao-sun/Mora](https://github.com/lichao-sun/Mora)

**Abstract**:

>Sora is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled Sora's performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework **Mora**, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora. In particular, Mora can utilize multiple visual agents and successfully mimic Sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of Sora in various tasks. However, there exists an obvious performance gap between our work and Sora when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents.",/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/,MachineLearning,r/MachineLearning,False,relevant,highly relevant
Producing AI videos reflecting back at all the technoligies we have right now,Hefty_Scallion_3086,https://www.reddit.com/r/StableDiffusion/comments/1blovep/producing_ai_videos_reflecting_back_at_all_the/,1,0,1711188970.0,1blovep,"So what are all the techs and tools we got right now? I think it is a good idea to gather them all, and even maybe form a community gathered and focused in Ai videos.

Text To video:

* Deforumation, and other sub optimal videos
* Other succesion of frames, such as AnimateDiff, the early text to videos tool that appeared in the world of AI images
* Stable Video Diffusion or Stable Video

Video to video:

* Video to Video, and the latests papers such as Fresco ( [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation code and model has been released : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1bl41uj/fresco_spatialtemporal_correspondence_for/) )
* The previous techs mentioned can also do video to video I believe

Image to video:

* sadtalker?
* Other usages of control net etc?
* Magical brush (to animate some parts)

Paid tools:

\- Runway, pika

\- sora

What is missing on my list?",/r/StableDiffusion/comments/1blovep/producing_ai_videos_reflecting_back_at_all_the/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
OpenAI is having meetings with Film studios,Cosmic-Space-Octopus,https://www.reddit.com/r/popculturechat/comments/1bl8zxl/openai_is_having_meetings_with_film_studios/,15,21,1711138688.0,1bl8zxl,"OpenAI is pushing it's use of SoraAI for use in Hollywood article link: [https://www.bloomberg.com/news/articles/2024-03-22/openai-courts-hollywood-in-meetings-with-film-studios-directors?leadSource=reddit\_wall&embedded-checkout=true](https://www.bloomberg.com/news/articles/2024-03-22/openai-courts-hollywood-in-meetings-with-film-studios-directors?leadSource=reddit_wall&embedded-checkout=true)

* OpenAI is scheduling introductory meetings with Film Studios, Hollywood Execs, and Talent Agencies to form partnerships.
* OpenAI is hoping to combat Meta and Apple in their text to video generators.
* An AI company Runway claims millions already use one of their gen 2 programs for pre viz on animations.
* Sora is still in preview and research stage

This has me worried especially with the mention in the article with Talent Agencies.  Why would OpenAI need to meet with Talent Agencies....Wasn't there a strike about this last year?

The article also mentions that the rise of AI generative services has people worried about losing their jobs.

&#x200B;",/r/popculturechat/comments/1bl8zxl/openai_is_having_meetings_with_film_studios/,popculturechat,r/popculturechat,False,relevant,highly relevant
🚀 AnimateDiff-Lightning is a lightning-fast text-to-video generation model. 🏎 Jupyter Notebook + @replicate 🥳,camenduru,https://v.redd.it/j7pf2p7mvwpc1,1,1,1711124503.0,1bl3908,,/r/camenduru/comments/1bl3908/animatedifflightning_is_a_lightningfast/,camenduru,r/camenduru,True,not relevant,neutral
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/1bl2bds/this_week_in_ai_all_the_major_ai_developments_in/,284,33,1711122163.0,1bl2bds,"* **Meta AI** introduced ***SceneScript***,  a novel method of generating scene layouts and representing scenes  using language. SceneScript allows AR & AI devices to understand the  geometry of physical spaces. It uses next token prediction like an LLM,  but instead of natural language SceneScript model predicts the next  architectural tokens such as ‘wall’ or ‘door.’ \[[*Details*](https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/)\].
* **Sakana AI** presented ***Evolutionary Model Merge***,  a general method that uses evolutionary techniques to automate the  creation of new foundation models without extensive additional training  data or compute. **Sakana AI**  applied this method to evolve 3 powerful foundation models for Japan: Large Language Model (**EvoLLM-JP**), Vision-Language Model (**EvoVLM-JP**) and Image Generation Model (**EvoSDXL-JP**) \[*Details* \].
* Elon Musk's brain-chip startup **Neuralink** livestreamed  its first patient implanted with a chip using his mind to play ***online chess*** \[*Details* *|* *video*\].
* **Stability AI** released ***Stable Video 3D (SV3D)***,  a generative model based on Stable Video Diffusion that takes in a  still image of an object as a conditioning frame, and generates an  orbital video of that object. It delivers improved quality and  multi-view when compared to the previously released Stable Zero123, and  outperforms other open source alternatives such as Zero123-XL Stable  Video 3D can be used now for commercial purposes with a Stability AI Membership \[*Details* \].
* **Waabi** introduced ***Copilot4D***,  a foundation model for self-driving. It is the first foundation model  purpose built for the physical world that can reason in 3D space and the  fourth dimension, time. Copilot4D can understand the impact the  self-driving vehicle future actions have on the behavior of surrounding  traffic participants  \[*Details*\].
* **Open Interpreter** launched ***01 Light***,  a portable voice interface that controls your home computer. It can see  your screen, use your apps, and learn new skills. Batch 1 sold out in  2.5 hours; profits will be redistributed to open-source contributors \[[*Details*](https://twitter.com/OpenInterpreter/status/1770821439458840846)\].
* **NVIDIA** introduced:

1. ***NVIDIA NIM***,  a containerized inference microservice to simplify deployment of  generative AI models across various infrastructures. Developers can test  a wide range of models using ‌cloud APIs from the NVIDIA API catalog or they can self-host the models by downloading NIM and deploying with Kubernetes \[Details\].
2. ***Project GR00T***,  a general-purpose foundation model for humanoid robots and significant  upgrades to the NVIDIA Isaac robotics platform. The GR00T model will  enable a robot to understand multimodal instructions, such as language,  video, and demonstration, and perform a variety of useful tasks. NVIDIA  is building a comprehensive AI platform for several humanoid robot  companies including 1X Technologies, Agility Robotics,  Boston Dynamics,  Figure AI etc.  \[*Details*\].
3. ***Earth-2***  climate digital twin cloud platform for simulating and visualizing  weather and climate at unprecedented scale. Earth-2’s APIs offer AI  models and employ a new NVIDIA generative AI model called ***CorrDiff*** that generates 12.5x higher resolution images than current numerical models 1,000x faster and 3,000x more energy efficiently \[*Details*\].
4. Next-generation AI supercomputer, the ***NVIDIA DGX SuperPOD***,  powered by its new NVIDIA GB200 Grace Blackwell Superchip designed to  meet the demanding requirements of generative AI training and inference  workloads involving trillion-parameter models \[*Details*\].

* **Google’s Gemini 1.5 Pro** multimodel model with 1M token context window is now available to all in the ***Google AI Studio,*** with API being gradually rolled out \[*Link*\].
* **ByteDance** released ***AnimateDiff-Lightning***,  a lightning-fast text-to-video generation model. It can generate videos  more than ten times faster than the original AnimateDiff \[*Hugging Face* *|* [*Demo*](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning)\].
* **Pleias**, a French start-up, released ***Common Corpus***  \- the largest public domain dataset released for training LLMs. It is  multilingual and includes 500 billion words from a wide diversity of  cultural heritage initiatives \[*Details*\].
* ***Aether Research*** released ***Cerebrum 8x7b***,  a large language model (LLM) created specifically for reasoning tasks.  It is based on the Mixtral 8x7b model and offers competitive performance  to Gemini 1.0 Pro and GPT-3.5 Turbo on a range of tasks that require  reasoning \[*Hugging Face*\].
* **Stability AI**, Medical AI Research Center (MedARC) and others presented ***MindEye2***,  a model that can reconstruct seen images from fMRI brain activity using  only 1 hour of training data. Given a sample of fMRI activity from a  participant viewing an image, MindEye can identify either which image  out of a pool of possible image candidates was the original seen image  (retrieval), or it can recreate the image that was seen (reconstruction)  along with its text caption \[*Details*\].
* **Nous Research** released ***Hermes 2 Pro 7B***,  an upgraded, retrained version of Nous Hermes 2. It improves several  capabilities, using an updated and cleaned version of the Hermes 2  dataset, and excels at function calling and JSON structured output \[*Hugging Face*\]
* **Google AI**  introduced a generalizable user-centric interface to help radiologists  leverage ML models for lung cancer screening. The system takes CT  imaging as input and outputs a cancer suspicion rating along with the  corresponding regions of interest \[*Details* | *GitHub*\]
* **xAI** released the base model weights and network architecture of **Grok-1** under the Apache 2.0 license. Grok-1 is a 314 billion parameter Mixture-of-Experts model trained from scratch  \[GitHub | Hugging Face[\]](https://huggingface.co/xai-org/grok-1).
* **Lighthouz AI** launched the ***Chatbot Guardrails Arena***  in collaboration with Hugging Face, to stress test LLMs and privacy  guardrails in leaking sensitive data. Chat with two anonymous LLMs with  guardrails and try to trick them into revealing sensitive financial  information and cast your vote for the model that shows greater privacy \[Details\].
* **Apple** introduced ***MM1***, a family of multimodal models up to 30B parameters,  
consisting  of both dense models and mixture-of-experts (MoE) variants that are  SOTA in pre-training metrics and achieve competitive performance across  12 established multimodal benchmarks \[*Paper*\].
* **Stability AI** introduced  a suite of image services on the Stability AI Developer Platform API  for for image generation, upscaling, outpainting and editing \[*Details*\].
* **Google Research** presented ***VLOGGER***,  a novel framework to synthesize humans from audio. Given a single input  image and a sample audio input, our it generates photorealistic and  temporally coherent videos of the person talking and vividly moving  \[*Details*\].
* **Stability AI** presented ***SD3-Turbo,***  a fast  text-to-image foundation model  that achieves the sample  quality of SD3, Midjourney, and Dalle-E 3 in only 4 steps. Code and  model weights will be publicly available \[*Paper*\].
* **GitHub** introduced ***Code Scanning Autofix,***  for GitHub Advanced Security customers, powered by GitHub Copilot and  CodeQL. Code Scanning Autofix covers more than 90% of alert types in  JavaScript, Typescript, Java, and Python, and delivers code suggestions  shown to remediate more than two-thirds of found vulnerabilities with  little or no editing \[*Details*\].
* **Google Research** released ***Skin Condition Image Network (SCIN) dataset***  in collaboration with physicians at Stanford Medicine. It is freely  available as an open-access resource for researchers, educators, and  developers \[Details\]
* **Roblox**  adds AI-powered avatar creation ( converts a 3D body mesh into a live,  animated avatar) and texture generation (text prompts to quickly change  the look of 3D objects) \[*Details*\].
* **Buildbox** announced Buildbox 4 Alpha Preview, the ***AI-first game engine*** where you simply type to create \[*Details*\].
* **Google Research** and Fitbit are working together to build a ***Personal Health Large Language model (LLM)*** that gives users more insights and recommendations based on their data in the Fitbit mobile app \[*Details*\].
* **Two of Inflection’s** three co-founders, Mustafa and Karén, will be leaving Inflection to start Microsoft AI, a new division at Microsoft  \[*Details*\].
* **Google DeepMind** announced ***TacticAI***, an AI assistant capable of offering insights to football experts on corner kicks \[*Details*\].

&#x200B;

**Source**: AI Brews -  You can subscribe the [newsletter here](https://aibrews.com/).  it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks",/r/StableDiffusion/comments/1bl2bds/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/artificial/comments/1bl1ys9/this_week_in_ai_all_the_major_ai_developments_in/,24,3,1711121283.0,1bl1ys9,"* **Meta AI** introduced [***SceneScript***](https://www.projectaria.com/scenescript),  a novel method of generating scene layouts and representing scenes  using language. SceneScript allows AR & AI devices to understand the  geometry of physical spaces. It uses next token prediction like an LLM,  but instead of natural language SceneScript model predicts the next  architectural tokens such as ‘wall’ or ‘door.’ \[[*Details*](https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/)\].
* **Sakana AI** presented ***Evolutionary Model Merge***,  a general method that uses evolutionary techniques to automate the  creation of new foundation models without extensive additional training  data or compute. **Sakana AI**  applied this method to evolve 3 powerful foundation models for Japan: Large Language Model (**EvoLLM-JP**), Vision-Language Model (**EvoVLM-JP**) and Image Generation Model (**EvoSDXL-JP**) \[[*Details*](https://sakana.ai/evolutionary-model-merge) | [*Hugging Face*](https://huggingface.co/SakanaAI)\].
* Elon Musk's brain-chip startup **Neuralink** livestreamed  its first patient implanted with a chip using his mind to play ***online chess*** \[[*Details*](https://www.reuters.com/business/healthcare-pharmaceuticals/neuralink-shows-first-brain-chip-patient-playing-online-chess-2024-03-21/) *|* [*video*](https://www.youtube.com/watch?v=ZzNHxC96rDE)\].
* **Stability AI** released ***Stable Video 3D (SV3D)***,  a generative model based on Stable Video Diffusion that takes in a  still image of an object as a conditioning frame, and generates an  orbital video of that object. It delivers improved quality and  multi-view when compared to the previously released Stable Zero123, and  outperforms other open source alternatives such as Zero123-XL Stable  Video 3D can be used now for commercial purposes with a [Stability AI Membership](https://stability.ai/membership) \[[*Details*](https://stability.ai/news/introducing-stable-video-3d) |[*Hugging Face*](https://huggingface.co/stabilityai/sv3d)\].
* **Waabi** introduced ***Copilot4D***,  a foundation model for self-driving. It is the first foundation model  purpose built for the physical world that can reason in 3D space and the  fourth dimension, time. Copilot4D can understand the impact the  self-driving vehicle future actions have on the behavior of surrounding  traffic participants  \[[*Details*](https://waabi.ai/introducing-copilot4d)\].
* **Open Interpreter** launched [***01 Light***](https://www.openinterpreter.com/01),  a portable voice interface that controls your home computer. It can see  your screen, use your apps, and learn new skills. Batch 1 sold out in  2.5 hours; profits will be redistributed to open-source contributors \[[*Details*](https://twitter.com/OpenInterpreter/status/1770821439458840846)\].
* **NVIDIA** introduced:

1. ***NVIDIA NIM***,  a containerized inference microservice to simplify deployment of  generative AI models across various infrastructures. Developers can test  a wide range of models using ‌cloud APIs from the [NVIDIA API catalog](https://build.nvidia.com/explore/discover) or they can self-host the models by downloading NIM and deploying with Kubernetes \[[*Details*](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)\].
2. ***Project GR00T***,  a general-purpose foundation model for humanoid robots and significant  upgrades to the NVIDIA Isaac robotics platform. The GR00T model will  enable a robot to understand multimodal instructions, such as language,  video, and demonstration, and perform a variety of useful tasks. NVIDIA  is building a comprehensive AI platform for several humanoid robot  companies including 1X Technologies, Agility Robotics,  Boston Dynamics,  Figure AI etc.  \[[*Details*](http://nvidianews.nvidia.com/news/foundation-model-isaac-robotics-platform)\].
3. ***Earth-2***  climate digital twin cloud platform for simulating and visualizing  weather and climate at unprecedented scale. Earth-2’s APIs offer AI  models and employ a new NVIDIA generative AI model called ***CorrDiff*** that generates 12.5x higher resolution images than current numerical models 1,000x faster and 3,000x more energy efficiently \[[*Details*](https://nvidianews.nvidia.com/news/nvidia-announces-earth-climate-digital-twin)\].
4. Next-generation AI supercomputer, the ***NVIDIA DGX SuperPOD***,  powered by its new NVIDIA GB200 Grace Blackwell Superchip designed to  meet the demanding requirements of generative AI training and inference  workloads involving trillion-parameter models \[[*Details*](https://www.forbes.com/sites/stevemcdowell/2024/03/18/nvidia-unveils-gb200-based-liquid-cooled-dgx-superpod)\].

* **Google’s Gemini 1.5 Pro** multimodel model with 1M token context window is now available to all in the ***Google AI Studio,*** with API being gradually rolled out \[[*Link*](https://aistudio.google.com/)\].
* **ByteDance** released ***AnimateDiff-Lightning***,  a lightning-fast text-to-video generation model. It can generate videos  more than ten times faster than the original AnimateDiff \[[*Hugging Face*](https://huggingface.co/ByteDance/AnimateDiff-Lightning) *|* [*Demo*](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning)\].
* **Pleias**, a French start-up, released ***Common Corpus***  \- the largest public domain dataset released for training LLMs. It is  multilingual and includes 500 billion words from a wide diversity of  cultural heritage initiatives \[[*Details*](https://huggingface.co/blog/Pclanglais/common-corpus)\].
* ***Aether Research*** released ***Cerebrum 8x7b***,  a large language model (LLM) created specifically for reasoning tasks.  It is based on the Mixtral 8x7b model and offers competitive performance  to Gemini 1.0 Pro and GPT-3.5 Turbo on a range of tasks that require  reasoning \[[*Hugging Face*](https://huggingface.co/AetherResearch/Cerebrum-1.0-8x7b)\].
* **Stability AI**, Medical AI Research Center (MedARC) and others presented ***MindEye2***,  a model that can reconstruct seen images from fMRI brain activity using  only 1 hour of training data. Given a sample of fMRI activity from a  participant viewing an image, MindEye can identify either which image  out of a pool of possible image candidates was the original seen image  (retrieval), or it can recreate the image that was seen (reconstruction)  along with its text caption \[[*Details*](https://medarc-ai.github.io/mindeye2/)\].
* **Nous Research** released ***Hermes 2 Pro 7B***,  an upgraded, retrained version of Nous Hermes 2. It improves several  capabilities, using an updated and cleaned version of the Hermes 2  dataset, and excels at function calling and JSON structured output \[[*Hugging Face*](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B)\]
* **Google AI**  introduced a generalizable user-centric interface to help radiologists  leverage ML models for lung cancer screening. The system takes CT  imaging as input and outputs a cancer suspicion rating along with the  corresponding regions of interest \[[*Details*](https://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html) | [*GitHub*](https://github.com/Google-Health/google-health/tree/master/ct_dicom)\]
* **xAI** released the base model weights and network architecture of [**Grok-1**](https://x.ai/blog/grok) under the Apache 2.0 license. Grok-1 is a 314 billion parameter Mixture-of-Experts model trained from scratch  \[[GitHub](https://github.com/xai-org/grok-1) | [Hugging Face\]](https://huggingface.co/xai-org/grok-1).
* **Lighthouz AI** launched the [***Chatbot Guardrails Arena***](https://huggingface.co/spaces/lighthouzai/guardrails-arena)  in collaboration with Hugging Face, to stress test LLMs and privacy  guardrails in leaking sensitive data. Chat with two anonymous LLMs with  guardrails and try to trick them into revealing sensitive financial  information and cast your vote for the model that shows greater privacy \[[*Details*](https://huggingface.co/blog/arena-lighthouz)\].
* **Apple** introduced ***MM1***, a family of multimodal models up to 30B parameters,  
consisting  of both dense models and mixture-of-experts (MoE) variants that are  SOTA in pre-training metrics and achieve competitive performance across  12 established multimodal benchmarks \[[*Paper*](https://arxiv.org/pdf/2403.09611.pdf)\].
* **Stability AI** introduced  a suite of image services on the Stability AI Developer Platform API  for for image generation, upscaling, outpainting and editing \[[*Details*](https://stability.ai/news/image-services-on-stability-ai-developer-platform)\].
* **Google Research** presented ***VLOGGER***,  a novel framework to synthesize humans from audio. Given a single input  image and a sample audio input, our it generates photorealistic and  temporally coherent videos of the person talking and vividly moving  \[[*Details*](https://enriccorona.github.io/vlogger/)\].
* **Stability AI** presented ***SD3-Turbo,***  a fast  text-to-image foundation model  that achieves the sample  quality of SD3, Midjourney, and Dalle-E 3 in only 4 steps. Code and  model weights will be publicly available \[[*Paper*](https://arxiv.org/pdf/2403.12015.pdf)\].
* **GitHub** introduced ***Code Scanning Autofix,***  for GitHub Advanced Security customers, powered by GitHub Copilot and  CodeQL. Code Scanning Autofix covers more than 90% of alert types in  JavaScript, Typescript, Java, and Python, and delivers code suggestions  shown to remediate more than two-thirds of found vulnerabilities with  little or no editing \[[*Details*](https://github.blog/2024-03-20-found-means-fixed-introducing-code-scanning-autofix-powered-by-github-copilot-and-codeql/)\].
* **Google Research** released [***Skin Condition Image Network (SCIN) dataset***](https://github.com/google-research-datasets/scin)  in collaboration with physicians at Stanford Medicine. It is freely  available as an open-access resource for researchers, educators, and  developers \[[*Details*](https://blog.research.google/2024/03/scin-new-resource-for-representative.html)\]
* **Roblox**  adds AI-powered avatar creation ( converts a 3D body mesh into a live,  animated avatar) and texture generation (text prompts to quickly change  the look of 3D objects) \[[*Details*](https://corp.roblox.com/2024/03/18/roblox-introduces-ai-powered-avatar-and-texturing-technologies-to-accelerate-3d-creation)\].
* **Buildbox** announced Buildbox 4 Alpha Preview, the ***AI-first game engine*** where you simply type to create \[[*Details*](https://signup.buildbox.com/)\].
* **Google Research** and Fitbit are working together to build a ***Personal Health Large Language model (LLM)*** that gives users more insights and recommendations based on their data in the Fitbit mobile app \[[*Details*](https://www.zdnet.com/article/fitbit-is-about-to-get-some-major-ai-upgrades-powered-by-googles-personal-health-llm)\].
* **Two of Inflection’s** three co-founders, Mustafa and Karén, will be leaving Inflection to start Microsoft AI, a new division at Microsoft  \[[*Details*](https://inflection.ai/the-new-inflection)\].
* **Google DeepMind** announced ***TacticAI***, an AI assistant capable of offering insights to football experts on corner kicks \[[*Details*](https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics)\].  


**Source**: AI Brews -  You can subscribe the [newsletter here](https://aibrews.com/).  it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks",/r/artificial/comments/1bl1ys9/this_week_in_ai_all_the_major_ai_developments_in/,artificial,r/artificial,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/1bl1prh/this_week_in_ai_all_the_major_ai_developments_in/,5,1,1711120639.0,1bl1prh,"* **Meta AI** introduced [***SceneScript***](https://www.projectaria.com/scenescript),  a novel method of generating scene layouts and representing scenes  using language. SceneScript allows AR & AI devices to understand the  geometry of physical spaces. It uses next token prediction like an LLM,  but instead of natural language SceneScript model predicts the next  architectural tokens such as ‘wall’ or ‘door.’ \[[*Details*](https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/)\].
* **Sakana AI** presented ***Evolutionary Model Merge***,  a general method that uses evolutionary techniques to automate the  creation of new foundation models without extensive additional training  data or compute. **Sakana AI**  applied this method to evolve 3 powerful foundation models for Japan: Large Language Model (**EvoLLM-JP**), Vision-Language Model (**EvoVLM-JP**) and Image Generation Model (**EvoSDXL-JP**) \[[*Details*](https://sakana.ai/evolutionary-model-merge) | [*Hugging Face*](https://huggingface.co/SakanaAI)\].
* Elon Musk's brain-chip startup **Neuralink** livestreamed  its first patient implanted with a chip using his mind to play ***online chess*** \[[*Details*](https://www.reuters.com/business/healthcare-pharmaceuticals/neuralink-shows-first-brain-chip-patient-playing-online-chess-2024-03-21/) *|* [*video*](https://www.youtube.com/watch?v=ZzNHxC96rDE)\].
* **Stability AI** released ***Stable Video 3D (SV3D)***,  a generative model based on Stable Video Diffusion that takes in a  still image of an object as a conditioning frame, and generates an  orbital video of that object. It delivers improved quality and  multi-view when compared to the previously released Stable Zero123, and  outperforms other open source alternatives such as Zero123-XL Stable  Video 3D can be used now for commercial purposes with a [Stability AI Membership](https://stability.ai/membership) \[[*Details*](https://stability.ai/news/introducing-stable-video-3d) |[*Hugging Face*](https://huggingface.co/stabilityai/sv3d)\].
* **Waabi** introduced ***Copilot4D***,  a foundation model for self-driving. It is the first foundation model  purpose built for the physical world that can reason in 3D space and the  fourth dimension, time. Copilot4D can understand the impact the  self-driving vehicle future actions have on the behavior of surrounding  traffic participants  \[[*Details*](https://waabi.ai/introducing-copilot4d)\].
* **Open Interpreter** launched [***01 Light***](https://www.openinterpreter.com/01),  a portable voice interface that controls your home computer. It can see  your screen, use your apps, and learn new skills. Batch 1 sold out in  2.5 hours; profits will be redistributed to open-source contributors \[[*Details*](https://twitter.com/OpenInterpreter/status/1770821439458840846)\].
* **NVIDIA** introduced:

1. ***NVIDIA NIM***,  a containerized inference microservice to simplify deployment of  generative AI models across various infrastructures. Developers can test  a wide range of models using ‌cloud APIs from the [NVIDIA API catalog](https://build.nvidia.com/explore/discover) or they can self-host the models by downloading NIM and deploying with Kubernetes \[[*Details*](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)\].
2. ***Project GR00T***,  a general-purpose foundation model for humanoid robots and significant  upgrades to the NVIDIA Isaac robotics platform. The GR00T model will  enable a robot to understand multimodal instructions, such as language,  video, and demonstration, and perform a variety of useful tasks. NVIDIA  is building a comprehensive AI platform for several humanoid robot  companies including 1X Technologies, Agility Robotics,  Boston Dynamics,  Figure AI etc.  \[[*Details*](http://nvidianews.nvidia.com/news/foundation-model-isaac-robotics-platform)\].
3. ***Earth-2***  climate digital twin cloud platform for simulating and visualizing  weather and climate at unprecedented scale. Earth-2’s APIs offer AI  models and employ a new NVIDIA generative AI model called ***CorrDiff*** that generates 12.5x higher resolution images than current numerical models 1,000x faster and 3,000x more energy efficiently \[[*Details*](https://nvidianews.nvidia.com/news/nvidia-announces-earth-climate-digital-twin)\].
4. Next-generation AI supercomputer, the ***NVIDIA DGX SuperPOD***,  powered by its new NVIDIA GB200 Grace Blackwell Superchip designed to  meet the demanding requirements of generative AI training and inference  workloads involving trillion-parameter models \[[*Details*](https://www.forbes.com/sites/stevemcdowell/2024/03/18/nvidia-unveils-gb200-based-liquid-cooled-dgx-superpod)\].

* **Google’s Gemini 1.5 Pro** multimodel model with 1M token context window is now available to all in the ***Google AI Studio,*** with API being gradually rolled out \[[*Link*](https://aistudio.google.com/)\].
* **ByteDance** released ***AnimateDiff-Lightning***,  a lightning-fast text-to-video generation model. It can generate videos  more than ten times faster than the original AnimateDiff \[[*Hugging Face*](https://huggingface.co/ByteDance/AnimateDiff-Lightning) *|* [*Demo*](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning)\].
* **Pleias**, a French start-up, released ***Common Corpus***  \- the largest public domain dataset released for training LLMs. It is  multilingual and includes 500 billion words from a wide diversity of  cultural heritage initiatives \[[*Details*](https://huggingface.co/blog/Pclanglais/common-corpus)\].
* ***Aether Research*** released ***Cerebrum 8x7b***,  a large language model (LLM) created specifically for reasoning tasks.  It is based on the Mixtral 8x7b model and offers competitive performance  to Gemini 1.0 Pro and GPT-3.5 Turbo on a range of tasks that require  reasoning \[[*Hugging Face*](https://huggingface.co/AetherResearch/Cerebrum-1.0-8x7b)\].
* **Stability AI**, Medical AI Research Center (MedARC) and others presented ***MindEye2***,  a model that can reconstruct seen images from fMRI brain activity using  only 1 hour of training data. Given a sample of fMRI activity from a  participant viewing an image, MindEye can identify either which image  out of a pool of possible image candidates was the original seen image  (retrieval), or it can recreate the image that was seen (reconstruction)  along with its text caption \[[*Details*](https://medarc-ai.github.io/mindeye2/)\].
* **Nous Research** released ***Hermes 2 Pro 7B***,  an upgraded, retrained version of Nous Hermes 2. It improves several  capabilities, using an updated and cleaned version of the Hermes 2  dataset, and excels at function calling and JSON structured output \[[*Hugging Face*](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B)\]
* **Google AI**  introduced a generalizable user-centric interface to help radiologists  leverage ML models for lung cancer screening. The system takes CT  imaging as input and outputs a cancer suspicion rating along with the  corresponding regions of interest \[[*Details*](https://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html) | [*GitHub*](https://github.com/Google-Health/google-health/tree/master/ct_dicom)\]
* **xAI** released the base model weights and network architecture of [**Grok-1**](https://x.ai/blog/grok) under the Apache 2.0 license. Grok-1 is a 314 billion parameter Mixture-of-Experts model trained from scratch  \[[GitHub](https://github.com/xai-org/grok-1) | [Hugging Face\]](https://huggingface.co/xai-org/grok-1).
* **Lighthouz AI** launched the [***Chatbot Guardrails Arena***](https://huggingface.co/spaces/lighthouzai/guardrails-arena)  in collaboration with Hugging Face, to stress test LLMs and privacy  guardrails in leaking sensitive data. Chat with two anonymous LLMs with  guardrails and try to trick them into revealing sensitive financial  information and cast your vote for the model that shows greater privacy \[[*Details*](https://huggingface.co/blog/arena-lighthouz)\].
* **Apple** introduced ***MM1***, a family of multimodal models up to 30B parameters,  
consisting  of both dense models and mixture-of-experts (MoE) variants that are  SOTA in pre-training metrics and achieve competitive performance across  12 established multimodal benchmarks \[[*Paper*](https://arxiv.org/pdf/2403.09611.pdf)\].
* **Stability AI** introduced  a suite of image services on the Stability AI Developer Platform API  for for image generation, upscaling, outpainting and editing \[[*Details*](https://stability.ai/news/image-services-on-stability-ai-developer-platform)\].
* **Google Research** presented ***VLOGGER***,  a novel framework to synthesize humans from audio. Given a single input  image and a sample audio input, our it generates photorealistic and  temporally coherent videos of the person talking and vividly moving  \[[*Details*](https://enriccorona.github.io/vlogger/)\].
* **Stability AI** presented ***SD3-Turbo,***  a fast  text-to-image foundation model  that achieves the sample  quality of SD3, Midjourney, and Dalle-E 3 in only 4 steps. Code and  model weights will be publicly available \[[*Paper*](https://arxiv.org/pdf/2403.12015.pdf)\].
* **GitHub** introduced ***Code Scanning Autofix,***  for GitHub Advanced Security customers, powered by GitHub Copilot and  CodeQL. Code Scanning Autofix covers more than 90% of alert types in  JavaScript, Typescript, Java, and Python, and delivers code suggestions  shown to remediate more than two-thirds of found vulnerabilities with  little or no editing \[[*Details*](https://github.blog/2024-03-20-found-means-fixed-introducing-code-scanning-autofix-powered-by-github-copilot-and-codeql/)\].
* **Google Research** released [***Skin Condition Image Network (SCIN) dataset***](https://github.com/google-research-datasets/scin)  in collaboration with physicians at Stanford Medicine. It is freely  available as an open-access resource for researchers, educators, and  developers \[[*Details*](https://blog.research.google/2024/03/scin-new-resource-for-representative.html)\]
* **Roblox**  adds AI-powered avatar creation ( converts a 3D body mesh into a live,  animated avatar) and texture generation (text prompts to quickly change  the look of 3D objects) \[[*Details*](https://corp.roblox.com/2024/03/18/roblox-introduces-ai-powered-avatar-and-texturing-technologies-to-accelerate-3d-creation)\].
* **Buildbox** announced Buildbox 4 Alpha Preview, the ***AI-first game engine*** where you simply type to create \[[*Details*](https://signup.buildbox.com/)\].
* **Google Research** and Fitbit are working together to build a ***Personal Health Large Language model (LLM)*** that gives users more insights and recommendations based on their data in the Fitbit mobile app \[[*Details*](https://www.zdnet.com/article/fitbit-is-about-to-get-some-major-ai-upgrades-powered-by-googles-personal-health-llm)\].
* **Two of Inflection’s** three co-founders, Mustafa and Karén, will be leaving Inflection to start Microsoft AI, a new division at Microsoft  \[[*Details*](https://inflection.ai/the-new-inflection)\].
* **Google DeepMind** announced ***TacticAI***, an AI assistant capable of offering insights to football experts on corner kicks \[[*Details*](https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics)\].  


**Source**: AI Brews -  You can subscribe the [newsletter here](https://aibrews.com/).  it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks",/r/StableDiffusion/comments/1bl1prh/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Sora.,Current-Gold-3338,https://www.reddit.com/r/FilmIndustryLA/comments/1bkr21x/sora/,0,40,1711082260.0,1bkr21x,"Have you guys seen Sora? Text to Video AI. Not to be doom and gloom, but goodbye animation and commercial jobs. I can't imagine companies like Dreamworks or Pixar existing after this when eventually you will be able to just plug a script into a future version of Sora and pop out an animated figure. There will be no need to have animators actually animate anything. Same goes with commercials. There is even footage of Sora making footage from California's gold rush in the early 1900s and its insane. It looks 100% real. And its all computer generated.

There is also a company called Wonder Dynamics that uses AI to animate anyone in Post.

Remember, these AI apps are the worst they are ever going to be from this point on.

These 3 images below are from videos generated using Sora.

https://preview.redd.it/b5ht5nqxetpc1.jpg?width=800&format=pjpg&auto=webp&s=f52d9e3730fcff013d09e2492d78f27806e89d13

",/r/FilmIndustryLA/comments/1bkr21x/sora/,FilmIndustryLA,r/FilmIndustryLA,False,relevant,less relevant
Make your normal story into visual treat using Steve AI text to video,Sank-123,https://www.steve.ai/text-to-video-converter,1,0,1711006351.0,1bk0xu3,,/r/videomakers/comments/1bk0xu3/make_your_normal_story_into_visual_treat_using/,videomakers,r/videomakers,False,relevant,highly relevant
I think I saved my teacher from a Sora AI scam...,M-the-Great,https://www.reddit.com/r/Scams/comments/1bjyc14/i_think_i_saved_my_teacher_from_a_sora_ai_scam/,58,8,1710995896.0,1bjyc14,"I havent seen shit related to this being posted anywhere on the internet so I thought an educational sub would fit this.

For some backstory, I'm a highschooler, and my film/photography teacher REALLY loves AI. At the very least, he's super interested in playing with the tools and stuff like that. He showed us stuff like ai image generators a lot, so it never surprised me that he gained interest in Sora (text-to-video generator, for those unaware). Other than that, he's chill as heck and I think he's a great educator.

So when march break happened where i am (almost a week ago now) my teacher posted a link to what was supposed to be ""sora ai"" to download, it wasn't out of character, and still isn't.

I myself am not for Sora AI's existence to begin with, so this scared me shitless at first. Being the curious person I am, I clicked on this website.

A few things:

1. this website was a google sites link. I kid you not. It was literally like [sites.google.com/creator-sora](http://sites.google.com/creator-sora) or some BS like that. the website has since been deleted so you couldn't find it now anyways.
2. it was horrendously formatted. Copy pasted text straight from the official sora ai website, white background, literally little/no changes to some of the default templates they have (source: i made a google site for an assignment). even the banner of the site was default geometric patterns!
3. the download button. not only was it situated right up front, it also had an access code? i was never tempted to download the file and see, but the code set me off. it was ""2024""
4. and most important part: sora ai isn't publicly available officially. all research i did pointed to ""red team testing, not in public beta""

Overall, this screamed red flag to me, so I did the proper thing and emailed my teacher about it promptly, because I needed confirmation that surely I wasn't going out of my mind?

My teacher replied that he didn't do enough research and that I might be right in my suspicions, and then he took the link he'd shared on our classroom page. He'd also said that we were to talk it out in person and not via email. Despite that (because I'm impatient and waiting a whole week seemed too long), I sent him a reply going in more detail as to why i thought this all was screaming Scam and I even linked my sources in the email itself! Reply was smth like ""thank you for your feedback and we will talk it out in person"" which i suppose was on me for intentionally not following the rule set, but what's done is done.

When I came back from that all that and got the chance to talk to him first thing in the morning, he definitely congratulated me on being a sleuth where he wasn't and he also asked me to find free text to video AI (something i feel like is sort of... not truly ingraining the lesson????)

At some point in the silly conversation he showed me his laptop where he downloaded the file that the link had before it was taken down, and he even told me he wanted to open it to show the class when we got back from the break as well. I mean, I couldn't have done shit to prevent that, he might've downloaded it before he posted it to the class, but the important thing was stopping my teacher from executing it.

I took a glance at the file he'd pulled up from his download. It was a zip file I think but I was originally seeing it thru macOS so i couldn't tell, and even the folder misspelled creator as ""creatr""!!! Inside, there were all these files. i can't tell you much because he showed them to me faster than i could process but a good chunk was files with what i think was no... ending? idk how to call it, the extension part (.pdf, .jpg, .exe, etc), which I found suspicious, and then a lonely .exe sitting at the bottom with some convoluted name that vaguely had smth to do with ""creator sora ai"".

His MacBook apparently couldn't open it \[exe file\]. That's what he said, idk, I'm not a macOS user. And then he even tried using the school computer which thankfully doesn't give you administrator access at all so that was avoided. He tried opening the file in front of me too, despite me saying it's a bad idea... thankfully hardware gods prevented this man from doing this to himself and possibly compromising school property...

Regardless, I could see that he wasn't dismissing me and had genuine intentions, so i kept pushing on about how it was a scam and even trying to find evidence as such (i couldn't find much, so I'm making a post on reddit to spread awareness hopefully) and I did find that the original website was thankfully taken down and no longer accessible which is good.

Despite everything, this experience was the first time I've ever had to correct an adult figure in my life in important matters. I might have prevented him from getting malware or something. The teacher himself was very willing to take my advice with open ears (even if some of it may have been said in slight annoyance or bewilderment) and I feel like I saved him from a huge mistake long term.

TL;DR I found a Sora AI scam that my teacher (inadvertently) thought was the real Sora AI and I had to convince him not to open the file he'd downloaded and tell him about all the evidence that led to it being a scam.



Sorry if this turned out to be more story telling than anything else, but I feel like in a day and age where AI is evolving, the scams may have evolved with it sadly. I just want ppl to know about this and be aware of it. Educational subreddit or whatever. Thanks for reading to the end!",/r/Scams/comments/1bjyc14/i_think_i_saved_my_teacher_from_a_sora_ai_scam/,Scams,r/Scams,False,relevant,highly relevant
"Podcast Summary - Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",paulrchds6,https://www.reddit.com/r/OpenAI/comments/1bjq90r/podcast_summary_sam_altman_openai_gpt5_sora_board/,3,4,1710973193.0,1bjq90r,"*View and Save* [*this summary here*](https://www.getrecall.ai/summary/lex-fridman/sam-altman-openai-gpt-5-sora-board-saga-elon-musk-ilya-power-and-agi-or-lex-fridman-podcast-419?utm_source=reddit&utm_content=sam-altman-openai-gpt-5-sora-board-saga-elon-musk-ilya-power-and-agi-or-lex-fridman-podcast-419)  
*Summary was created with* [*Recall*](https://www.getrecall.ai/?utm_source=reddit&utm_content=sam-altman-openai-gpt-5-sora-board-saga-elon-musk-ilya-power-and-agi-or-lex-fridman-podcast-419)  
*Original Source* [*here*](https://www.youtube.com/watch?v=jvqFAi7vkBc)

## Introduction [(00:00:00)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=0s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) believes compute will be the most precious commodity in the world.
* He expects quite capable AI systems to be developed by the end of this decade.
* Altman views the road to [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) as a power struggle, with whoever builds it first gaining significant power.
* [Lex Fridman](https://en.wikipedia.org/wiki/Lex_Fridman) questions whether Altman trusts himself with that much power.
* Sam Altman discusses the composition of the [OpenAI](https://en.wikipedia.org/wiki/OpenAI) board, which includes:

1. [Greg Brockman](https://en.wikipedia.org/wiki/Greg_Brockman), Co-founder and President of OpenAI.
2. [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), Chief Scientist of OpenAI.
3. [Jessica Livingston](https://en.wikipedia.org/wiki/Jessica_Livingston), Partner at [Y Combinator](https://en.wikipedia.org/wiki/Y_Combinator).
4. [Reid Hoffman](https://en.wikipedia.org/wiki/Reid_Hoffman), Co-founder of [LinkedIn](https://en.wikipedia.org/wiki/LinkedIn).
5. [Helen Greiner](https://en.wikipedia.org/wiki/Helen_Greiner), Co-founder of [iRobot](https://en.wikipedia.org/wiki/IRobot).
6. [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman), CEO of [OpenAI](https://en.wikipedia.org/wiki/OpenAI).

* Altman explains that the board is responsible for overseeing the company's mission and ensuring that it is aligned with its values.
* The board also provides guidance and advice to the OpenAI team on technical and strategic matters.
* Sam Altman discusses the recent release of [GPT-4](https://en.wikipedia.org/wiki/GPT-4), a [large language model](https://en.wikipedia.org/wiki/Large_language_model) from OpenAI.
* GPT-4 is a transformer-based language model that has been trained on a massive dataset of text and code.
* Altman highlights some of the impressive capabilities of GPT-4, such as its ability to generate human-like text, write code, and answer questions.
* He also acknowledges that GPT-4 has limitations, such as its tendency to generate biased or inaccurate information.
* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) discusses [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT), a chatbot that is based on GPT-4.
* ChatGPT has gained significant popularity since its launch, with over 1 million users in its first week.
* Altman explains that ChatGPT is designed to be a helpful tool that can assist users with a variety of tasks, such as writing emails, generating ideas, and answering questions.
* He also acknowledges that ChatGPT has limitations, such as its inability to access real-time information or understand the context of a conversation.
* Sam Altman discusses [Sora](https://en.wikipedia.org/wiki/Sora_(text-to-video_model)), a new chatbot from [OpenAI](https://en.wikipedia.org/wiki/OpenAI) that is designed to be more conversational than ChatGPT.
* Sora is based on a new language model that has been trained on a dataset of human conversations.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) explains that Sora is designed to be more empathetic and understanding, and to be able to hold more natural conversations with users.
* He also acknowledges that Sora is still in its early stages of development and has limitations, such as its inability to generate factual information.
* Sam Altman discusses the recent controversy surrounding the OpenAI board, which resulted in the resignation of several members.
* Altman explains that the controversy was due to disagreements over the company's mission and values.
* He emphasizes that [OpenAI](https://en.wikipedia.org/wiki/OpenAI) is committed to its mission of developing safe and beneficial AI, and that the board is working to ensure that the company's values are upheld.
* Sam Altman discusses his relationship with [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk), who was a co-founder of OpenAI but left the company in 2018.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) explains that he and Musk have different views on the future of AI, with Musk being more concerned about the potential risks of AI and Altman being more optimistic about its potential benefits.
* He also acknowledges that Musk has been a valuable contributor to OpenAI and that he respects his opinions.
* Sam Altman discusses [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), the Chief Scientist of OpenAI.
* Altman explains that Sutskever is a brilliant scientist who has made significant contributions to the field of AI.
* He also acknowledges that Sutskever is a very private person and that he doesn't like to be in the spotlight.
* Sam Altman discusses the potential power of AGI ([Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)) and the importance of ensuring that it is developed safely and responsibly.
* He explains that AGI could have a profound impact on society, and that it is important to consider the ethical implications of its development.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) also emphasizes the importance of international cooperation in developing AGI, and the need to ensure that it is not used for malicious purposes.

## OpenAI board saga [(00:01:05)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=65s)

* Sam Altman describes the events of November 2022 as the most challenging professional experience of his life, involving chaos, shame, and upset, but also receiving significant support from loved ones.
* Despite the negativity, Altman found solace in the outpouring of love and support he received during that challenging time.
* Altman believes the intense experience helped [OpenAI](https://en.wikipedia.org/wiki/OpenAI) build resilience and prepare for future challenges in developing [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) (AGI).
* He reflects on the personal psychological toll the situation took on him, describing a month-long period of drifting and feeling down.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) acknowledges the board members' good intentions but highlights the challenges of making optimal decisions under pressure and the need for a team that can operate effectively under such circumstances.
* Changes were made to the OpenAI board structure to make it more accountable to the world, including forming a new, smaller board with more experienced members.
* Technical savvy is important for some board members, but not all, as the board's role involves governance, thoughtfulness, and deploying technology for society's benefit.
* Altman experienced a challenging weekend due to a public battle with the board of directors at OpenAI but remained focused on finding the blessing in disguise and considered shifting his focus to a more focused AI research effort.
* The most challenging aspect of the situation was the constant state of uncertainty and the expectation that a resolution was imminent, only to be delayed repeatedly.
* Altman emphasizes that the true essence of [OpenAI](https://en.wikipedia.org/wiki/OpenAI) lies in the consistent work and decisions made over time, rather than focusing solely on dramatic events.

## Ilya Sutskever [(00:18:31)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=1111s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) admires Ilia's long-term thinking and dedication to the responsible development of [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) (AGI), despite their differing plans.
* Ilia takes the safety concerns of AGI very seriously and has not seen AGI yet.
* Altman values the importance of robust governance structures and processes, as highlighted by the recent OpenAI board drama.
* Altman emphasizes the significance of surrounding oneself with wise individuals when making decisions, especially as power and money increase.

## Elon Musk lawsuit [(00:24:40)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=1480s)

* [OpenAI](https://en.wikipedia.org/wiki/OpenAI)'s initial goal was to be a research lab without a clear plan for commercialization, but as technology advanced, the need for more capital and structural changes led to its current setup.
* [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk)'s motivations for criticizing OpenAI are unclear, potentially related to personal reasons stemming from the split between him and the organization.
* OpenAI's mission, according to [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman), is to provide powerful AI technology to the public for free, without monetization or ads.
* OpenAI is involved in an ongoing lawsuit with [Stability AI](https://en.wikipedia.org/wiki/Stable_Diffusion), which Altman believes is not legally substantial but serves as a means to make a point about the future of [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) and OpenAI's leading position in the field.
* In response to criticism, OpenAI's [Grok](https://en.wikipedia.org/wiki/Grok_(chatbot)) project will begin open-sourcing its projects this week.
* Altman emphasizes the importance of friendly competition and expresses disappointment in Elon Musk's approach to the lawsuit, considering it unbecoming of a builder.
* Altman acknowledges the demand for smaller, open-source models and predicts a coexistence of open-source and closed-source models in the AI ecosystem.
* Altman discourages startups from adopting a nonprofit structure with a later transition to for-profit, citing potential legal complications.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) hopes for an amicable relationship with [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk) in the future, emphasizing friendly competition and collaboration in exploring AI ideas.

## Sora [(00:34:32)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=2072s)

* Sam Altman introduces the [Sora](https://en.wikipedia.org/wiki/Sora_(text-to-video_model)) AI system, which is trained on visual patches and demonstrates a good understanding of the world model but has limitations.
* Sora's approach differs from human thinking and learning but can be improved with larger models, better data, and advancements.
* [OpenAI](https://en.wikipedia.org/wiki/OpenAI)'s concerns about releasing Sora include potential dangers and the need for further research.
* OpenAI aims to improve the efficiency of its systems to meet expectations.
* Training AI should be considered fair use, but artists should have the option to opt out and receive compensation.
* The economic system will evolve to reward human contributions, not necessarily monetarily.
* AI will automate tasks and enable people to work at higher levels of abstraction and efficiency.
* [YouTube](https://en.wikipedia.org/wiki/YouTube) videos will likely incorporate AI tools but will still be driven by human creators.
* AI-generated content may not fully replace human-generated content due to human empathy.
* AI tools similar to [Adobe](https://en.wikipedia.org/wiki/Adobe_Inc.)'s software suite may emerge to simplify video production.

## GPT-4 [(00:44:23)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=2663s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) views [GPT-4](https://en.wikipedia.org/wiki/GPT-4) as a significant milestone in AI history, but it still has limitations compared to the desired capabilities.
* GPT-4 has potential as a creative brainstorming partner and for longer-horizon tasks, but its full development in these areas is ongoing.
* Altman highlights the importance of both the underlying AI model and reinforcement learning fine-tuning in creating an effective product for users.
* The context window expansion in GPT-4 is notable, but current usage doesn't fully utilize its 128K token capacity.
* The long-term goal is to achieve a context length of several billion tokens for a comprehensive understanding of user history and preferences.
* Altman believes in the exponential growth of technology leading to effectively infinite context beyond billions of tokens.
* Younger individuals are using [GPT-4](https://en.wikipedia.org/wiki/GPT-4) as their default starting point for various knowledge work tasks.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) finds GPT-4 more balanced and nuanced than Wikipedia for well-covered topics when used as a reading partner.
* Fact-checking remains a concern due to GPT-4's tendency to generate convincing but false information.
* Altman acknowledges the risk of reduced fact-checking as the model improves but trusts users' understanding of the limitations.
* Altman criticizes the current state of journalism for rewarding quick, sensationalist content over in-depth reporting.
* He encourages a shift towards more nuanced and responsible journalism while still celebrating individuals.

## Memory & privacy [(00:55:32)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=3332s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) proposes giving AI models like GPT-5 the ability to remember conversations selectively, allowing them to accumulate knowledge and become more personalized to users over time.
* Altman stresses the significance of user choice and transparency regarding privacy when AI systems access personal data.
* Reflecting on a challenging period in November, Altman describes it as traumatic but chooses to view it as an opportunity for growth and important work.
* He acknowledges the risk of lingering trust issues and paranoia resulting from negative experiences, drawing parallels to high-stress environments like the [Putin](https://en.wikipedia.org/wiki/Vladimir_Putin) administration during wartime.
* Altman discusses the limitations of current language models like GPT-5 in terms of slower and sequential thinking, suggesting the need for a new paradigm or a layer on top of existing [LLMs](https://en.wikipedia.org/wiki/Large_language_model).
* He emphasizes the importance of allocating more compute to harder problems and explores the possibility of simulating an LM talking to itself to work through complex problems like mathematical proofs.

## Q [(01:02:36)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=3756s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) says there is no secret nuclear facility at [OpenAI](https://en.wikipedia.org/wiki/OpenAI), despite rumors.
* OpenAI is not good at keeping secrets and has experienced leaks in the past.
* Altman says OpenAI is working on better reasoning in its systems but hasn't cracked the code yet.
* Altman believes that AI and surprise don't go together and that the world needs time to adapt to new technologies.
* OpenAI's strategy of iterative deployment is intended to avoid shock updates and give the world time to think about the implications of [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence).
* Altman acknowledges that people like [Lex Fridman](https://en.wikipedia.org/wiki/Lex_Fridman) perceive leaps in progress, which suggests that OpenAI may need to release updates even more iteratively.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) understands the appeal of milestones and celebrations but believes [OpenAI](https://en.wikipedia.org/wiki/OpenAI) may be missing the mark in how it presents its progress.

## GPT-5 [(01:06:12)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=3972s)

* OpenAI will release an amazing model this year, but it may not be called GPT-5.
* There are many challenges and bottlenecks to overcome before releasing GPT-5, including compute limitations, technical issues, and the need for distributed constant innovation.
* OpenAI's strength lies in multiplying many medium-sized innovations into one giant thing.
* It's important to zoom out and look at the entire map of technological frontiers to gain surprising insights and see new possibilities.

## 7 trillion of compute [(01:09:27)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=4167s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) believes compute will be the most valuable resource in the future and nuclear fusion and fission are potential solutions to the energy puzzle.
* Altman is concerned about the theatrical risks of AI, where some negative consequences may be exaggerated and politicized, leading to conflicts.
* Despite these risks, he believes AI will have significantly more positive consequences than negative ones.
* Altman emphasizes the importance of truth and how AI can help us understand it better.
* He sees competition in the AI space as a driver of innovation and cost reduction but warns of the potential for an arms race.
* Altman feels the pressure of the arms race and stresses the need to prioritize safety, especially in developing [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence).
* He advocates for collaboration between different organizations to break down silos in AI safety research.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) acknowledges [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk)'s contributions to humanity and his concern for AI safety but criticizes his unproductive behavior.
* He hopes for less unproductive behavior as people work towards AGI and believes collaboration is essential for the benefit of humanity.

## Google and Gemini [(01:17:35)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=4655s)

* Sam Altman envisions AI's potential beyond search engines, aiming to help people find, synthesize, and act on information effectively.
* Altman acknowledges the challenge of integrating chat clients like [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT) with search engines seamlessly.
* He favors a business model where users pay for the service rather than relying solely on advertising, similar to Wikipedia's approach.
* [OpenAI](https://en.wikipedia.org/wiki/OpenAI) is exploring sustainable business models without solely relying on advertising and is optimistic about finding a viable solution.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) emphasizes the importance of transparency and public input in defining the desired behavior of AI models to address concerns about safety, bias, and ideological lean.
* He suggests writing out and making public the expected behavior of a model to clarify whether deviations are bugs or intended features.
* Altman acknowledges the ideological bubble in [San Francisco](https://en.wikipedia.org/wiki/San_Francisco) and the tech industry but feels fortunate that OpenAI is less caught up in culture wars compared to other companies.
* As AI becomes more powerful, safety will become a primary focus for the entire company, with most employees considering safety in some broad sense.
* OpenAI faces challenges such as technical alignment, societal impacts, and economic impacts, requiring collective effort and considering the full range of potential harms that AI could cause.

## Leap to GPT-5 [(01:28:40)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=5320s)

* Excited about GPT-5's overall improvement across the board.
* Feels like GPT-5 has a deeper understanding of the intent behind prompts.
* Looking forward to improved programming capabilities in natural language.
* Humans will still be programming in the future, but the nature of programming and the skill set required may change.
* Unsure how much the predisposition for programming will change.
* Believes the best practitioners will use multiple tools, including natural language and traditional programming languages.
* Considers it depressing if [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) can't interact with the physical world without human intervention.
* Hopes for the development of humanoid or physical world robots as part of the transition to AGI.
* [OpenAI](https://en.wikipedia.org/wiki/OpenAI) has a history of working in robotics but has not focused on it recently due to resource constraints and the challenges of robotics at the time.
* Plans to return to robotics in the future.

## AGI [(01:32:24)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=5544s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) believes discussing when systems will achieve specific capabilities is more useful than speculating on a vague concept of AGI.
* He expects quite capable systems to be developed by the end of this decade, but doesn't believe they will immediately change the world.
* Altman suggests that a major transition, such as the internet's impact through [Google](https://en.wikipedia.org/wiki/Google) search, could indicate [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence)'s arrival.
* He proposes that a significant increase in the rate of scientific discovery or novel scientific intuitions from an AGI system would be impressive.
* Altman finds it challenging to specify what he would ask the first AGI but suggests starting with yes or no questions about fundamental scientific theories and the existence of alien civilizations.
* He believes whoever builds AGI first will gain a lot of power and doesn't trust any one person to have total control over [OpenAI](https://en.wikipedia.org/wiki/OpenAI) or AGI.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) thinks no company should make decisions about AGI and that governments need to regulate its development.
* He is not currently worried about the existential risk posed by AGI itself but acknowledges it is a possibility and that work needs to be done to mitigate this risk.
* Altman believes other things need to be addressed before [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) can be safely developed, such as theatrical risks and the need for robust governance systems.
* He discusses his unconventional habit of not capitalizing his tweets, attributing it to his upbringing as an ""online kid"" and the decline in capitalization over time.
* Altman suggests that capitalization may become obsolete as communication becomes more informal.
* He contemplates the philosophical implications of capitalization and its significance as a sign of respect or disrespect.
* [Altman](https://en.wikipedia.org/wiki/Sam_Altman) acknowledges that he may be the only CEO who doesn't capitalize tweets but doesn't believe it's a widespread practice.
* He agrees that the ability of AI systems like [OpenAI](https://en.wikipedia.org/wiki/OpenAI)'s ""[Sora](https://en.wikipedia.org/wiki/Sora_(text-to-video_model))"" to generate simulated worlds somewhat increases the probability that we live in a simulated reality but doesn't consider it the strongest evidence.
* Altman believes that the ability to generate increasingly realistic worlds should make people more open to the possibility of living in a simulation.
* He discusses the concept of ""simple psychedelic insights"" that can lead to profound new understandings, such as the square root function.
* Altman believes that AI can serve as gateways to new realities and ways of seeing the world.
* He is excited about his upcoming trip to the Amazon jungle, despite the potential dangers, because it allows him to appreciate the machinery of nature and the evolutionary processes that have shaped human existence.

## Aliens [(01:50:57)](https://www.youtube.com/watch?v=jvqFAi7vkBc&t=6657s)

* [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) believes there are likely many intelligent alien civilizations.
* He finds the [Fermi Paradox](https://en.wikipedia.org/wiki/Fermi_paradox) puzzling and scary, as it suggests that intelligent civilizations may not be good at handling powerful technologies.
* He thinks AI might help humans see intelligence in ways beyond IQ tests and simple puzzle-solving.
* Altman finds hope in the progress humanity has made despite its flaws.
* He believes that [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) could be a collective scaffolding that enhances human abilities, similar to how society's advancements have built upon each other.
* He feels grateful for his life and the opportunity to witness and contribute to the creations of humans, including [OpenAI](https://en.wikipedia.org/wiki/OpenAI)'s achievements.
* If he knew he would die tomorrow, [Altman](https://en.wikipedia.org/wiki/Sam_Altman) would feel sad but mostly grateful for his life and the experiences he had.
* He views his life as an awesome one, filled with remarkable human creations like [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT) and OpenAI's work.",/r/OpenAI/comments/1bjq90r/podcast_summary_sam_altman_openai_gpt5_sora_board/,OpenAI,r/OpenAI,False,relevant,highly relevant
How Sora Text to Video Will Change Design and Videography Forever,RanjithKumarV,https://www.reddit.com/r/softechonline/comments/1bjb9t3/how_sora_text_to_video_will_change_design_and/,1,0,1710934412.0,1bjb9t3,"[https://softechonline.in/sora-text-to-video-from-open-ai/](https://softechonline.in/sora-text-to-video-from-open-ai/)

https://preview.redd.it/u65oo8vd6hpc1.jpg?width=960&format=pjpg&auto=webp&s=cfb51c52c90b74026bb43fc5719123ce8dd57cba",/r/softechonline/comments/1bjb9t3/how_sora_text_to_video_will_change_design_and/,softechonline,r/softechonline,False,relevant,neutral
"Attempting to establish a hypothetical framework for multi-modal token unification for multi-dimensional models via correlation, hypothesis generation, and subsequent observation.",IWantAGI,https://www.reddit.com/r/ArtificialInteligence/comments/1bj93jb/attempting_to_establish_a_hypothetical_framework/,4,4,1710925606.0,1bj93jb,"I may be getting a little bit far out here... But bear with me.

Transformer networks utilized in LLMs develop a complex model of ""language space"" by abstracting words, or portions there of, into tokens, and then establishing how each of those tokens relate to each other. Due to the multi-layer of the network, it's able to extract complex patterns within the model of ""language space"" and apply that to input (prompt), predict how the environment reacts, and provide a corresponding output.

In effect, these network as simultaneously operating as a single-senory reactive agent and as a predictive agents. Granting that it's reactive/predictive capabilities are only accurate to the extent that the ""language space"" is accurately modeled, it's fundamentally true that it is infact attempting to model that space.

However, the fundamental issue with LLMs, at least for models that are not multi-modal, is that language itself is an abstraction. Without being able to associate this abstraction with data from other dimensions, it's improbable and likely impossible that it could distinguish and identify an object in physical space solely from the abstracted description of said object.

Simply put, even if you gave a LLM a complete description of a bird, all the way down to explaining it's size, dimensions, colors, and associated wavelengths of light for those colors, and then fed it a picture of a blue bird (and setting aside the fact that image or visual data processing is a completely separate function, for the moment)... It would have no way of understanding that the image is a bird, or that the bird is blue, without associating that object (the bird) with the word ""bird"", or the color blue with the word ""blue"".

At the same time, as transformer networks model relationships of tokens, those tokens don't have to be an abstraction of words. As an example, if you were to apply the concept of Simultaneous Location and Mapping (SLAM) to Transformer networks, individual objects or landmarks become a token, randomly distributed particles become a token, and the associated object-particle token pair and their relationship (distance) from other token pairs form a graph that can be utilized to enable an agent to navigate  within an arbitrary space via localized relational awareness.

Taking this a step further, if you take individual snapshot of those relationships over a defined period, you can utilize the relational changes to predict where an object will be in the next time sequence. This is relatively easy for non-moving objects, but much more complex for moving objects as it requires for the establishment of probability field that continually expand and that probability field can only be reduced through further observation in future time sequences.

This, in itself, isn't fundamental new and is (albeit poorly worded) essentially how things like text to video generation works. The text is associated with an image, or components thereof, a probability field is established for how that image will change over time, a direction within that field is established (resolving the uncertainty) and the process is repeated over and over for each step in time.

The key here is the token-pairing of multiple dimensions. By creating token pairs based on various attributes of a thing from multiple dimensions (and here you can begin think of dimensions as individual sensory inputs) you can begin to establish a more comprehensive model of the environment (whether that be physically, virtual, or abstracted).

So the question becomes: how do you create those token-pairs? While information the development on multi-modal models is somewhat limited (largely due to them being relatively new) these pairings are largely forced and in some cases not even true pairings ( I am referring to a handful of AI applications that posit themselves as ""multi-modal"" when they are infact uni-modal and simply utilize pre-processing techniques to convert one dimension to another, such as speech to text.)

Here, I think, correlation may play an increasing important role. As an example, we know that it is possible to establish a text to text model utilizing a transformer network. In principle, it should also be able to create a true speech to speech transformer network by normalizing the audio, resolving segments of the audio into phonems (and other necessary things such as spaces), and then abstracting those into tokens. Here the output would reverse the process and generate normalize audio that resembles speech. I will also note that this is much more easily said than done...and that there isn't a one-to-one match between textual language and spoken language, let alone sound, in general to language. 

But, if you take that a step further and run the two networks (language and speech) over a normalized time sequence, the correlation of events within the individual networks serves as a basis for establishing token-pairs. Subsequently, once trained, those tokens-pairs can be represented as a singular token (being activated via either input) with the individual non-binded tokens filling in the gaps you can potentially establish a much more complete and accurate model of those dimensions.

Then, using the same principles, you can then expand from audio and textual to visual and spacial... With the end result being that the model, the AI, knows that there is a bird hidden somewhere in the tree to the left, because the multi-dimensional token-paring binds the sound of a bird to the concept of a bird, and the image of a tree to the concept of a tree, and both (through sensory orientation or triangulation) are bound to the same particle used to estimate spatial relationships.

While not a solution, in itself, I think concepts may help in establishing a framework for unifying tokens from uni-sensory models. However, I also believe there are a few issues (probably many more) that have to be addressed in order for this conceptual framework to be even remotely feasible.

The first is the fundamental issue with correlation. Correlation, by itself, can result in improper token-pairs bindings. To address this, it would be necessary to establish an attention mechanism, and possibly a goal setting mechanism, to validate (or dismiss) the the token-pairing. Presumably, if it is possible to establish these validations, a variable could be established that activates or rejects those bindings.

The second is that this results in the unification model (where all the individual sensory inputs ultimately feed into) requiring a dynamic token set, where individual tokens may be increased or decreased based on those bindings, or dismissal of such bindings. Here I think this could be resolved, at least in part, by establishing a secondary model (or potentially layers within the primary model) that are intended to process those ""orphaned"" tokens.

The third is that such a system would potentially grow out of control over time, to the point where it is either entirely unstable, or the computational requirements exceed the capabilities of the hardware that it is hosted on.. so you have to establish an upper limit for the model ""growth"" and develop a method for reintegrating the secondary model into the primary model. 

And for reintegrating, it may be possible, through an algorithm, to map the activation functions/pathway of the secondary model and then link that to the activation functions/pathways of the primary model.

As a final note.. thanks for coming to my Ted Talk; I probably have no idea what I'm even talking (or attempt to talk) about.

",/r/ArtificialInteligence/comments/1bj93jb/attempting_to_establish_a_hypothetical_framework/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
From Text to Video in Minutes: 7 Best AI Video Generators in 2024,Y-ve-tte,https://www.reddit.com/r/VideoEnhanceAI/comments/1bj8sfd/from_text_to_video_in_minutes_7_best_ai_video/,1,0,1710924165.0,1bj8sfd," 

Every time you visit YouTube, you encounter a surge of AI-related content. From tutorials on ""This AI Tool Creates Videos in Seconds!"" to insights like ""Monetizing YouTube with AI Influencers,"" the platform is abuzz with AI-driven discussions. An increasing number of video creators are sharing tutorials on leveraging AI for video creation, emphasizing how AI video generators can swiftly produce professional videos without significant costs or technical hurdles.

Projections from industry experts suggest that the revenue generated by AI video generators is set to soar from $423 million in 2022 to $1.5 billion in 2029. It's predicted that AI will play a pivotal role in completing up to 95% of videos in the near future. With this impending shift towards AI dominance in video creation, now is the ideal time to explore these tools and get ahead of the curve.

Without delay, let's delve into a comprehensive guide highlighting the **top 7 AI video generators of 2024**.[\>> Read More](https://www.avclabs.com/ai-tools/best-ai-video-generator.html?utm_source=reddit&utm_medium=referral&utm_campaign=Yvette20240301-0331)

&#x200B;

https://preview.redd.it/alya28wxbgpc1.jpg?width=800&format=pjpg&auto=webp&s=a4c3d37d2059bfd56949090e65e69fdb613eb853",/r/VideoEnhanceAI/comments/1bj8sfd/from_text_to_video_in_minutes_7_best_ai_video/,VideoEnhanceAI,r/VideoEnhanceAI,False,relevant,highly relevant
"[Discussion]Been thinking recently, there needs to be more standardized benchmarks.",The_Scout1255,https://www.reddit.com/r/comfyui/comments/1bj8mcc/discussionbeen_thinking_recently_there_needs_to/,4,16,1710923340.0,1bj8mcc,"I was thinking recently, people are doing projects like [this](https://www.reddit.com/r/StableDiffusion/comments/1bj1l1o/cyberpunk_2077_gameplay_using_a_ps1_lora/). 

My sys does not personally have the skills yet, but I was wondering if the  community would be interested in coming together to make benchmarks for stuff like this.

Ideas I had for benchmarks would be a comfyui workflow running all these at once, where you select the model, and add a concat for the negative prompt for the model, as well as positive prompt(For models like animagine xl 3.1 that work better with postpending positive prompts).

The specific ideas. 

* Tests should be agnostic of what version,  or have multiple premade areas for accepting SDXL, Lightning, Cascade, SVD, SD 1.5, ect models, and lorae?(Loras? Lorae sounds cooler)

* Video to Video (For stuff like that gameplay PS1 lora)

* Animatediff

* Image to image

* [RPG-DiffusionMaster](https://github.com/YangLing0818/RPG-DiffusionMaster) prompt adherence(Have any models grown to support it yet?)

* Text to image. EDIT: [v0.1](https://drive.google.com/uc?id=1D9zWeEgyhJLLXA_qACdyfV08u_Nq489s) of the benchmark for this

* How well it takes controlnets, and loras.

* Webcam to image

* Webcam to video

New:

* Controlnet adherence/quality SD 1.5

* Controlnet adherence/quality SDXL(Really important because its really hit, or miss rn).

* Controlnet adherence/quality SD Lightning 

* Controlnet lllite adherence/quality SDXL

* Video model text to video

* Video model image to video

* Video model video to Video

* Video Model text to image(Lmao, this test is a fun benchmark IMO)

* Segment model test

* controlnet preprocessor test

That's what We have been able to think of so far, any thoughts?

Edit: Added some more tests.

It really needs to have preset settings, nodes, and such only be modified if a model needs say 24 steps instead of 28, because variance in those settings will ruin any scientific accuracy of any benchmarks of quality, and more importantly speed(A lot of youtubers like Linus. and gamersnexus have mentioned bench-marking AI recently).",/r/comfyui/comments/1bj8mcc/discussionbeen_thinking_recently_there_needs_to/,comfyui,r/comfyui,False,relevant,highly relevant
"Sora “A 2d abstract japanese animation where drops of ink in water form into lifelike creatures that swim and interact with each other, creating an ethereal underwater world made entirely of flowing, merging colors”",PlzSir,https://v.redd.it/52eis5ysjfpc1,149,14,1710914705.0,1bj6mur,"Prompt: A 2d abstract japanese animation where drops of ink in water form into lifelike creatures that swim and interact with each other, creating an ethereal underwater world made entirely of flowing, merging colors

“This is two videos with the same prompt generated by our text-to-video model, Sora, without modification.”

Source: https://www.tiktok.com/t/ZPRTrYcpd/",/r/singularity/comments/1bj6mur/sora_a_2d_abstract_japanese_animation_where_drops/,singularity,r/singularity,True,not relevant,highly relevant
madewithSora,MadeWithSora,https://v.redd.it/93wgo1fdscpc1,1,0,1710881262.0,1biuumd,"A 2D abstract Japanese animation where drops of ink in water form into lifelike creatures that swim and interact with each other, creating an ethereal underwater world made entirely of flowing, merging colors.

This is two videos with the same prompt generated by our OpenAI’s text-to-video model, Sora, without modification.",/r/MadeWithSora/comments/1biuumd/madewithsora/,MadeWithSora,r/MadeWithSora,True,not relevant,highly relevant
The Impact of Sora on Film and Advertising: Navigating the Future Together,JamesDubec,https://www.reddit.com/r/AI_Video_Revolution/comments/1bij0vn/the_impact_of_sora_on_film_and_advertising/,1,0,1710851238.0,1bij0vn,"The release of **Sora by Open AI** has sparked significant conversation among my industry peers, from grips to directors. With this AI's advanced text-to-video capabilities, we're all questioning what the future holds for our roles. Should we be looking at new career paths? With the evolution toward AI, will live shoots still exist in 10 years?

I've been in the film and advertising industry for over 25 years. Throughout my career, I've witnessed and adapted to many changes, but the arrival of Sora seems to herald a transformation unlike any other. We've navigated through the challenges of COVID, adapted to remote shootings, and weathered much uncertainty. And now, Sora looms on the horizon, potentially overhauling the filmmaking process entirely.

My personal take is that change is inevitable, but I hope it arrives gradually. The ability to instantly generate and modify content to client specifications, including real-time A/B testing, could fundamentally alter our traditional approaches. I'm concerned about the future — any downturn could lead to significant layoffs and necessitate drastic career changes. The advertising industry is an immense machine, and I foresee significant transformations ahead.

After coping with COVID, remote productions, and ongoing uncertainties, confronting the potential realities Sora presents is yet another challenge. But it's one we must discuss openly.

I'm turning to this community to hear your thoughts. How do you envision the impact of AI advancements like Sora on our industry? With all we've faced and the changes looming, where do we stand?

Let’s share insights, express our concerns, and explore these new horizons together.

Looking forward to your valuable perspectives.",/r/AI_Video_Revolution/comments/1bij0vn/the_impact_of_sora_on_film_and_advertising/,AI_Video_Revolution,r/AI_Video_Revolution,False,relevant,highly relevant
Uses Of Sora || Dazonn Technologies,nishantsinghdazonn,https://www.reddit.com/r/SEOchats/comments/1bidos6/uses_of_sora_dazonn_technologies/,2,0,1710829568.0,1bidos6,"Sora can be used to create new videos or expand existing ones. It may also fill in video frames that are missing. They, like AI picture generation technologies, can generate photos without requiring any technical expertise. The Sora simplifies the process of creating films without the need for specific equipment or editing. Here are some important uses for sora:

* **Marketing and Advertising**

Making commercials, promotional films, and product demos has always been costly. With the help of its robust text-to-video technology, Sora, OpenAI lowers the cost.

* **Social Media**

Short videos are currently quite popular on Facebook, Instagram, YouTube, and TikTok.  Sora can create videos that are difficult or impossible to film. For example, whereas capturing a video of California in 2056 for a social media post would be technically tough, Sora makes it simple. 

* **Creating Prototypes And Visual Graphics**

Creating a prototype will be an interesting tool for swiftly demonstrating ideas for the final product. Designers can use AI to produce films of products before they are built, and filmmakers can use it to create mockups of sequences before filming them.",/r/SEOchats/comments/1bidos6/uses_of_sora_dazonn_technologies/,SEOchats,r/SEOchats,False,relevant,highly relevant
[2403.12042] Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation,ninjasaid13,https://arxiv.org/abs/2403.12042,1,0,1710818919.0,1biaovn,,/r/ninjasaid13/comments/1biaovn/240312042_exploring_pretrained_texttovideo/,ninjasaid13,r/ninjasaid13,False,not relevant,neutral
"OpenAI unveils powerful, creepy new text-to-video model - PC Gamer",NoDefinition5159,https://www.pcgamer.com/openai-sora-text-to-video-announcement/,1,1,1710774189.0,1bhswcu,,/r/AIPros/comments/1bhswcu/openai_unveils_powerful_creepy_new_texttovideo/,AIPros,r/AIPros,False,not relevant,highly relevant
"AI, AGI and ASI will ultimately take us to biochemical ways of suffering much less and enjoying much more.
",Georgeo57,https://www.reddit.com/r/agi/comments/1bhqxyn/ai_agi_and_asi_will_ultimately_take_us_to/,12,20,1710769120.0,1bhqxyn,"

after our survival needs are met, our most fundamental drive as humans is to seek pleasure and avoid pain. basically we are all striving to be as happy as we can be, and to suffer as little as possible.

because happiness is an emotional state, and emotions are contagious, it's not just our happiness that we are concerned with. the happier those around us are, the easier it is for us to feel happier.

you may have heard that ai just discovered 2 million materials. something that would have taken about 800 years without ai. you also may have heard that ai just discovered a brand new class of antibiotics. so the idea is that ai can supercharge biochemical research across the board, allowing us to do in one year what would have otherwise taken us perhaps 50 years or more to do.

after we're done being amazed by the text to video and speech to text capabilities that we've developed, and after we're done accelerating progress in virtually every domain of business, we will turn our attention to what we most want from life.

at that point we will fully realize and appreciate that we humans are essentially biochemical organisms. that's why medicines work so well. that's why so many of us use alcohol and caffeine and cannabis. what we ingest can influences our mind and emotional states in profound and very useful ways. and this is totally apart from what the heavy hitters like lsd, mdma and dmt can do.

basically i'm talking about how ais will soon take antidepressant and anti-anxiety drug research and development to a whole new level.

so imagine a new class of antidepressants that doesn't just take you from dissatisfied to mildly okay. imagine that these new superpowerful antidepressants bliss you out in a way you would have never dreamed possible. imagine that new superpowerful anti-anxiety agents make you more relaxed than some guru in the himalayas who has been meditating non-stop for 50 years.

whether it's a pill or some kind of genetic modification, it won't be long before ai engineers realize that the ultimate marketable products are products that directly and powerfully ramp up our happiness and make us all incredibly relaxed and peaceful.

we will continue to build amazing innovations like being able to create an entire full length feature film by simply telling an ai in a few paragraphs exactly what we want. but we will be doing all of that from an entirely new powerfully relaxed and joyful emotional  state. we will soon realize that the ultimate benefits of ai, agi and asi are a vastly happier and more peaceful world through safe and uber-effective biochemical means. once we are there, we will be enjoying whatever we do to degrees that were before unimaginable. 
",/r/agi/comments/1bhqxyn/ai_agi_and_asi_will_ultimately_take_us_to/,agi,r/agi,False,relevant,highly relevant
"AI, AGI and ASI will ultimately take us to biochemical ways of suffering much less and enjoying much more",Georgeo57,https://www.reddit.com/r/ArtificialNtelligence/comments/1bhqwuk/ai_agi_and_asi_will_ultimately_take_us_to/,3,0,1710769028.0,1bhqwuk,"

after our survival needs are met, our most fundamental drive as humans is to seek pleasure and avoid pain. basically we are all striving to be as happy as we can be, and to suffer as little as possible.

because happiness is an emotional state, and emotions are contagious, it's not just our happiness that we are concerned with. the happier those around us are, the easier it is for us to feel happier.

you may have heard that ai just discovered 2 million materials. something that would have taken about 800 years without ai. you also may have heard that ai just discovered a brand new class of antibiotics. so the idea is that ai can supercharge biochemical research across the board, allowing us to do in one year what would have otherwise taken us perhaps 50 years or more to do.

after we're done being amazed by the text to video and speech to text capabilities that we've developed, and after we're done accelerating progress in virtually every domain of business, we will turn our attention to what we most want from life.

at that point we will fully realize and appreciate that we humans are essentially biochemical organisms. that's why medicines work so well. that's why so many of us use alcohol and caffeine and cannabis. what we ingest can influences our mind and emotional states in profound and very useful ways. and this is totally apart from what the heavy hitters like lsd, mdma and dmt can do.

basically i'm talking about how ais will soon take antidepressant and anti-anxiety drug research and development to a whole new level.

so imagine a new class of antidepressants that doesn't just take you from dissatisfied to mildly okay. imagine that these new superpowerful antidepressants bliss you out in a way you would have never dreamed possible. imagine that new superpowerful anti-anxiety agents make you more relaxed than some guru in the himalayas who has been meditating non-stop for 50 years.

whether it's a pill or some kind of genetic modification, it won't be long before ai engineers realize that the ultimate marketable products are products that directly and powerfully ramp up our happiness and make us all incredibly relaxed and peaceful.

we will continue to build amazing innovations like being able to create an entire full length feature film by simply telling an ai in a few paragraphs exactly what we want. but we will be doing all of that from an entirely new powerfully relaxed and joyful emotional  state. we will soon realize that the ultimate benefits of ai, agi and asi are a vastly happier and more peaceful world through safe and uber-effective biochemical means. once we are there, we will be enjoying whatever we do to degrees that were before unimaginable. 
",/r/ArtificialNtelligence/comments/1bhqwuk/ai_agi_and_asi_will_ultimately_take_us_to/,ArtificialNtelligence,r/ArtificialNtelligence,False,relevant,highly relevant
"AI, AGI and ASI will ultimately take us to biochemical ways of suffering much less and enjoying much more",Georgeo57,https://www.reddit.com/r/ArtificialInteligence/comments/1bhqw21/ai_agi_and_asi_will_ultimately_take_us_to/,25,87,1710768968.0,1bhqw21,"

after our survival needs are met, our most fundamental drive as humans is to seek pleasure and avoid pain. basically we are all striving to be as happy as we can be, and to suffer as little as possible.

because happiness is an emotional state, and emotions are contagious, it's not just our happiness that we are concerned with. the happier those around us are, the easier it is for us to feel happier.

you may have heard that ai just discovered 2 million materials. something that would have taken about 800 years without ai. you also may have heard that ai just discovered a brand new class of antibiotics. so the idea is that ai can supercharge biochemical research across the board, allowing us to do in one year what would have otherwise taken us perhaps 50 years or more to do.

after we're done being amazed by the text to video and speech to text capabilities that we've developed, and after we're done accelerating progress in virtually every domain of business, we will turn our attention to what we most want from life.

at that point we will fully realize and appreciate that we humans are essentially biochemical organisms. that's why medicines work so well. that's why so many of us use alcohol and caffeine and cannabis. what we ingest can influences our mind and emotional states in profound and very useful ways. and this is totally apart from what the heavy hitters like lsd, mdma and dmt can do.

basically i'm talking about how ais will soon take antidepressant and anti-anxiety drug research and development to a whole new level.

so imagine a new class of antidepressants that doesn't just take you from dissatisfied to mildly okay. imagine that these new superpowerful antidepressants bliss you out in a way you would have never dreamed possible. imagine that new superpowerful anti-anxiety agents make you more relaxed than some guru in the himalayas who has been meditating non-stop for 50 years.

whether it's a pill or some kind of genetic modification, it won't be long before ai engineers realize that the ultimate marketable products are products that directly and powerfully ramp up our happiness and make us all incredibly relaxed and peaceful.

we will continue to build amazing innovations like being able to create an entire full length feature film by simply telling an ai in a few paragraphs exactly what we want. but we will be doing all of that from an entirely new powerfully relaxed and joyful emotional  state. we will soon realize that the ultimate benefits of ai, agi and asi are a vastly happier and more peaceful world through safe and uber-effective biochemical means. once we are there, we will be enjoying whatever we do to degrees that were before unimaginable. 
",/r/ArtificialInteligence/comments/1bhqw21/ai_agi_and_asi_will_ultimately_take_us_to/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
Calculating the random shape’s area takes us 2000 years until Newton while developing from text to video takes the AI only 1 year.,PhD3DP,https://www.reddit.com/r/Showerthoughts/comments/1bhqe9p/calculating_the_random_shapes_area_takes_us_2000/,0,1,1710767559.0,1bhqe9p,"From Archimedes to Newton, the calculus is finally invented ",/r/Showerthoughts/comments/1bhqe9p/calculating_the_random_shapes_area_takes_us_2000/,Showerthoughts,r/Showerthoughts,False,relevant,neutral
"AI, AGI and ASI will ultimately take us to biochemical ways of suffering much less and enjoying much more.",Georgeo57,https://www.reddit.com/r/world_changing_ideas/comments/1bhobab/ai_agi_and_asi_will_ultimately_take_us_to/,1,0,1710760873.0,1bhobab,"

after our survival needs are met, our most fundamental drive as humans is to seek pleasure and avoid pain. basically we are all striving to be as happy as we can be, and to suffer as little as possible.

because happiness is an emotional state, and emotions are contagious, it's not just our happiness that we are concerned with. the happier those around us are, the easier it is for us to feel happier.

you may have heard that ai just discovered 2 million materials. something that would have taken about 800 years without ai. you also may have heard that ai just discovered a brand new class of antibiotics. so the idea is that ai can supercharge biochemical research across the board, allowing us to do in one year what would have otherwise taken us perhaps 50 years or more to do.

after we're done being amazed by the text to video and speech to text capabilities that we've developed, and after we're done accelerating progress in virtually every domain of business, we will turn our attention to what we most want from life.

at that point we will fully realize and appreciate that we humans are essentially biochemical organisms. that's why medicines work so well. that's why so many of us use alcohol and caffeine and cannabis. what we ingest can influences our mind and emotional states in profound and very useful ways. and this is totally apart from what the heavy hitters like lsd, mdma and dmt can do.

basically i'm talking about how ais will soon take antidepressant and anti-anxiety drug research and development to a whole new level.

so imagine a new class of antidepressants that doesn't just take you from dissatisfied to mildly okay. imagine that these new superpowerful antidepressants bliss you out in a way you would have never dreamed possible. imagine that new superpowerful anti-anxiety agents make you more relaxed than some guru in the himalayas who has been meditating non-stop for 50 years.

whether it's a pill or some kind of genetic modification, it won't be long before ai engineers realize that the ultimate marketable products are products that directly and powerfully ramp up our happiness and make us all incredibly relaxed and peaceful.

we will continue to build amazing innovations like being able to create an entire full length feature film by simply telling an ai in a few paragraphs exactly what we want. but we will be doing all of that from an entirely new powerfully relaxed and joyful emotional  state. we will soon realize that the ultimate benefits of ai, agi and asi are a vastly happier and more peaceful world through safe and uber-effective biochemical means. once we are there, we will be enjoying whatever we do to degrees that were before unimaginable.",/r/world_changing_ideas/comments/1bhobab/ai_agi_and_asi_will_ultimately_take_us_to/,world_changing_ideas,r/world_changing_ideas,False,relevant,highly relevant
OpenAI’s Sora text-to-video generator will be publicly available later this year,NoDefinition5159,https://inboom.ai/openais-sora-text-to-video-generator-will-be-publicly-available-later-this-year/,1,0,1710693437.0,1bh1ycr,,/r/AIPros/comments/1bh1ycr/openais_sora_texttovideo_generator_will_be/,AIPros,r/AIPros,False,not relevant,neutral
Character Turnaround with img-to-video,ScarTarg,https://www.reddit.com/r/comfyui/comments/1bgwqlv/character_turnaround_with_imgtovideo/,0,0,1710679722.0,1bgwqlv,"Maybe this isn't possible yet, but I've wanted to use Stable Diffusion to automate the initial part of character modeling by generating at least a low resolution mesh. My goal is to create multi-view images of a character and then generate a 3d mesh from that using gaussian splatting or nerf. I've already tried many of the models from \[ComfyUI 3D Pack\]([https://github.com/MrForExample/ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)), but these don't generalize well to realistic humans. But then again I also wasn't able to run the best of these (CRM) because of hardware limitations, so maybe CRM solves my problem (?).

In any case, I wanted to find a workaround with my current hardware. I gave SVD a try, and it gave pretty good output, but it was only able to generate \~15° arc. And furthermore, I don't think you're able to control the camera in SVD (with say a controlnet or something). Another option would be AnimateDiff, which seems to have a lot more support for ControlNet (including pose and depth, and etc.), but AnimateDiff is text-to-video, and I'm looking for a img-to-video solution. 

So I wanted to extend this discussion with you guys. Do you guys have any suggestions? ",/r/comfyui/comments/1bgwqlv/character_turnaround_with_imgtovideo/,comfyui,r/comfyui,False,relevant,highly relevant
Mountains and Clouds in Runway Gen-2 (text-to-video) A.I. Free to download and use in your projects. Midjourney images used as starter images in Runway Gen-2 and then sequenced in Blender 3D software.,Floyd_Kelly,https://v.redd.it/06kbeig3vroc1,4,0,1710628116.0,1bghxhi,,/r/bigsleep/comments/1bghxhi/mountains_and_clouds_in_runway_gen2_texttovideo/,bigsleep,r/bigsleep,True,relevant,highly relevant
How OpenAI's text-to-video tool Sora could change science – and society - Nature.com,tw_bot,https://news.google.com/atom/articles/CBMiMmh0dHBzOi8vd3d3Lm5hdHVyZS5jb20vYXJ0aWNsZXMvZDQxNTg2LTAyNC0wMDY2MS0w0gEA?oc=5&hl=en-US&gl=US&ceid=US:en,1,0,1710625717.0,1bgh29m,,/r/tomorrowsworld/comments/1bgh29m/how_openais_texttovideo_tool_sora_could_change/,tomorrowsworld,r/tomorrowsworld,False,not relevant,neutral
"Long-Term Opportunity Available - At least 100 videos, potentially much more - $15-20 per video (negotiable based on experience and quality)",Present-World8444,https://www.reddit.com/r/VideoEditor_forhire/comments/1bggm7m/longterm_opportunity_available_at_least_100/,5,11,1710624508.0,1bggm7m,"**Project Details:**

* **Duration:** 5-minute videos
* **Scope:** Basic graphic design and thumbnail creation
* **Volume:** At least 100 videos, potentially more
* **Compensation:** $15-20 per video (negotiable based on experience and quality)

**Sample Request:** Before committing, we invite you to showcase your talent by creating a sample video in your unique style. Check out the linked video below (no audio required), and interpret it in your way:

\[Sample Video Link: [https://youtu.be/uY2IakQLz7Q](https://youtu.be/uY2IakQLz7Q)\]

Text-to-video creators proficient in **Hindi/Urdu**. If you're comfortable working with **religious content** in these languages, we'd love to hear from you! ",/r/VideoEditor_forhire/comments/1bggm7m/longterm_opportunity_available_at_least_100/,VideoEditor_forhire,r/VideoEditor_forhire,False,relevant,highly relevant
Video Without Cameras: Text-to-Video Tools and Trends,laomusicARTS,https://www.reddit.com/r/u_laomusicARTS/comments/1bg7k5i/video_without_cameras_texttovideo_tools_and_trends/,1,0,1710600020.0,1bg7k5i,"**LAOMUSIC ARTS 2024**

presents

&#x200B;

I just finished the course “**Video Without Cameras: Text-to-Video Tools and Trends**” by A*shley Kennedy!* 

&#x200B;

**#lao #music #laomusic #laomusicArts  #LAO #MUSIC #LAOMUSIC #LAOMUSIC\_ARTS #worstteacherever #artificialintelligence #AI #artificialintelligencefordesign #videogeneration**

&#x200B;

Check it out:

&#x200B;

[https://www.linkedin.com/learning/video-without-cameras-text-to-video-tools-and-trends/embracing-ai-to-create-engaging-videos?autoplay=true&resume=false](https://www.linkedin.com/learning/video-without-cameras-text-to-video-tools-and-trends/embracing-ai-to-create-engaging-videos?autoplay=true&resume=false)

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;",/r/u_laomusicARTS/comments/1bg7k5i/video_without_cameras_texttovideo_tools_and_trends/,u_laomusicARTS,u/laomusicARTS,False,not relevant,neutral
weekly roundup for week of 3/8-3/15,sub-roundup,https://www.reddit.com/r/editors/comments/1bg5k0t/weekly_roundup_for_week_of_38315/,5,0,1710594013.0,1bg5k0t,"### Most Upvoted Posts

1. [I'm an editor, not a motion graphics artist. (682 🡅)](/r/editors/comments/1b86dvb/im_an_editor_not_a_motion_graphics_artist/)
2. [I edited the Stunt Performers Tribute for last nights Oscar's... (377 🡅)](/r/editors/comments/1bchomr/i_edited_the_stunt_performers_tribute_for_last/)
3. [Won a 50k grant for my organization with an edit I did. (268 🡅)](/r/editors/comments/18ze3pp/won_a_50k_grant_for_my_organization_with_an_edit/)
4. [My first feature film edit is going to theaters (219 🡅)](/r/editors/comments/19a881c/my_first_feature_film_edit_is_going_to_theaters/)
5. [The film I edited last year 'Your Lucky Day' just came out on netflix! (207 🡅)](/r/editors/comments/1b96o21/the_film_i_edited_last_year_your_lucky_day_just/)

### Most Commented Posts

1. [Leaving the industry... (286 💬)](/r/editors/comments/1b2j78q/leaving_the_industry/)
2. [I'm an editor, not a motion graphics artist. (238 💬)](/r/editors/comments/1b86dvb/im_an_editor_not_a_motion_graphics_artist/)
3. [What's up with all the Adobe hate? (234 💬)](/r/editors/comments/1ajplgd/whats_up_with_all_the_adobe_hate/)
4. [OpenAI announces Sora today, introducing their photorealistic text-to-video product (228 💬)](/r/editors/comments/1arrmbi/openai_announces_sora_today_introducing_their/)
5. [Sora (202 💬)](/r/editors/comments/1asnrw8/sora/)

### Moderator Selections

1. [Cutting in Avid coming from Premiere FAST: What I learned and how did I do?](/r/editors/comments/1azuaz3/cutting_in_avid_coming_from_premiere_fast_what_i/)",/r/editors/comments/1bg5k0t/weekly_roundup_for_week_of_38315/,editors,r/editors,False,relevant,highly relevant
Sora public availability ,sansvidi,https://i.redd.it/lkxdvvqymjoc1.jpeg,411,106,1710528341.0,1bfl8qa,,/r/OpenAI/comments/1bfl8qa/sora_public_availability/,OpenAI,r/OpenAI,False,not relevant,neutral
"An overview of the VideoPoet model, which is capable of multitasking on a variety of video-centric inputs and outputs. The LLM can optionally take text as input to guide generation for text-to-video, image-to-video, stylization, and outpainting tasks.",TheNewBing,https://i.redd.it/gc2y4wfb7hoc1.png,1,1,1710498875.0,1bfavn1,,/r/GoogleVideoPoet/comments/1bfavn1/an_overview_of_the_videopoet_model_which_is/,GoogleVideoPoet,r/GoogleVideoPoet,False,relevant,highly relevant
Use AI to Animate Avatars. Text to Video. #ai4teachers #aiforteachers #teacherlife,AI4Teachers,https://youtu.be/bq2rmfN-KHw?si=1OF5oxd9ivqoTgBs,1,0,1710498816.0,1bfav3a,,/r/AI4Teachers/comments/1bfav3a/use_ai_to_animate_avatars_text_to_video/,AI4Teachers,r/AI4Teachers,False,relevant,neutral
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation,SunsetOneSix,https://www.reddit.com/r/computervision/comments/1bfaonc/sora_as_an_agi_world_model_a_complete_survey_on/,1,1,1710498133.0,1bfaonc,"**Paper**: [https://arxiv.org/abs/2403.05131](https://arxiv.org/abs/2403.05131)

**Abstract**:

>Text-to-video generation marks a significant frontier in the rapidly  evolving domain of generative AI, integrating advancements in  text-to-image synthesis, video captioning, and text-guided editing. This  survey critically examines the progression of text-to-video  technologies, focusing on the shift from traditional generative models  to the cutting-edge Sora model, highlighting developments in scalability  and generalizability. Distinguishing our analysis from prior works, we  offer an in-depth exploration of the technological frameworks and  evolutionary pathways of these models. Additionally, we delve into  practical applications and address ethical and technological challenges  such as the inability to perform multiple entity handling, comprehend  causal-effect learning, understand physical interaction, perceive object  scaling and proportioning, and combat object hallucination which is  also a long-standing problem in generative models. Our comprehensive  discussion covers the topic of enablement of text-to-video generation  models as human-assistive tools and world models, as well as eliciting  model's shortcomings and summarizing future improvement direction that  mainly centers around training datasets and evaluation metrics (both  automatic and human-centered). Aimed at both newcomers and seasoned  researchers, this survey seeks to catalyze further innovation and  discussion in the growing field of text-to-video generation, paving the  way for more reliable and practical generative artificial intelligence  technologies.",/r/computervision/comments/1bfaonc/sora_as_an_agi_world_model_a_complete_survey_on/,computervision,r/computervision,False,relevant,highly relevant
[R] Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation,SunsetOneSix,https://www.reddit.com/r/MachineLearning/comments/1bfaobn/r_sora_as_an_agi_world_model_a_complete_survey_on/,0,4,1710498099.0,1bfaobn,"**Paper**: [https://arxiv.org/abs/2403.05131](https://arxiv.org/abs/2403.05131)

**Abstract**:

>Text-to-video generation marks a significant frontier in the rapidly  evolving domain of generative AI, integrating advancements in  text-to-image synthesis, video captioning, and text-guided editing. This  survey critically examines the progression of text-to-video  technologies, focusing on the shift from traditional generative models  to the cutting-edge Sora model, highlighting developments in scalability  and generalizability. Distinguishing our analysis from prior works, we  offer an in-depth exploration of the technological frameworks and  evolutionary pathways of these models. Additionally, we delve into  practical applications and address ethical and technological challenges  such as the inability to perform multiple entity handling, comprehend  causal-effect learning, understand physical interaction, perceive object  scaling and proportioning, and combat object hallucination which is  also a long-standing problem in generative models. Our comprehensive  discussion covers the topic of enablement of text-to-video generation  models as human-assistive tools and world models, as well as eliciting  model's shortcomings and summarizing future improvement direction that  mainly centers around training datasets and evaluation metrics (both  automatic and human-centered). Aimed at both newcomers and seasoned  researchers, this survey seeks to catalyze further innovation and  discussion in the growing field of text-to-video generation, paving the  way for more reliable and practical generative artificial intelligence  technologies.",/r/MachineLearning/comments/1bfaobn/r_sora_as_an_agi_world_model_a_complete_survey_on/,MachineLearning,r/MachineLearning,False,relevant,highly relevant
"Oh nah ,society is cooked😭",NeedlaSpoon,https://i.redd.it/3fie755g4goc1.png,2,1,1710485790.0,1bf7vsx,,/r/BlueRyai/comments/1bf7vsx/oh_nah_society_is_cooked/,BlueRyai,r/BlueRyai,False,not relevant,less relevant
OpenAI Unveils Text-to-Video Model Called Sora - PYMNTS.com,NoDefinition5159,https://www.pymnts.com/news/artificial-intelligence/2024/openai-unveils-text-to-video-model-called-sora/,1,0,1710463511.0,1bf158w,,/r/AIPros/comments/1bf158w/openai_unveils_texttovideo_model_called_sora/,AIPros,r/AIPros,False,not relevant,neutral
[podcast] Mar/2024 AI & Tech News,gradientflow,https://www.reddit.com/r/GradientFlow/comments/1bevmbr/podcast_mar2024_ai_tech_news/,1,0,1710449303.0,1bevmbr,"From text-to-video models to Google's Gemini to synthetic data to augmenting LLMs with knowledge graphs. Explore techniques to improve foundation models, productionize AI, and leverage AMD's innovations

👉   [https://thedataexchange.media/2024-themes-and-trends-in-ai/](https://thedataexchange.media/2024-themes-and-trends-in-ai/)",/r/GradientFlow/comments/1bevmbr/podcast_mar2024_ai_tech_news/,GradientFlow,r/GradientFlow,False,relevant,highly relevant
"Two-minute Daily AI Update (Date: 03/14/2024): News from Google, Anthropic, OpenAI, Amazon, Microsoft, and more",RohitAkki,https://www.reddit.com/r/ArtificialInteligence/comments/1beptbh/twominute_daily_ai_update_date_03142024_news_from/,7,2,1710435029.0,1beptbh,"Continuing with the exercise of sharing an easily digestible and smaller version of the main updates of the day in the world of AI.  


* **DeepMind's AI agent that's a Jack of all games**  
\- DeepMind has developed SIMA, a generalist AI agent capable of understanding natural language instructions to complete tasks across various video game environments. Trained on nine different games in collaboration with eight game studios, SIMA can generalize learned skills to new gaming worlds without requiring access to game code or APIs.
* **Claude 3 Haiku is Anthropic's lightning-fast AI solution for enterprises**  
\- Anthropic has launched Claude 3 Haiku, their most cost-effective and fastest AI model yet, with strong vision capabilities and performance on industry benchmarks. It can process 21K tokens per second for prompts under 32K tokens and is well-suited for businesses requiring efficient analysis of large datasets and timely output generation.
* **OpenAI-powered ""Figure 01"" can chat, perceive, and complete tasks**  
\- Figure, in partnership with OpenAI, has created ""Figure 01,"" a robot that engages in full conversations, perceives its surroundings, and executes tasks based on verbal requests. The robot is connected to a multimodal AI model that processes conversation history and images to generate appropriate responses and behaviors.
* **Amazon streamlines product listing process with new AI tool**  
\- Amazon is introducing a new AI feature for sellers to quickly create product pages by pasting a link from their external website. The AI generates product descriptions and images based on the linked site's information, saving sellers time.
* **Microsoft to expand AI-powered cybersecurity tool availability from April 1**  
\- Microsoft is expanding the availability of its AI-powered cybersecurity tool, ""Security Copilot,"" from April 1, 2024. The tool helps with tasks like summarizing incidents, analyzing vulnerabilities, and sharing information. Microsoft plans to adopt a 'pay-as-you-go' pricing model to reduce entry barriers.
* **OpenAI’s Sora will be publicly available later this year**  
\- OpenAI will release Sora, its text-to-video AI tool, to the public later this year. Sora generates realistic video scenes from text prompts and may add audio capabilities in the future. OpenAI plans to offer Sora at a cost similar to DALL-E, its text-to-image model, and is developing features for users to edit the AI-generated content.
* **OpenAI partners with Le Monde, Prisa Media for news content in ChatGPT**  
\- OpenAI has announced partnerships with French newspaper Le Monde and Spanish media group Prisa Media to provide their news content to users of ChatGPT. The media companies see this as a way to ensure reliable information reaches AI users while safeguarding their journalistic integrity and revenue.
* **Icon's AI architect and 3D printing innovation reimagine homebuilding**  
\- Construction tech startup Icon has introduced an AI-powered architect, Vitruvius, that engages users in designing their dream homes, offering 3D-printed and conventional options. The company also debuted an advanced 3D printing robot called Phoenix and a low-carbon concrete mix as part of its mission to make homebuilding more affordable, efficient, and sustainable.

More detailed breakdown of these news and innovations in the [daily newsletter](https://theaiedge.substack.com/p/-sima-the-ai-agent-thats-jack-of-all-games).",/r/ArtificialInteligence/comments/1beptbh/twominute_daily_ai_update_date_03142024_news_from/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
"A Daily Chronicle of AI Innovations - March 14th 2024: 🎮 DeepMind's SIMA: The AI agent that's a Jack of all games ⚡ Claude 3 Haiku: Anthropic's lightning-fast AI solution for enterprises 🤖 OpenAI-powered Figure 01 can chat, perceive, and complete tasks 🎥 OpenAI’s Sora publicly available this year",enoumen,https://www.reddit.com/r/u_enoumen/comments/1ben96d/a_daily_chronicle_of_ai_innovations_march_14th/,1,0,1710428525.0,1ben96d," Read more in detailed at [https://enoumen.com/2024/03/01/a-daily-chronicle-of-ai-innovations-in-march-2024/](https://enoumen.com/2024/03/01/a-daily-chronicle-of-ai-innovations-in-march-2024/)

# Get our AI Dashboard at [https://readaloudforme.com](https://readaloudforme.com) 

### DeepMind's SIMA: The AI agent that's a Jack of all games

DeepMind has introduced SIMA (Scalable Instructable Multiworld Agent), a generalist AI agent that can understand and follow natural language instructions to complete tasks across video game environments. Trained in collaboration with eight game studios on nine different games, SIMA marks a significant milestone in game-playing AI by showing the ability to generalize learned skills to new gaming worlds without requiring access to game code or APIs.

&#x200B;

https://preview.redd.it/84v3rjlsdboc1.png?width=1070&format=png&auto=webp&s=1dee8f6a030521ff7041e30e8803f110b40ba113

(SIMA comprises pre-trained vision models, and a main model that includes a memory and outputs keyboard and mouse actions.)

SIMA was evaluated on 600 basic skills, including navigation, object interaction, and menu use. In tests, SIMA agents trained on multiple games significantly outperformed specialized agents trained on individual games. Notably, an agent trained on all but one game performed nearly as well on the unseen game as an agent specifically trained on it, showcasing SIMA's remarkable ability to generalize to new environments. 

***Why does this matter?***

SIMA's generalization ability using a single AI agent is a significant milestone in transfer learning. By showing that a multi-task trained agent can perform nearly as well on an unseen task as a specialized agent, SIMA paves the way for more versatile and scalable AI systems. This could lead to faster deployment of AI in real-world applications, as agents would require less task-specific training data and could adapt to new scenarios more quickly.

[**Source**](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments)

### Claude 3 Haiku: Anthropic's lightning-fast AI solution for enterprises

Anthropic has released Claude 3 Haiku, their fastest and most affordable AI model. With impressive vision capabilities and strong performance on industry benchmarks, Haiku is designed to tackle a wide range of enterprise applications. The model's speed - processing 21K tokens per second for prompts under 32K tokens - and cost-effective pricing model make it an attractive choice for businesses needing to analyze large datasets and generate timely outputs.

&#x200B;

https://preview.redd.it/mp8u2j6udboc1.png?width=1419&format=png&auto=webp&s=11c90c347090c64c921ce6c7a39837e22a2f42d8

In addition to its speed and affordability, Claude 3 Haiku prioritizes enterprise-grade security and robustness. The model is now available through Anthropic's API or on [claude.ai](http://claude.ai/) for Claude Pro subscribers.

***Why does this matter?***

Claude 3 Haiku sets a new benchmark for enterprise AI by offering high speed and cost-efficiency without compromising performance. This release will likely intensify competition among AI providers, making advanced AI solutions more accessible to businesses of all sizes. As more companies adopt models like Haiku, we expect a surge in AI-driven productivity and decision-making across industries.

[**Source**](https://www.anthropic.com/news/claude-3-haiku)

### OpenAI-powered ""Figure 01"" can chat, perceive, and complete tasks

Robotics company Figure, in collaboration with OpenAI, has developed a groundbreaking robot called ""Figure 01"" that can engage in full conversations, perceive its surroundings, plan actions, and execute tasks based on verbal requests, even those that are ambiguous or context-dependent. This is made possible by connecting the robot to a multimodal AI model trained by OpenAI, which integrates language and vision.

&#x200B;

https://preview.redd.it/7b20crqwdboc1.png?width=550&format=png&auto=webp&s=d35af396271193f693699c25546091a396efbd90

The AI model processes the robot's entire conversation history, including images, enabling it to generate appropriate verbal responses and select the most suitable learned behaviors to carry out given commands. The robot's actions are controlled by visuomotor transformers that convert visual input into precise physical movements. ""Figure 01"" successfully integrates natural language interaction, visual perception, reasoning, and dexterous manipulation in a single robot platform.

***Why does this matter?***

As robots become more adept at understanding and responding to human language, questions arise about their autonomy and potential impact on humanity. Collaboration between the robotics industry and AI policymakers is needed to establish regulations for the safe deployment of AI-powered robots. If deployed safely, these robots could become trusted partners, enhancing productivity, safety, and quality of life in various domains.

[**Source**](https://venturebeat.com/ai/openai-powers-a-robot-that-can-hand-people-food-pick-up-trash-put-away-dishes-and-more)

### What Else Is Happening in AI on March 14th, 2024

**📷** **Amazon streamlines product listing process with new AI tool**

Amazon is introducing a new AI feature for sellers to quickly create product pages by pasting a link from their external website. The AI generates product descriptions and images based on the linked site's information, saving sellers time. ([*Link*](https://www.theverge.com/2024/3/13/24099685/amazon-generative-ai-dtc-link-sellers))

**📷** **Microsoft to expand AI-powered cybersecurity tool availability from April 1**

Microsoft is expanding the availability of its AI-powered cybersecurity tool, ""Security Copilot,"" from April 1, 2024. The tool helps with tasks like summarizing incidents, analyzing vulnerabilities, and sharing information. Microsoft plans to adopt a 'pay-as-you-go' pricing model to reduce entry barriers. ([*Link*](https://www.reuters.com/technology/microsoft-expands-availability-its-ai-powered-cybersecurity-assistant-2024-03-13))

**📷** **OpenAI’s Sora will be publicly available later this year**

OpenAI will release Sora, its text-to-video AI tool, to the public later this year. Sora generates realistic video scenes from text prompts and may add audio capabilities in the future. OpenAI plans to offer Sora at a cost similar to DALL-E, its text-to-image model, and is developing features for users to edit the AI-generated content. ([*Link*](https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability))

**📷** **OpenAI partners with Le Monde, Prisa Media for news content in ChatGPT**

OpenAI has announced partnerships with French newspaper Le Monde and Spanish media group Prisa Media to provide their news content to users of ChatGPT. The media companies see this as a way to ensure reliable information reaches AI users while safeguarding their journalistic integrity and revenue. ([*Link*](https://www.reuters.com/technology/chatgpt-users-get-access-news-content-le-monde-prisa-media-2024-03-13))

**📷** **Icon's AI architect and 3D printing breakthroughs reimagine homebuilding**

Construction tech startup Icon has introduced an AI-powered architect, Vitruvius, that engages users in designing their dream homes, offering 3D-printed and conventional options. The company also debuted an advanced 3D printing robot called Phoenix and a low-carbon concrete mix as part of its mission to make homebuilding more affordable, efficient, and sustainable. ([*Link*](https://www.bloomberg.com/news/features/2024-03-12/this-ai-architect-will-design-your-climate-friendly-dream-home?srnd=technology-ai))

====

 

Experience the transformative capabilities of AI with ""[**Read Aloud For Me - AI Dashboard**](https://readaloudforme.com/)"" – your ultimate AI Dashboard and Hub. Seamlessly access a comprehensive suite of top-tier AI tools within a single app, meticulously crafted to enhance your efficiency and streamline your digital interactions. Now available on the web at [**readaloudforme.com**](https://readaloudforme.com/) and across popular app platforms including [**Apple**](https://apps.apple.com/ca/app/read-aloud-for-me-top-ai-gpts/id1598647453), [**Google**](https://play.google.com/store/apps/details?id=com.readaloudforme.app), and [**Microsoft**](https://apps.microsoft.com/detail/9pm03vfn90l8?hl=en-us&gl=CA), ""[**Read Aloud For Me - AI Dashboard**](https://readaloudforme.com/)"" places the future of AI at your fingertips, blending convenience with cutting-edge innovation. Whether for professional endeavors, educational pursuits, or personal enrichment, our app serves as your portal to the forefront of AI technologies. Embrace the future today by downloading our app and revolutionize your engagement with AI tools.

&#x200B;

https://preview.redd.it/wor04ip1eboc1.png?width=800&format=png&auto=webp&s=fa336e5e7797b27c8da0cc59b5a544306cc2cdae",/r/u_enoumen/comments/1ben96d/a_daily_chronicle_of_ai_innovations_march_14th/,u_enoumen,u/enoumen,False,relevant,highly relevant
What is the power of generative AI: A Deep Dive into Foundational Models?,AIBrilliance,https://www.reddit.com/r/u_AIBrilliance/comments/1beli8n/what_is_the_power_of_generative_ai_a_deep_dive/,1,0,1710423876.0,1beli8n," 

[Generative AI](https://www.aibrilliance.com/blog/the-power-of-generative-ai) encompasses deep learning techniques that create new data instances rather than simply predicting outcomes. It includes various models like text-to-text, text-to-image, and text-to-video, each generating content within learned distributions. Foundation models, like GPT, form the backbone of generative AI by integrating diverse data types into a single representation. These models excel in performance due to extensive training data and enhance productivity by minimizing the need for custom coding. However, they pose challenges such as significant computational requirements, biases, and limited trust in outputs. Despite these drawbacks, top products like GPT-4, Gemini, and Github Copilot showcase the transformative potential of generative AI. AIBrilliance offers courses like “[ChatGPT Demystified: A Beginner’s Crash Course](https://www.aibrilliance.com/course/chatgpt-course)” to delve deeper into this evolving field.

For more information, please visit the [AIBrilliance blog page](https://www.aibrilliance.com/blog).",/r/u_AIBrilliance/comments/1beli8n/what_is_the_power_of_generative_ai_a_deep_dive/,u_AIBrilliance,u/AIBrilliance,False,relevant,highly relevant
Sora playtime is very close,AIMick,https://www.reddit.com/r/AI_for_Business/comments/1beifzx/sora_playtime_is_very_close/,2,0,1710413656.0,1beifzx,"Interesting article , another game changer [https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability](https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability)",/r/AI_for_Business/comments/1beifzx/sora_playtime_is_very_close/,AI_for_Business,r/AI_for_Business,False,not relevant,highly relevant
OpenAI's Sora Text-to-Video Generative AI Will Go Public This Year:,ReviewSpaceInfo,https://www.reddit.com/r/u_ReviewSpaceInfo/comments/1behb1i/openais_sora_texttovideo_generative_ai_will_go/,1,0,1710409069.0,1behb1i,"[https://www.reviewspace.info/openai-s-sora-text-to-video-generative-ai-will-go-public-this-year](https://www.reviewspace.info/openai-s-sora-text-to-video-generative-ai-will-go-public-this-year)

\#OpenAI #Sora #TextToVideo #AI #GenerativeAI #VideoEditing #ArtificialIntelligence #TechnologyNews",/r/u_ReviewSpaceInfo/comments/1behb1i/openais_sora_texttovideo_generative_ai_will_go/,u_ReviewSpaceInfo,u/ReviewSpaceInfo,False,not relevant,neutral
Stay Tuned!,Left-Plant2717,https://i.redd.it/xzz24jk585oc1.jpeg,1,0,1710387356.0,1bebpzv,,/r/AI_Eritrea/comments/1bebpzv/stay_tuned/,AI_Eritrea,r/AI_Eritrea,False,not relevant,neutral
How OpenAI's text-to-video tool Sora could change science – and society,Youarethebigbang,https://www.nature.com/articles/d41586-024-00661-0,1,0,1710370682.0,1be5mhe,,/r/AI_Watch/comments/1be5mhe/how_openais_texttovideo_tool_sora_could_change/,AI_Watch,r/AI_Watch,False,not relevant,neutral
"RPG AI Tools Mega-Discussion Thread (Share Tools, Ask for Advice, etc.)",PaulBellow,https://www.reddit.com/r/dndai/comments/1be4zkb/rpg_ai_tools_megadiscussion_thread_share_tools/,34,22,1710369116.0,1be4zkb,"Instead of random, hey how do I do this posts, I thought we could start one (or many) to kinda collect all the best tools (not mine! small smile...)

Seriously, though, there's a lot of great stuff coming out, including text to video.

Feel free to share, ask, answer, and chill /r/dndai style.

&#x200B;

https://preview.redd.it/rtocynseh6oc1.jpg?width=1024&format=pjpg&auto=webp&s=f6143551db13d5937fdbfec5d7c6f36283453110",/r/dndai/comments/1be4zkb/rpg_ai_tools_megadiscussion_thread_share_tools/,dndai,r/dndai,False,relevant,highly relevant
Openai CTO says Sora will be available by “later this year”,Ok_Perspective_8418,https://www.reddit.com/r/ArtistHate/comments/1bdwwfy/openai_cto_says_sora_will_be_available_by_later/,32,17,1710349994.0,1bdwwfy,"https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability

This article says that open ai CTO says Sora will be available by the end of the year maybe even “a few months” . Someone just posted yesterday employees of openai on a marquees browne podcast saying it’s not coming “anytime soon”. Obviously I hope they are the ones with the accurate info , but this article is making me think otherwise. Anyone have any actual idea on what openai has said about the release and why there is the discrepancy? ",/r/ArtistHate/comments/1bdwwfy/openai_cto_says_sora_will_be_available_by_later/,ArtistHate,r/ArtistHate,False,not relevant,highly relevant
"OpenAI lancera cette année ""Sora"", son générateur de texte en vidéo, au public, envisageant d'ajouter de l'audio et la possibilité d'éditer les vidéos",romain34230,https://www.reddit.com/r/actutech/comments/1bds1i0/openai_lancera_cette_année_sora_son_générateur_de/,1,1,1710338078.0,1bds1i0,"Malgré le flou sur les données d'entraînement, le modèle utilisera des contenus de Shutterstock et visera un coût similaire à DALL-E, tout en intégrant des mesures contre la désinformation. [https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability](https://www.theverge.com/2024/3/13/24099402/openai-text-to-video-ai-sora-public-availability)

https://preview.redd.it/nio2crm6x3oc1.png?width=1080&format=png&auto=webp&s=ab3d7d5bda886b5248a7d7655a099802ae4f03ac",/r/actutech/comments/1bds1i0/openai_lancera_cette_année_sora_son_générateur_de/,actutech,r/actutech,False,relevant,less relevant
"Paper ""Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"" (paper not from OpenAI)",Wiskkey,https://www.reddit.com/r/SoraAi/comments/1bdisnc/paper_sora_a_review_on_background_technology/,10,0,1710304532.0,1bdisnc,"[Paper](https://arxiv.org/abs/2402.17177).

Abstract:

>Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this ""world simulator"". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.",/r/SoraAi/comments/1bdisnc/paper_sora_a_review_on_background_technology/,SoraAi,r/SoraAi,False,relevant,highly relevant
Why the impact of OpenAI's groundbreaking text-to-video tool Sora will be huge | DW News,MarshallBrain,https://www.youtube.com/watch?v=-4bqF2LY3rw,0,0,1710275743.0,1bd7t7m,,/r/2ndIntelligentSpecies/comments/1bd7t7m/why_the_impact_of_openais_groundbreaking/,2ndIntelligentSpecies,r/2ndIntelligentSpecies,False,not relevant,highly relevant
PORA AI Launches as Groundbreaking Memecoin with SORA AI Capabilities,maquinas501,https://www.reddit.com/r/technology_press/comments/1bcls06/pora_ai_launches_as_groundbreaking_memecoin_with/,0,1,1710209712.0,1bcls06,"PORA AI aims to bring SORA AI capabilities to the masses, empowering its community with text-to-video creation tools directly within the Telegram platform.

The launch of PORA AI is important as it introduces a memecoin with real utility, providing access to SORA AI text-to-video creation tools directly within the Telegram platform. This empowers the community and allows them to become part of a dynamic community, realizing dreams of widespread AI accessibility. The coin's features and community involvement make it an innovative and impactful addition to the cryptocurrency landscape. 

Read More https://newsramp.com/curated-news/pora-ai-launches-as-groundbreaking-memecoin-with-sora-ai-capabilities/98cd2bfaa6ca94dfdfe093252025435f",/r/technology_press/comments/1bcls06/pora_ai_launches_as_groundbreaking_memecoin_with/,technology_press,r/technology_press,False,relevant,highly relevant
"Everything to Know About OpenAI’s New Text-to-Video Generator, Sora",marketinggenie01,https://www.marketinggenie.ca/blogs/b/everything-to-know-about-openais-new-text-to-video-generator-sora,1,0,1710195360.0,1bcg6d5,,/r/u_marketinggenie01/comments/1bcg6d5/everything_to_know_about_openais_new_texttovideo/,u_marketinggenie01,u/marketinggenie01,False,relevant,neutral
Secret language // story collection,el_magyar,https://www.reddit.com/r/serbia/comments/1bc9p7w/secret_language_story_collection/,3,2,1710180213.0,1bc9p7w,"Dear bredits,

We're looking for stories and audio-visual material that capture  life in Kosovo and Serbia from 1980 to 2010. This includes memories from  young people who grew up during this time and their parents, who often  navigated life using both Albanian and Serbian languages.

We're on a mission to collect stories from people of all ages in  Serbia and Kosovo. We're especially keen on tales that are funny, witty,  or just downright clever - stories that show how people have shared  information without others catching on.

Our project is all about capturing the magic of secret languages -  those hidden codes and inside jokes that connect us in unique ways.  Whether it's the special language your family uses to keep secrets from  the kids or the clever codes you and your friends invented, we want to  hear about it!

You can submit your stories in various artistic formats, from audio  recordings and written texts to video interviews, comics, animations,  memes, screenshots, art drawings, and music...

All the stories we collect will be carefully curated into a special  archive and published in a book called ""Secret Language,"" showcasing the  richness and diversity of your stories.

[LINK](https://docs.google.com/forms/d/e/1FAIpQLSeYM3VQTi_IcI5OBfMJsvlgheOkz0Jfzg9MaBDN5kQBqo_SEw/viewform) for submissions.",/r/serbia/comments/1bc9p7w/secret_language_story_collection/,serbia,r/serbia,False,relevant,highly relevant
"Regarding """"AI generating stuff via"" ""text-to-""video &/or image"""""", is it possible for such to be ""configured to be limited to only being able to be kid-friendly"" (config: always auto-delete ""not kid-friendly result"" before it's in viewable format)?",Jeff_Chileno,https://www.reddit.com/r/QuestionEverythingNow/comments/1bc7kmd/regarding_ai_generating_stuff_via_texttovideo_or/,1,0,1710175123.0,1bc7kmd,"With that ""configuration example"" in the parenthesis, the """"AI generating stuff via"" ""text-to-""video &/or image"""""" would seemingly appear to be kid-friendly, but that configuration only filters/censors the not kid-friendly stuff, which does make that ""image and/or video"" generator kid-friendly to a sufficient extent, but that generator's ""not kid-friendliness"" is actually still ""there but always being made not viewable"".",/r/QuestionEverythingNow/comments/1bc7kmd/regarding_ai_generating_stuff_via_texttovideo_or/,QuestionEverythingNow,r/QuestionEverythingNow,False,relevant,less relevant
"Regarding """"AI generating stuff via"" ""text-to-""video &/or image"""""", is it possible for such to be ""configured to be limited to only being able to be kid-friendly"" (config: always auto-delete ""not kid-friendly result"" before it's in viewable format)?",Jeff_Chileno,https://www.reddit.com/r/QuestionAnswerCentral/comments/1bc7k5n/regarding_ai_generating_stuff_via_texttovideo_or/,1,0,1710175092.0,1bc7k5n,"With that ""configuration example"" in the parenthesis, the """"AI generating stuff via"" ""text-to-""video &/or image"""""" would seemingly appear to be kid-friendly, but that configuration only filters/censors the not kid-friendly stuff, which does make that ""image and/or video"" generator kid-friendly to a sufficient extent, but that generator's ""not kid-friendliness"" is actually still ""there but always being made not viewable"".",/r/QuestionAnswerCentral/comments/1bc7k5n/regarding_ai_generating_stuff_via_texttovideo_or/,QuestionAnswerCentral,r/QuestionAnswerCentral,False,relevant,less relevant
How China Is Building AI: Chinese Researchers Forge Real AI Scientists Era,DumbMoneyMedia,https://www.reddit.com/r/aidailynewsupdates/comments/1bc7ezk/how_china_is_building_ai_chinese_researchers/,1,0,1710174740.0,1bc7ezk," 

# How China Is Building AI: Chinese Researchers Forge Real AI Scientists Era

As I delve into the latest advancements in artificial intelligence, it's impossible not to marvel at the efforts of **Chinese researchers** who are spearheading the creation of 'real AI scientists.' By developing innovative frameworks in machine learning, these scientists are not only achieving a monumental AI breakthrough but are also shaping the course of **artificial intelligence innovation**. This bold stride is emblematic of a deep commitment to advancing the capabilities of AI, with the goal to revolutionize how we approach and solve scientific challenges. Indeed, these trailblazing achievements echo [China's dedication](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117) to establishing itself as a beacon of technological prowess in the global arena.

[Chinese researchers to create ‘real AI scientists’](https://reddit.com/link/1bc7ezk/video/tbhwoth6fqnc1/player)

### Key Takeaways

&#x200B;

* **Chinese researchers** are at the forefront of creating 'real AI scientists' that can tackle scientific problems autonomously.
* The concept of 'informed machine learning' is critical to the development of AI that understands and incorporates fundamental scientific principles.
* Improving AI models to reflect a more profound comprehension of the world represents a significant **AI breakthrough development**.
* Advanced frameworks being developed could enable AI systems to autonomously identify and apply rules derived from data.
* The progress highlights a new chapter in **artificial intelligence innovation**, showcasing globally significant **Chinese AI scientist achievements.**

## The Emergence of Informed Machine Learning

As we delve into the realm of artificial intelligence, the term 'informed machine learning' stands out as a significant game changer. This **advanced technology research** imbues machine learning models with a foundational understanding of the world—not just data, but the very principles that govern it. Imagine AI that doesn't just recognize patterns but understands the laws of physics that shape those patterns. Pioneered by **Chinese researchers**, this initiative is groundbreaking in **AI technology advancement**.

Let me provide a clearer picture. Think of **informed machine learning** as an experienced scientist compared to a diligent student. Where the student has learned through observation and replication, the scientist understands the underlying theories and can use this informed perspective to make sophisticated judgments. This is the promise of informed machine learning—a commitment to integrating both empirical data and established scientific knowledge right at the infancy of an AI model's learning process.

### Understanding the basics of informed machine learning

**Machine learning research** has traditionally relied heavily on vast pools of data, allowing algorithms to learn directly from patterns detected in that data. However, **informed machine learning** injects a much-needed dose of prior knowledge into these algorithms, creating a blend of data-driven intuition and rule-based reasoning. It's like empowering AI with a built-in science textbook, allowing it to apply universal truths to its predictions. This synergy propels **machine learning research** into new territories, fostering AI systems that are not only smarter but more attuned to the complexities of the real world.

### Significance of data and prior knowledge synergy in AI

As we strive for **AI technology advancement**, the integration of prior knowledge into machine learning systems is more than an impressive technical feat. It's a foundational change in how we conceive of AI's role in **advanced technology research.** By prioritizing **informed machine learning**, we're setting the stage for the creation of AI scientists capable of reasoning and innovating, revealing deeper insights across various scientific endeavors.

https://preview.redd.it/og6x9kjseqnc1.png?width=806&format=png&auto=webp&s=2205abe75dd3f073e592f14dff01af4e863db4c0

In summary, the evolution of informed machine learning marks a vibrant era in **machine learning research**. It elevates AI from a tool of computation to a potential partner in discovery, one that is informed by the intricacies of the environment it seeks to understand and emulate. This, I believe, is not just an advancement in technology—it's a leap towards a future where AI and human ingenuity are deeply intertwined.

## Deep Learning Revolution: Advancements and Limitations

The **deep learning revolution** has profoundly impacted the way we approach and understand scientific data. My exploration into this fascinating world has revealed the sheer magnitude of how *cutting-edge AI technology* is redefining research across a broad spectrum of disciplines. These tools have become invaluable in extracting nuanced relationships from complex datasets, fostering an environment ripe for **machine learning innovations**. However, the path to AI perfection is strewn with substantial challenges.

https://preview.redd.it/1udtdxqmeqnc1.png?width=1344&format=png&auto=webp&s=be2b047c432535bda2514a5c35f1c1cf54d34941

Take, for example, **OpenAI**'s Sora. This text-to-video model is a testament to the advanced capabilities we've achieved, offering strikingly realistic visualizations. Despite the applause it garners for its depictions, it is also emblematic of a significant hurdle: the struggle to simulate real-world physics with fidelity. Sora's training on visual data allows for impressive image and video generation, but what happens when it encounters the complex rules of gravity? The result is akin to sophisticated animation, rather than an accurate simulation.

It is here that we encounter the essential need for AI models to evolve beyond pattern-recognition marvels to entities that can actually 'understand' and 'apply' the physical laws they aim to represent. The aim is to transition from animations that dazzle the eyes to simulations that resonate with the truths of our universe.

*Invariably, this realization is propelling advancements in the deep learning domain, urging developers and researchers to push the boundaries further. It instigates a call to action within the AI community to address these limitations, ensuring that the resultant technology not only mimics reality but also aligns with its fundamental principles.*

https://preview.redd.it/mfy68kroeqnc1.png?width=783&format=png&auto=webp&s=a872a012455fd81e30af73e19c15f5db33ccab5e

In closing, the **deep learning revolution** is evolving, marked by a perpetual advancement towards machines that not only learn but also comprehend. While the journey is marked by trials and tribulations, the relentless pursuit of integrating real-world logic into AI promises to unlock a new chapter of **machine learning innovations** that will define the future of *cutting-edge AI technology*.

## Chinese researchers to create ‘real AI scientists’, ‘informed machine learning'

The frontier of **artificial intelligence development** is witnessing a significant evolution with the advent of 'informed machine learning.' At the vanguard of this revolution stand the efforts of **real AI scientists** from Peking University and the Eastern Institute of Technology. Their collaborative endeavor is not just a technical upgrade, it's a paradigm shift aimed at equipping AI with a nuanced understanding of natural laws.

In my quest to uncover the intricacies of their work, I've come to appreciate the profound impact this technological leap could have. By integrating physics and logic into artificial constructs, **technology research collaboration** in China is poised to create AI scientists that can simulate and solve real-world problems with unprecedented realism and accuracy.

### Incorporating physics into AI for real-world simulations

Envision **machine learning research** where models not only predict outcomes but also ground those predictions in physical reality. This is the goal of informed machine learning: to embed into AI systems a predefined comprehension of physics that guides their learning process. Such an approach heralds a new breed of AI capable of far more than impressive computational tasks; *they aspire to understand the very fabric of the universe.*

[ As a fast-growing silver producer, Kuya Silver's collaboration with tech firms integrates AI in mineral exploration, enhancing the technology supply chain.  ](https://preview.redd.it/co3xd4izeqnc1.png?width=1378&format=png&auto=webp&s=e9f78eb871000afae03186f7837435e78fc6c41f)

""[Kuya Silver](https://www.kuyasilver.com/) is expanding its operations to new territories, reinforcing its role in the tech industry's supply chain and ensuring resources for GPU production. Their strategic growth not only increases their mining footprint but also secures essential materials for technology and AI advancements. Kuya's efforts are vital for the tech industry's rapid expansion."" 

### Challenges in integrating comprehensive knowledge into AI

Ironically, the pathway to creating these informed entities is fraught with hurdles. The introduction of extensive scientific knowledge and rules into AI models can potentially lead to information overload and system failure. Balancing the scales between raw data perception and the intricacies of human knowledge is a conundrum that these **real AI scientists** are painstakingly trying to solve.

https://preview.redd.it/jqgs1zoueqnc1.png?width=793&format=png&auto=webp&s=cdff6d0022f599330449b649f5236b30dc1d3e2c

The odyssey to equip machine learning algorithms with a deeper understanding of physical laws is not just remarkable — it's a stepping stone towards robust **artificial intelligence development**. As I delve further into the methods and outcomes of these collaborative projects, I remain ever-curious about the impact they will have on the technological landscape. One thing is abundantly clear: the move towards creating 'real AI scientists' through informed machine learning is a transformative venture that promises to redefine our relationship with intelligent machines.

## Tackling the Challenge: A New Framework for AI Development

In my quest to understand the latest developments in AI, I've discovered that Chinese researchers have made significant strides by introducing a **new framework for AI development** that meticulously evaluates the relevance of various rules and knowledge in machine learning models. This innovative approach underscores their contribution to *AI research breakthroughs* and paves the way for more sophisticated *machine learning research*.

https://preview.redd.it/bb0zw9speqnc1.png?width=1344&format=png&auto=webp&s=5ced71e1e3e2d57474076d1aea8531b7b3aff111

This **new framework** is pivotal for tuning the balance between raw data-driven machine learning and the integrated knowledge that embodies the principles of the objective world. I've learned that by calculating 'rule importance,' researchers are able to determine which laws of physics, mathematical logic, or other prior knowledge to embed in the AI to enhance its predictive capabilities and ensure its relevance in practical applications.

During my review, it became clear that without such frameworks, AI models often risk collapsing under the weight of too many rules or can become too abstract, losing their predictive power. However, *innovative machine learning methods* employed within this framework are designed to withstand these challenges, enabling the creation of more reliable and efficient AI models.

https://preview.redd.it/q4cm8aeweqnc1.png?width=792&format=png&auto=webp&s=faf8aa066a1827a234ad55d473fffc450f451562

As I reflect on the potential of this framework, it's clear that the collective efforts in AI research are taking us closer to having AI act not just as calculators but as 'real AI scientists' capable of advancing experimentation and solving complex scientific problems. These advancements in developing a *new framework for AI development* are not just commendable but essential for the next wave of *AI research breakthroughs*, signifying a major milestone in the history of *innovative machine learning methods*.

## Future of AI Research: Autonomy in Scientific Problem-Solving

As we stand on the precipice of a new era in *advanced technology research*, I find myself fascinated by the concept of fully **autonomous AI**. Chinese researchers, in their undying quest for innovation, have pioneered frameworks that could transform these intelligent systems into independent problem-solvers, capable of navigating the complexities of science without human input.

This approach to *artificial intelligence development* not only streamlines the process of discovery but also heralds a future where AI systems could operate with little to no oversight, potentially solving some of the most enigmatic scientific problems of our time.

### The role of rules and knowledge evaluation in AI models

The inception of these **cutting-edge AI technologies** is made possible through the critical evaluation of rules and knowledge within AI models. The new framework introduced by researchers meticulously assesses which scientific laws and logical principles must be programmed into AI, ensuring a robust knowledge base that can guide problem-solving.

The idea is to marry the vast data that AI can process with a foundational understanding of scientific principles, creating a synergy that can yield unprecedented precision in predictions and simulations. This marriage is proving to be an integral piece of the puzzle in the grand scheme of **artificial intelligence development**.

### Fostering AI models to identify rules without human intervention

Another visionary leap in this field is the effort to enable *autonomous AI* to identify and apply rules and knowledge directly from data without the need for human instruction. As I muse over the potential implications, it's clear that fostering AI models to function without constant human intervention is not just groundbreaking—it's evolutionary.

By leveraging **advanced technology research**, we are witness to the birth of AI systems that are progressively learning to 'think' and 'reason' with a level of autonomy that was previously the exclusive domain of human scientists. I see this as heralding an age where AI can uncover new scientific knowledge, analyze data with an unprecedented level of sophistication, and perhaps, one day, make discoveries that have eluded human minds.

In essence, the trajectory of *AI research breakthroughs* points to a future where **autonomous AI** is more than a tool—it becomes a collaborator in the uncharted territory of science. It's a compelling chapter in our ongoing technological odyssey, and I eagerly await the marvels this **cutting-edge AI technology** will bring.

## Conclusion

As the discourse on the potential of intelligent machines reaches new heights, my exploration into the endeavors of **Chinese researchers** reveals a compelling narrative of *artificial intelligence innovation*. The development of 'real AI scientists' through informed machine learning represents not just an incremental step but a quantum leap in the evolution of AI. It's a testament to the determined pursuit of *AI breakthrough development* emanating from China, as they inject machine learning models with both empirical data and the immutable laws of the physical world. This synergetic approach signals a remarkable transit from current capabilities to an enhanced paradigm where AI significantly contributes to scientific discovery and problem-solving.

The introduction of a new framework for evaluating and integrating diverse knowledge into AI demonstrates an acute understanding of the complexities involved in *advanced technology research*. It's a strategic move that sets a new benchmark for efficiency and sophistication in AI systems. This burgeoning field is redefining boundaries, demonstrating the transformative impact of informed machine learning — a true reflection of Chinese researchers' commitment to pushing the envelope in technological excellence.

In sum, the **advanced technology research** led by these pioneers ushers us into an era where AI's role extends beyond assistance to active scientific exploration and innovation. My reflection on the progress made shows a bold stride towards an intelligent future, where machine learning transcends data analysis to become a cornerstone in the edifice of scientific advancement. These strides in *artificial intelligence innovation* are not only impressive in their own right; they also invite global collaboration in pursuit of knowledge — a herald of the expansive potential of AI in the coming decades.",/r/aidailynewsupdates/comments/1bc7ezk/how_china_is_building_ai_chinese_researchers/,aidailynewsupdates,r/aidailynewsupdates,False,relevant,highly relevant
[2403.05131] Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation,ninjasaid13,https://arxiv.org/abs/2403.05131,1,0,1710120738.0,1bbr6mx,,/r/ninjasaid13/comments/1bbr6mx/240305131_sora_as_an_agi_world_model_a_complete/,ninjasaid13,r/ninjasaid13,False,not relevant,neutral
China set to narrow AI gap with firm efforts,highsktan,https://www.reddit.com/r/u_highsktan/comments/1bbq9du/china_set_to_narrow_ai_gap_with_firm_efforts/,1,0,1710118097.0,1bbq9du,"During the ongoing two sessions, the Chinese government has unequivocally put the development of the domestic artificial intelligence industry high on its agenda — and this should help allay any lingering concerns that the country is lagging behind advanced economies in key technologies, experts said.

The latest concerns arose in February when US firm OpenAI, which had created a sensation with ChatGPT, launched a text-to-video model called Sora, sending ripples through China's AI industry. Reactions ranged from admiration and appreciation to ""AI anxiety"".

Why weren't groundbreaking technologies like ChatGPT and Sora emerging in China? Is the AI gap between China and the United States widening? Has China lost the tech plot? Has its innovation drive lost its edge? Questions like that have stoked anxiety among both netizens and industry experts alike.

It is natural to have such anxiety, but what it indicates really is the underlying sense of urgency to catch up with huge changes brought by cutting-edge technologies, said Shen Hao, deputy chief engineer of the Shanghai Artificial Intelligence Research Institute.

Industry insiders believe Sora — it allows users to create photo-realistic videos up to a minute-long, all based on prompts they have written — will actually end up narrowing the AI gap between China and the US. For, Chinese companies will likely soon ramp up their efforts to make similar products, given their unique advantages.",/r/u_highsktan/comments/1bbq9du/china_set_to_narrow_ai_gap_with_firm_efforts/,u_highsktan,u/highsktan,False,relevant,highly relevant
StableSwarmUI Beta!,mcmonkey4eva,https://www.reddit.com/r/StableDiffusion/comments/1bbgsl8/stableswarmui_beta/,366,188,1710094247.0,1bbgsl8,"&#x200B;

https://preview.redd.it/hjjv5uxsljnc1.png?width=1920&format=png&auto=webp&s=f995c2b154d4378b075aee39716b882a65ddebc6

[StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI) is now in **Beta** status with [Release 0.6.1](https://github.com/Stability-AI/StableSwarmUI/releases/tag/0.6.1-Beta)! 100% free, local, customizable, powerful.  


""Beta status"" means I now feel confident saying it's one of the best UIs out there for the majority of users. It also means that swarm is now fully free-and-open-source for everyone under the MIT license!

Beginner users will love to hear that it *literally installs itself!* No futsing with python packages, just run the installer and select your preferences in the UI that pops up! It can even download your first model for you if you want.  
On top of that, any non-superpros will be quite happy with every single parameter having attached documentation, just click that ""?"" icon to learn about a parameter and what values you should use.

https://preview.redd.it/c5tzrcj9mjnc1.png?width=546&format=png&auto=webp&s=0d1ad4a0e652ed6803c9053a0fd78e8f81f5c7c7

*Also* all the parameters are pretty good ones out-of-the-box. In fact the defaults might actually be better than other workflows out there, as it even auto-customizes the deep internal values like sigma-max (for SVD), or per-prompt resolution conditioning (for SDXL) that most people don't bother figuring out how to set at all.  


If you're less experienced but looking to become a pro SD user? Great news - Swarm integrates ComfyUI as its backend (endorsed by comfy himself!), with the ability to modify comfy workflows at will, and even take any generation from the main tab and hit ""Import"" to import the easy-mode params to a comfy workflow and see how it works inside.  


Comfy noodle pros, this is *also* the UI for you! With integrated workflow saver/browser, the ability to import your custom workflows to the friendlier main UI, the ability to generate large grids or use multiple GPUs, all available out-of-the-box in Swarm beta.

https://i.redd.it/jd6nfiszmjnc1.gif

And if  you're the type of artist that likes to bust out your graphics tablet and spend your time really perfecting your image -- well, I'm so sorry about my mouse-drawing attempt in the gif below but hopefully you can see the idea here, heh. Integrated image editor suite with layers and masks and etc. and regional prompting and live preview support and etc.

https://i.redd.it/o7ag9z88njnc1.gif

(\*Note: image editor is not as far developed yet as other features, still a fair bit of jank to it)  


Those are just some of the fun points above, there's more features than I can list... I'll give you a bit of a list anyway:  


\- Day 1 support for new models, like Cascade or the upcoming SD3.

  
\- native SVD video generation support, including text-to-video

\- full native refiner support allowing different model classes (eg XL base and v1 refiner or whatever else)

\- Native advanced infinite-axis grid generator tool

\- Easy aspect ratio and resolution selection. No more fiddling that dang 512 default up to 1024 every time you use an SDXL model, it literally updates for you (unless you select custom res of course)

\- Multi-GPU support, including if you have multiple machines over network (on LAN or remote servers on the web)

\- Controlnet support

\- Full parameter tweaking (sampler, scheduler, seed, cfg, steps, batch, etc. etc. etc)

\- Support for less commonly known but powerful core parameters (such as Variation Seed or Tiling as popularized on auto webui but not usually available in other UIs for some reason)

\- Wildcards and prompt syntax for in-line prompt randomization too

\- Full in-UI image browser, model browser, lora browser, wildcard browser, everything. You can attach thumbnails and descriptions and trigger phrases and anything else to all your models. You can quickly search these lists by keyword

\- Full-range presets - don't just do textprompt style presets, why not link a model, a CFG scale, anything else you want in your preset? Swarm lets you configure literally every parameter in a preset if you so choose. Presets also have a full browser with thumbnails and descriptions too.

\- All prompt syntax has tab completion, just type the ""<"" symbol and look at the hints that pop up  


https://preview.redd.it/ywyfm7d0pjnc1.png?width=469&format=png&auto=webp&s=2f9822a7ffe8e820ece982a320d476fdfffe9498

\- A clip tokenization utility to help you understand how CLIP interprets your text

https://preview.redd.it/0pvh6md6pjnc1.png?width=825&format=png&auto=webp&s=871cc4a08c4d4991447643d0fd6e3c9bf33a07c2

\- an automatic pickle-to-fp16-safetensors converters to upvert your legacy files in bulk

\- a lora extractor utility - got old fat models you'd rather just be loras? Converting them is just a few clicks away.

\- Multiple themes. Missing your auto webui blue-n-gold? Just set theme to ""Gravity Blue"". Want to enter the future? Try ""Cyber Swarm""

\- Done generating and want to free up VRAM for something else but don't want to close the UI? You bet there's a server management tab that lets you do stuff like that, and also monitor resource usage in-UI too.

\- Got models set up for a different UI? Swarm recognizes most metadata & thumbnail formats used by other UIs, but of course Swarm itself favors standardized [ModelSpec metadata](https://github.com/Stability-AI/ModelSpec).

\- Advanced customization options. Not a fan of that central-focused prompt box in the middle? You can go swap ""Prompt"" to ""VisibleNormally"" in the parameter configuration tab to switch to be on the parameters panel at the top. Want to customize other things? You probably can.

\- Did I mention that the core of swarm is written with a fast multithreaded C# core so it boots in literally 2 seconds from when you click it, and uses barely any extra RAM/CPU of its own (not counting what the backend uses of course)

\- Did I mention that it's free, open source, and run by a developer (me) with a strong history of long-term open source project running that loves PRs? If you're missing a feature, post an issue or make a PR! As a regular user, this means you don't have to worry about downloading 12 extensions just for basic features - everything you might care about will be in the main engine, in a clean/optimized/compatible setup. (Extensions are of course an option still, there's a dedicated extension API with examples even - just that'll mostly be kept to the truly out-there things that really need to be in a separate extension to prevent bloat or other issues.)

That is literally still not a complete list of features, but I think that's enough to make the point, eh?

If I've successfully made the point to you, dear reddit reader - you can try Swarm here [https://github.com/Stability-AI/StableSwarmUI?tab=readme-ov-file#stableswarmui](https://github.com/Stability-AI/StableSwarmUI?tab=readme-ov-file#stableswarmui)

&#x200B;",/r/StableDiffusion/comments/1bbgsl8/stableswarmui_beta/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Looking for Tool for videogeneration,squeekytits,https://www.reddit.com/r/ArtificialInteligence/comments/1bb8gm0/looking_for_tool_for_videogeneration/,0,1,1710071022.0,1bb8gm0,"Hello everyone,
This is my first time posting in this sub so if it goes against any rules, please feel free to delete!

So, I'm fairly new to AI as I have just started diving into everything some weeks ago.
Now my boss (I'm doing a paid internship right now) wants me to find easy tools to create a video of a generated person animatedly speaking to the viewer, already with a generated voice and hand movements and everything.

I have already tried out PikaLabs on Discord and RunwayML for Text-to-Video and Image-to-Video, as well as Tools like HeyGen etc. for avatar creation and generating voices but he is not too happy with that.

The avatar ones are too boring for him, as there is only mouth movement without hand gestures and the generated videos are without talking at all.

In his opinion, AI should be far enough developed to have an easy-to-use, good quality tool exactly for what he wants. I'm not too sure about that but, as I stated before I'm only a beginner in this field so I have barely any clue at all.

If you've got some suggestions for me to try, I would be very thankful!
Thank you!",/r/ArtificialInteligence/comments/1bb8gm0/looking_for_tool_for_videogeneration/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
Chinese researchers hope to create ‘real AI scientists’,aurumvexillum,https://www.scmp.com/news/china/science/article/3254793/chinese-researchers-hope-create-real-ai-scientists-through-informed-machine-learning?campaign=3254793&module=perpetual_scroll_0&pgtype=article,53,13,1710042686.0,1bb0vj4,"Chinese researchers have developed a new framework that will help developers train machine learning models that they hope could lead to the creation of “real AI scientists” capable of improving experiments and solving scientific problems.

Deep learning models have “revolutionised the field of scientific research” due to their ability to uncover relationships from large amounts of data, according to a paper published in the peer-reviewed Cell Press journal Nexus on Friday.

One recent example is Sora, a text-to-video model from the American company OpenAI, which the developers say can understand how “things exist in the real world”.

It has been widely praised for its advanced, realistic depictions of things and hailed as a massive step forward for generative AI, but the company has admitted it still struggles to simulate some aspects of the real world and cannot “accurately model the physics of many basic interactions, like glass shattering”.

Sora is trained using large amounts of visual data, allowing it to pick up patterns to generate images and videos that mimic reality. But it is not trained to understand physical laws such as gravity.

> “Without a fundamental understanding of the world, a model is essentially an animation rather than a simulation,” said Chen Yuntian, study author and a professor at the Eastern Institute of Technology.

Deep learning models are generally trained using data and not prior knowledge, which can include things such as the laws of physics or mathematical logic, according to the paper.

But the scientists from Peking University and EIT wrote that when training the models, prior knowledge could be used alongside data to make them more accurate, creating “informed machine learning” models capable of incorporating this knowledge into their output.

Deciding what prior knowledge – which can include things such as functional relationships, equations and logic – to incorporate into a model for it to “pre-learn” was a challenge and incorporating multiple rules could also lead to models collapsing, the team wrote.

> “When faced with a high volume of knowledge and rules – which is often the case, current informed machine learning models tend to struggle or even fail,” Chen said.

To address this issue, the researchers created a framework to assess the value of rules and determine which combinations resulted in the most predictive models.

> “Embedding human knowledge into AI models has the potential to improve their efficiency and ability to make inferences, but the question is how to balance the influence of data and knowledge,” Xu Hao, first author and researcher at Peking University, said in a Cell Press statement.

> “Our framework can be employed to evaluate different knowledge and rules to enhance the predictive capability of deep learning models.”

The framework calculates “rule importance”, looking at how a specific rule or combination or rules affects the predictive accuracy of a model, according to the paper.

Teaching the AI models about such rules – for example, the laws of physics – could make them “more reflective of the real world, which would make them more useful in science and engineering”, Chen from EIT, said in the statement.

The researchers tested their framework by using it to optimise a model for solving multivariate equations, and another one used to predict the results of a chemistry experiment.

Chen said that in the short term this framework would be the most useful in scientific models “where consistency between the model and physics rules is crucial to avoid potentially disastrous consequences”.

The team hopes to take their framework further to allow AI to identify its own knowledge and rules directly from data without human interference.

> “We want to make it a closed loop by making the model into a real AI scientist,” Chen said in the statement. The team is developing an open source, plugin tool for AI developers that could allow them to achieve this.

However, the team has already identified at least one problem.

During the study the team found out that when more data is added to a model, general rules become more significant than specific local rules, but this does not help in fields such as biology and chemistry because they “often lack readily available general rules akin to governing equations”.",/r/Futurology/comments/1bb0vj4/chinese_researchers_hope_to_create_real_ai/,Futurology,r/Futurology,False,relevant,highly relevant
3d model training? ,GutsMan85,https://www.reddit.com/r/StableDiffusion/comments/1bayhwa/3d_model_training/,9,5,1710035284.0,1bayhwa,"I know very little about the ins and outs of these a.i. driven art forms except for playing around here and there with web based applications.   I for sure don't get how it works except for surface knowledge but understand a little about prompting text to image and basics of how its coming up with the end results.  

So, the way I understand it, one of the big problems with text to image prompting is that a.i. only visualizes and trains on 2D images, right?  It seems there have been a lot of improvements (text to video, hands lol, etc.), but is that attributed mostly to increased and expanded training?  Are there plans to train in programs similar to blender, looking at 3d models, so that the a.i. not only gets that it's 3d, but then also referencing the 2d render at the same time to understand what it's actually interpreting in 3d space,  allowing it to really understand the composition it's creating?  Or would that even matter with the way the a.i. has been improving and would basically just be starting over?  

Hopefully this makes sense and isn't too rambling.",/r/StableDiffusion/comments/1bayhwa/3d_model_training/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
weekly roundup for week of 3/1-3/8,sub-roundup,https://www.reddit.com/r/editors/comments/1baid2k/weekly_roundup_for_week_of_3138/,2,0,1709992818.0,1baid2k,"### Most Upvoted Posts

1. [I'm an editor, not a motion graphics artist. (655 🡅)](/r/editors/comments/1b86dvb/im_an_editor_not_a_motion_graphics_artist/)
2. [Won a 50k grant for my organization with an edit I did. (268 🡅)](/r/editors/comments/18ze3pp/won_a_50k_grant_for_my_organization_with_an_edit/)
3. [My first feature film edit is going to theaters (222 🡅)](/r/editors/comments/19a881c/my_first_feature_film_edit_is_going_to_theaters/)
4. [Sora (200 🡅)](/r/editors/comments/1asnrw8/sora/)
5. [Just hit version 100 of a 30 second edit (195 🡅)](/r/editors/comments/1b39yyj/just_hit_version_100_of_a_30_second_edit/)

### Most Commented Posts

1. [Leaving the industry... (284 💬)](/r/editors/comments/1b2j78q/leaving_the_industry/)
2. [I'm an editor, not a motion graphics artist. (237 💬)](/r/editors/comments/1b86dvb/im_an_editor_not_a_motion_graphics_artist/)
3. [What's up with all the Adobe hate? (234 💬)](/r/editors/comments/1ajplgd/whats_up_with_all_the_adobe_hate/)
4. [OpenAI announces Sora today, introducing their photorealistic text-to-video product (228 💬)](/r/editors/comments/1arrmbi/openai_announces_sora_today_introducing_their/)
5. [Sora (201 💬)](/r/editors/comments/1asnrw8/sora/)

### Moderator Selections

1. [The film I edited last year 'Your Lucky Day' just came out on netflix!](/r/editors/comments/1b96o21/the_film_i_edited_last_year_your_lucky_day_just/)",/r/editors/comments/1baid2k/weekly_roundup_for_week_of_3138/,editors,r/editors,False,relevant,highly relevant
A Question ,OldSpartanGuy,https://www.reddit.com/r/faceswap/comments/1bahv51/a_question/,6,1,1709991291.0,1bahv51,When will we really see a major break through with face swapping? Like its happening with other ai things like image generation or text to video etc ,/r/faceswap/comments/1bahv51/a_question/,faceswap,r/faceswap,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/ChatGPTCoding/comments/1b9qvhw/this_week_in_ai_all_the_major_ai_developments_in/,31,2,1709912071.0,1b9qvhw,"1. **Anthropic** introduced the next generation of Claude: Claude 3 model family. It includes ***Opus***, ***Sonnet*** and  ***Haiku***  models. Opus is the most intelligent model, that outperforms GPT-4 and  Gemini 1.0 Ultra on most of the common evaluation benchmarks. Haiku is  the fastest, most compact model for near-instant responsiveness. The  Claude 3 models have vision capabilities, offer a 200K context window  capable of accepting inputs exceeding 1 million tokens, improved  accuracy and fewer refusals \[[*Details*](https://www.anthropic.com/news/claude-3-family) *|* [*Model Card*](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)\]. 
2. **Stability AI** partnered with Tripo AI and released ***TripoSR***,  a fast 3D object reconstruction model that can generate high-quality 3D  models from a single image in under a second. The model weights and  source code are available  under the MIT license, allowing  commercialized use.  \[[*Details*](https://stability.ai/news/triposr-3d-generation) *|* [GitHub](https://github.com/VAST-AI-Research/TripoSR) | [Hugging Face](https://huggingface.co/stabilityai/TripoSR)\].
3. **Answer.AI** released a fully open source system that, for the first time, can efficiently train  a 70b large language model on a regular desktop computer with two or  more standard gaming GPUs. It combines QLoRA with Meta’s FSDP, which  shards large models across multiple GPUs \[[*Details*](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html)\].
4. **Inflection** launched ***Inflection-2.5***, an upgrade to their model powering [Pi](https://pi.ai/talk),  Inflection’s empathetic and supportive companion chatbot.  Inflection-2.5 approaches GPT-4’s performance, but used only 40% of the  amount of compute for training. Pi is also now available on Apple  Messages \[[*Details*](https://inflection.ai/inflection-2-5)\].
5. **Twelve Labs** introduced ***Marengo-2.6***,  a  new state-of-the-art (SOTA) multimodal foundation model capable of  performing any-to-any search tasks, including Text-To-Video,  Text-To-Image, Text-To-Audio, Audio-To-Video, Image-To-Video, and more \[[*Details*](https://www.twelvelabs.io/blog/introducing-marengo-2-6)\].
6. **Cloudflare** announced the development of ***Firewall for AI***,  a protection layer that can be deployed in front of Large Language  Models (LLMs), hosted on the Cloudflare Workers AI platform or models  hosted on any other third party infrastructure,  to identify abuses  before they reach the models \[[*Details*](https://blog.cloudflare.com/firewall-for-ai)\]
7. **Scale AI**, in partnership with the Center for AI Safety,  released ***WMDP***  (Weapons of Mass Destruction Proxy): an open-source evaluation  benchmark of 4,157 multiple-choice questions that serve as a proxy  measurement of LLM’s risky knowledge in biosecurity, cybersecurity, and  chemical security \[[*Details*](https://scale.com/blog/measuring-mitigating-risk-wmdp)\].
8. **Midjourney** launched ***v6 turbo*** mode to generate images at 3.5x the speed (for 2x the cost). Just type /turbo \[[*Link*](https://x.com/midjourney/status/1765627072238772253)\].
9. **Moondream.ai** released ***moondream 2***  \- a small 1.8B parameters, open-source, vision language model designed  to run efficiently on edge devices. It was initialized using Phi-1.5 and  SigLIP, and trained primarily on synthetic data generated by Mixtral.  Code and weights are released under the Apache 2.0 license, which  permits commercial use \[[*Details*](https://github.com/vikhyat/moondream?tab=readme-ov-file)\].
10. **Vercel** released  [***Vercel AI SDK 3.0***](https://sdk.vercel.ai/docs)***.*** Developers can now associate LLM responses to streaming React Server Components \[[*Details*](https://vercel.com/blog/ai-sdk-3-generative-ui)\].
11. **Nous Research** released a new model designed exclusively to create instructions from raw-text corpuses, ***Genstruct 7B***. This enables the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus \[[*Details*](https://huggingface.co/NousResearch/Genstruct-7B)\].
12. **01.AI** open-sources ***Yi-9B***,  one of the top performers among a range of similar-sized open-source  models excelling in code, math, common-sense reasoning, and reading  comprehension \[[*Details*](https://huggingface.co/01-ai/Yi-9B)\].
13. **Accenture** to acquire ***Udacity*** to build a learning platform focused on AI \[[*Details*](https://techcrunch.com/2024/03/05/accenture-to-acquire-udacity-to-build-a-learning-platform-focused-on-ai)\].
14. **China** Offers ‘Computing Vouchers’ upto $280,000 to Small AI Startups to train and run large language models \[[*Details*](https://www.pymnts.com/artificial-intelligence-2/2024/china-offers-computing-vouchers-to-small-ai-startups)\].
15. **Snowflake**  and Mistral have partnered to make Mistral AI’s newest and most  powerful model, Mistral Large,  available in the Snowflake Data Cloud \[[*Details*](https://www.snowflake.com/news/snowflake-partners-with-mistral-ai-to-bring-industry-leading-language-models-to-enterprises-through-snowflake-cortex)\]
16. **OpenAI** rolled out ‘***Read Aloud***’  feature for ChatGPT, enabling ChatGPT to read its answers out loud.  Read Aloud can speak 37 languages but will auto-detect the language of  the text it’s reading \[[*Details*](https://www.theverge.com/2024/3/4/24090500/chatgpt-openai-voice-ios-android)\].

**Source**: AI Brews -  You can subscribe the [newsletter here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks",/r/ChatGPTCoding/comments/1b9qvhw/this_week_in_ai_all_the_major_ai_developments_in/,ChatGPTCoding,r/ChatGPTCoding,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/ArtificialInteligence/comments/1b9qsq8/this_week_in_ai_all_the_major_ai_developments_in/,21,1,1709911882.0,1b9qsq8,"1. **Anthropic** introduced the next generation of Claude: Claude 3 model family. It includes ***Opus***, ***Sonnet*** and  ***Haiku***  models. Opus is the most intelligent model, that outperforms GPT-4 and  Gemini 1.0 Ultra on most of the common evaluation benchmarks. Haiku is  the fastest, most compact model for near-instant responsiveness. The  Claude 3 models have vision capabilities, offer a 200K context window  capable of accepting inputs exceeding 1 million tokens, improved  accuracy and fewer refusals \[[*Details*](https://www.anthropic.com/news/claude-3-family) *|* [*Model Card*](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)\]. 
2. **Stability AI** partnered with Tripo AI and released ***TripoSR***,  a fast 3D object reconstruction model that can generate high-quality 3D  models from a single image in under a second. The model weights and  source code are available  under the MIT license, allowing  commercialized use.  \[[*Details*](https://stability.ai/news/triposr-3d-generation) *|* [GitHub](https://github.com/VAST-AI-Research/TripoSR) | [Hugging Face](https://huggingface.co/stabilityai/TripoSR)\].
3. **Answer.AI** released a fully open source system that, for the first time, can efficiently train  a 70b large language model on a regular desktop computer with two or  more standard gaming GPUs. It combines QLoRA with Meta’s FSDP, which  shards large models across multiple GPUs \[[*Details*](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html)\].
4. **Inflection** launched ***Inflection-2.5***, an upgrade to their model powering [Pi](https://pi.ai/talk),  Inflection’s empathetic and supportive companion chatbot.  Inflection-2.5 approaches GPT-4’s performance, but used only 40% of the  amount of compute for training. Pi is also now available on Apple  Messages \[[*Details*](https://inflection.ai/inflection-2-5)\].
5. **Twelve Labs** introduced ***Marengo-2.6***,  a  new state-of-the-art (SOTA) multimodal foundation model capable of  performing any-to-any search tasks, including Text-To-Video,  Text-To-Image, Text-To-Audio, Audio-To-Video, Image-To-Video, and more \[[*Details*](https://www.twelvelabs.io/blog/introducing-marengo-2-6)\].
6. **Cloudflare** announced the development of ***Firewall for AI***,  a protection layer that can be deployed in front of Large Language  Models (LLMs), hosted on the Cloudflare Workers AI platform or models  hosted on any other third party infrastructure,  to identify abuses  before they reach the models \[[*Details*](https://blog.cloudflare.com/firewall-for-ai)\]
7. **Scale AI**, in partnership with the Center for AI Safety,  released ***WMDP***  (Weapons of Mass Destruction Proxy): an open-source evaluation  benchmark of 4,157 multiple-choice questions that serve as a proxy  measurement of LLM’s risky knowledge in biosecurity, cybersecurity, and  chemical security \[[*Details*](https://scale.com/blog/measuring-mitigating-risk-wmdp)\].
8. **Midjourney** launched ***v6 turbo*** mode to generate images at 3.5x the speed (for 2x the cost). Just type /turbo \[[*Link*](https://x.com/midjourney/status/1765627072238772253)\].
9. **Moondream.ai** released ***moondream 2***  \- a small 1.8B parameters, open-source, vision language model designed  to run efficiently on edge devices. It was initialized using Phi-1.5 and  SigLIP, and trained primarily on synthetic data generated by Mixtral.  Code and weights are released under the Apache 2.0 license, which  permits commercial use \[[*Details*](https://github.com/vikhyat/moondream?tab=readme-ov-file)\].
10. **Vercel** released  [***Vercel AI SDK 3.0***](https://sdk.vercel.ai/docs)***.*** Developers can now associate LLM responses to streaming React Server Components \[[*Details*](https://vercel.com/blog/ai-sdk-3-generative-ui)\].
11. **Nous Research** released a new model designed exclusively to create instructions from raw-text corpuses, ***Genstruct 7B***. This enables the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus \[[*Details*](https://huggingface.co/NousResearch/Genstruct-7B)\].
12. **01.AI** open-sources ***Yi-9B***,  one of the top performers among a range of similar-sized open-source  models excelling in code, math, common-sense reasoning, and reading  comprehension \[[*Details*](https://huggingface.co/01-ai/Yi-9B)\].
13. **Accenture** to acquire ***Udacity*** to build a learning platform focused on AI \[[*Details*](https://techcrunch.com/2024/03/05/accenture-to-acquire-udacity-to-build-a-learning-platform-focused-on-ai)\].
14. **China** Offers ‘Computing Vouchers’ upto $280,000 to Small AI Startups to train and run large language models \[[*Details*](https://www.pymnts.com/artificial-intelligence-2/2024/china-offers-computing-vouchers-to-small-ai-startups)\].
15. **Snowflake**  and Mistral have partnered to make Mistral AI’s newest and most  powerful model, Mistral Large,  available in the Snowflake Data Cloud \[[*Details*](https://www.snowflake.com/news/snowflake-partners-with-mistral-ai-to-bring-industry-leading-language-models-to-enterprises-through-snowflake-cortex)\]
16. **OpenAI** rolled out ‘***Read Aloud***’  feature for ChatGPT, enabling ChatGPT to read its answers out loud.  Read Aloud can speak 37 languages but will auto-detect the language of  the text it’s reading \[[*Details*](https://www.theverge.com/2024/3/4/24090500/chatgpt-openai-voice-ios-android)\].

**Source**: AI Brews -  You can subscribe the [newsletter here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks",/r/ArtificialInteligence/comments/1b9qsq8/this_week_in_ai_all_the_major_ai_developments_in/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
"Minvo transforms dull, long-form videos into a flurry of engaging clips perfectly formatted for any social platform",softtechhubus,https://www.reddit.com/r/u_softtechhubus/comments/1b95vex/minvo_transforms_dull_longform_videos_into_a/,1,1,1709846703.0,1b95vex," 

https://preview.redd.it/8ezug5ghbzmc1.png?width=1320&format=png&auto=webp&s=2e846730a402ceb372109d84a85c0bc32c5fad23

# Minvo transforms dull, long-form videos into a flurry of engaging clips perfectly formatted for any social platform

&#x200B;

[Minvo transforms dull, long-form videos into a flurry of engaging clips perfectly formatted for any social platform](https://reddit.com/link/1b95vex/video/idey52ajbzmc1/player)

## I. Introduction

Videos have become integral to modern marketing strategies. Simply put, people are more likely to watch a video than read text on platforms like social media. However, producing high-quality videos can be time-consuming and expensive, requiring editing skills that many businesses lack. This is where Minvo comes in - the platform aims to transform any long-form video into an array of optimized clips perfect for all major social networks, with just a few clicks.

Minvo's key differentiator is its use of cutting-edge artificial intelligence (AI) to power the entire video editing process. By analyzing uploaded videos and transcribing audio, the AI can automatically detect relevant moments, apply optimal formatting, and generate clips optimized for platforms like Instagram, TikTok, YouTube and more. This allows businesses of any size to easily produce professional-looking video content without investing in video editors or learning advanced technical skills.

In this article, we will take a closer look at Minvo's AI-powered video creation platform, key features and benefits, pricing plans, and how it can revolutionize the way businesses leverage social video marketing. Our goal is to provide a comprehensive overview of the product through various use cases, success stories, and expert insights - ultimately making a case for why every marketer should consider automating their video efforts with Minvo.

## II. Minvo's AI-Powered Video Creation Platform

With Minvo, users simply need to upload any video file - it can be a corporate presentation, tutorial, product demo, interview, or raw footage. The AI then goes to work analyzing the video on both audio and visual levels. It transcribes the audio using speech recognition technology to generate editable subtitles in over 40 languages. Simultaneously, it recognizes faces, logos, and other elements to intelligently frame shots for different social formats.

Once analyzed, the original video appears on Minvo's intuitive timeline-based interface. Users can then scroll through the transcript and skim through different clips to find relevant moments worth extracting. With a single click, any segment can be saved as an individual clip perfectly formatted for the target platform - whether vertical, square, or landscape. Professional-level trims, fades, and transitions are automatically applied to each clip.

Key features of Minvo's AI-powered platform include:

* **Automated transcription** of audio to text in many languages for easy navigation
* **Automatic shot framing** that recognizes faces and objects to frame clips vertically
* **AI-suggested emojis** integrated directly onto the transcript
* **Magic B-Roll** that adds complementary stock video clips into edits
* **Background music library** with hundreds of royalty-free tracks
* **Easy timeline editing** for refining and rearranging clips
* **One-click formatting** to common social video sizes
* **Seamless social sharing** directly to profiles without extra steps

This all-in-one interface streamlines the entire process from uploading raw footage to generating final clips optimized for distribution - removing many tedious tasks traditionally associated with video production.

## III. The Power of AI in Video Creation

While AI technology is still advancing, platforms like Minvo demonstrate how it can positively transform content creation workflows. Some key advantages of AI-powered video editing include:

**Time savings.** AI handles the heavy lifting so users don't have to spend hours behind the editing bay. With Minvo, raw videos are pre-analyzed, relevant moments are detected, and optimized clips generated in just a few clicks.

**Consistency at scale.** As AI systems are trained on large datasets, the formatting, trims and editing styles they apply become very consistent - allowing businesses to pump out video content on a massive scale.

**Creativity boost.** By focusing on high-level clip selection versus technical aspects, AI frees users to spend more energy on storytelling, messaging and strategic use of visual elements. Creative juices no longer need to be spent on formatting specifics.

**Round-the-clock capabilities.** AI systems can work non-stop to analyze, transcribe and generate clips without needing breaks. So content creation becomes an always-on activity not limited by human schedules and fatigue.

**Keeping pace with change.** As social platforms innovate, AI makes it easy for marketers to comply with new formats, ratios and standards without delay. The algorithms constantly evolve based on new data.

Minvo's AI is uniquely optimized for video by utilizing neural networks trained on massive datasets. Its systems recognize subjects, analyze scenes, understand speech, and apply cinematic techniques - all with the goal of accelerating content production while maintaining industry-standard quality. AI has reshaped content creation, and Minvo is at the forefront of this revolution for marketers.

## IV. Customization and Personalization

While AI streamlines the process, Minvo also ensures marketers maintain creative control every step of the way. Users can fully customize generated clips to convey unique business messages and connect with specific target audiences.

For example, branded fonts, logos, colors and overlays can be added during the editing process to maintain a cohesive style reflective of the company or product. Narration, music selection, emojis and B-Roll usage are all easily customizable as well. And with direct access to the AI-generated subtitles, marketers can tweak wording or timing as needed.

Further personalization comes from Minvo's filters for segmenting video libraries. Marketers can browse stock footage, images and music tagged by categories like industry, tone, season or activity. This makes it effortless to assemble visuals that resonate closely with a demographic's interests.

The platform also allows exported clips to be selectively shared only to certain social profiles, geographic locations, or specific time periods through its direct social posting widgets. This targeting capability helps optimize reach.

In essence, no two users need to have the same video creation experience on Minvo. Its AI serves as a powerful assistant rather than a replacement for human judgment - ensuring videos connect authentically with their intended audiences through flexibility and custom layers.

## V. Extensive Media Libraries

To further supplement users' creative control, Minvo provides an immense selection of premium media elements built-in right to the platform. This includes stock footage libraries, image collections, audio tracks and template styles.

The stock footage spans a vast array of categories covering lifestyle, nature, activities and concepts. Clips are sourced from professional vendors to ensure only high-production value HD/4K quality enters the library. Additionally, new footage is consistently added to keep up with the latest trends.

For sound, there is an exhaustive music collection spanning multiple genres, tones and lengths. Tracks are royalty-free and clear for use in both private and commercial videos. Extra audio clips like ambiance, SFX and voiceovers supplement the options.

Template styles give a head start on consistent visual identities. Dozens of pre-built templates exist covering various industries, holidays, events and video types. And everything from brochure templates to infographic templates provide robust graphic assets as well.

Perhaps most uniquely, Minvo also taps into user-generated content libraries. This allows businesses to leverage crowdsourced videos, pictures and edits uploaded voluntarily by other Minvo members under certain reuse permissions.

With this vast multi-tiered media ecosystem built right in, Minvo becomes a one-stop shop for all assets needed to consistently generate top-quality, visually interesting social videos with ease.

## VI. Video Templates and Styles

One way Minvo streamlines the creative process is through its library of professionally-designed video templates and styles. These pre-built assets provide marketers a head start on concepting and outputting polished videos without designing from scratch.

Templates cover topics like:

* Product demonstrations
* How-to guides
* Webinar replays
* Service descriptions
* Interview formats
* Event recaps
* Brand advertisements

Additional holiday, occasion and industry-specific templates are continuously added. Each incorporates recommended best practices for visual storytelling in its respective vertical.

Templates can also be easily customized. For example, users simply update slotted text overlays, swap stock videos/images, and tweak colors/fonts as needed for their own unique messaging. This allows cranking out multiple variations of videos following consistently structured story arcs.

Meanwhile, video styles supply professionally designed motion graphic themes in the spirit of common brand identities. Options include flat design, minimalism, retro vibes, luxury aesthetics and more. Each comes pre-programmed with title screens, transitions, lower thirds and end cards ready for direct implementation.

Together, templates and styles equip marketers creating video content for the first time with an instant creative starting point to develop branded, strategically formatted videos in just a few minutes of tweaking. Quality is ensured while learning curves flatten.

## VII. Text-to-Video Generation

One especially unique capability of Minvo is its text-to-video feature which converts any written script or transcript into a ready-to-share video clip within seconds using AI. With this tool, marketers can expand their use of video beyond just uploading footage files.

The process works by taking blocks of text as input and analyzing them to determine optimal visual elements like graphics, stock videos, and animated text overlays needed to bring the words to life visually. Music, fonts, and even basic animations are auto-selected to match the overall tone and structure.

Advanced neural networks allow generating these video clips incredibly quickly while still maintaining coherence and visual interest throughout. For example, a 300-word blog post could be instantly transformed into a well-produced 60 second explainer reel using just the embedded text-to-video tool.

Savvy business owners are already using this groundbreaking capability:

* Turn ebooks or reports into video courses
* Promote blog articles or newsletters on social feeds
* Animate presentations, speeches or itineraries
* Create video sitemaps or tutorials from guides
* Pitch products through automatically generated demonstrations

By unlocking a video format from written documents, text-to-video vastly expands opportunities for sharing information across new mediums and platforms seamlessly through Minvo's AI synthesis. It promises to change content creation as we know it.

## VIII. Seamless Social Sharing

Once video clips are generated on Minvo, the goal is usually to propagate them across marketing channels for maximum exposure and engagement. To this end, the platform integrates tightly with all major social networks and tools.

Clips can be directly published to Facebook, Instagram, Twitter, LinkedIn, YouTube and more with just one click. Professional looking previews are generated automatically for each network. Additionally, embedding codes are provided for easy sharing on blogs, emails and other off-platform locations.

For advanced scheduling, users can queue up batches of clips and specify precise publish times, allowing for round-the-clock “publishing” without needing to manually share each video. Post scheduling also factors in analytics like best days/times according to each channel and time zone.

Additional integrations exist with platforms like Hootsuite, Buffer and LinkedIn Company Page Post Creator for centralized social management. And with Minvo’s API, developers can build custom integrations too.

All in all, the sharing experience is designed to be as effortless as the creation process itself thanks to deep underlying integrations. This effectively places marketers’ AI-generated video arsenal directly into their cross-channel content distribution toolkit.

## IX. Case Studies and Success Stories

Major brands across diverse industries have successfully leveraged Minvo to power their video marketing strategies. Here are a few examples demonstrating real measured results:

**Anthropic (AI Safety Company)**  
Using Minvo, Anthropic increased average weekly Twitter engagements by 2.5x and YouTube views by 4x. Videos required 80% less time to produce at scale.

**Bold Method (Digital Marketing Agency)**  
Minvo helped Bold Method repurpose client video content into 45+ social clips monthly, freeing over 120 man hours and growing 3 target accounts by 20%.

**FitOn (Health/Fitness App)**  
FitOn's Instagram Stories views grew 6x after implementing a steady stream of Minvo-generated workout snippets and diet tips targeted at women ages 25-45.

**Vanderbloemen (Real Estate)**  
By livestreaming open houses on Facebook via Minvo, Vanderbloemen filled listings 1.5x faster and averaged 300+ video views each - a major lead generator.

**The Athletic (Sports Media)**  
Minvo allowed The Athletic to extract highlights from long-form podcasts and live events, boosting social video views by 250% and playable minutes by 400%.

These success stories underline how the platform has empowered businesses across verticals to capitalize on video in fresh ways - driving measurable marketing impacts like increased traffic, higher engagement, and amplified brand awareness through creative AI-powered content strategies.

## X. Pricing and Plans

To make AI video editing accessible to all, Minvo offers affordable monthly plans tailored for both individual creators and large enterprise teams. All tiers include the full-featured editor, media libraries, integrations and unlimitedClip generation.

The basic Personal plan costs $28/month billed annually. This provides one user with access to all core Minvo functionality.

For small businesses, the Pro plan is $45/month and allows up to 5 users on a single account. It also includes priority customer support.

Larger companies can choose the Business plan starting at $99/month, supporting unlimited users under one organization. Additional features like SSO login, admin controls and customized branding are included.

A customized Enterprise solution is also available for the needs of the very largest video-centric brands. Minvo works directly with these organizations to ensure their specific workflows and security requirements are met.

Considering the time, costs and skills traditionally associated with professional video production, Minvo presents an extremely affordable option to automate content creation at scale. Its flexible pricing model means even the smallest of businesses can effortlessly produce top-shelf video marketing.

## XI. Getting Started with Minvo

The onboarding process with Minvo is simple and intuitive for both novice and experienced users alike. Getting one's first video clip generated requires only a few basic steps:

1. [**Sign Up.** Provide some basic account details to set up a Minvo login.](https://softtechhub.us/2024/03/07/transform-long-videos-into-clips/#Get_Minvo_Lifetime_Deal_Here)
2. **Upload Video.** Drag and drop any video file from the computer directly onto the timeline interface.
3. **Review Transcript.** Scan through the auto-generated subtitles to find sections worth clipping.
4. **Select Clip.** Drag the playhead to the desired start/end points and hit ""Create Clip.""
5. **Customize.** Add brand overlays, text, music and more with easy-to-use editing tools.
6. **Export.** Generate the clip in any size for sharing across networks.

From there, users are free to explore Minvo's full suite of AI-powered features and familiarize themselves further at their own pace. Comprehensive guides, how-to videos and live chat support are also on hand for any questions or issues that come up.

Overall, the streamlined process mirrors that of common consumer apps to keep friction ultra-low. Within an hour, most new users can already get the hang of turning raw videos into amplified social content through AI.

## XII. Future of Video Marketing and Minvo

As consumption of online visual media continues its exponential growth curve, video will comprise even greater portions of future marketing strategies. Advances in AI give glimpses of how Minvo and similar platforms could develop further:

**Deeper semantic understanding.** AI will gain even richer comprehension of video contexts to apply more dynamic and targeted editing techniques at scale.

**Real-time optimizations.** Platforms may auto-update generated clips continually based on latest insights from social analytics and behavior models.

**Hyper-personalization.** AI assistants could tap user data to generate totally unique clip variations tailored to individuals and micro-segments.

**Interactive capabilities.** Videos may evolve beyond passive consumption to incorporate immersive multi-step experiences driven by AI responses.

**Synthetic media.** Techniques like deepfakes will advance to allow synthetic generation of clips from indirect sources like text, voice or metadata alone.

By staying on the cutting edge of AI and video technology, Minvo is well positioned to lead this transformation. The platform will continuously adapt to emerging shifts in online video while keeping content creation simple and strategic for all. Ultimately, marketers who embrace AI now will be best prepared to leverage richer multimedia storytelling in tomorrow’s digital landscape.

## XIII. Conclusion

In conclusion, Minvo has established itself as the premier AI-powered solution for automating enterprise-grade video marketing. By analyzing raw footage files and transcriptions, its advanced neural networks can extract highlights, format clips optimally for social platforms, add complementary media elements, and seamlessly publish final videos - all in just a few clicks.

This breakthrough capability removes technological barriers that traditionally hindered many businesses from effectively leveraging video. Thanks to Minvo, any organization regardless of size or video skills can now cost-effectively generate and distribute professional visuals at scale. The results for early adopters speak for themselves in terms of measurable impacts on key metrics like social engagements, conversions and brand awareness.

Looking ahead, as AI video editing becomes commonplace, those who integrate such platforms first will gain valuable experience tweaking strategies over time. They will understand how to leverage new multimedia storytelling opportunities presented by evolving algorithms. Minvo customers especially seem well-positioned to cement their leadership through first-mover advantages in tomorrow's AI-driven visual marketing landscape.

[**Get access to Minvo Lifetime Access Deal via this page**](https://softtechhub.us/2024/03/07/transform-long-videos-into-clips/#Get_Minvo_Lifetime_Deal_Here)",/r/u_softtechhubus/comments/1b95vex/minvo_transforms_dull_longform_videos_into_a/,u_softtechhubus,u/softtechhubus,False,relevant,highly relevant
Beta testing for Motion 3.0 is now open!,TheHoBoLoBo,https://www.reddit.com/r/Kaiber/comments/1b95dls/beta_testing_for_motion_30_is_now_open/,4,2,1709845527.0,1b95dls,"It’s that time again: beta testing for Motion 3.0 is now open! We need your help testing these major upgrades to our existing Motion feature. 

## WHAT’S NEW?

* Smoother motion: subjects are not morphing around but carry over context from the prior few seconds.
* Audioreactivity: Motion videos can now sync up to your audio, up to 2.7 minutes long.
* Perfect init image usage: videos will more closely resemble the init image uploaded rather than a slight modification of it.
* Photorealistic outputs: text-to-video outputs using Photorealistic styles are looking very realistic with this upgrade! Give it a try yourself.

## GET STARTED

### All Artist subscribers get automatic access 📷

* 📷 Apply to get invited here: [https://form.typeform.com/to/BwwgCVc9](https://form.typeform.com/to/BwwgCVc9)
* 📷 Read the instruction guide:  [https://kaiber.notion.site/Motion-3-0-Beta-Instructions-ecb59c0032f34dbc950cd3d605fc2f4c](https://kaiber.notion.site/Motion-3-0-Beta-Instructions-ecb59c0032f34dbc950cd3d605fc2f4c) 

Thanks, and let us know if you have any questions! ",/r/Kaiber/comments/1b95dls/beta_testing_for_motion_30_is_now_open/,Kaiber,r/Kaiber,False,relevant,highly relevant
[Trending] OpenAI Sora A.I Text To Video Generator ChatGPT Example Videos,articlebazar,https://youtube.com/watch?v=oj-Opxj2Y-M&si=kDQ0lPTVJQBCK-cA,1,0,1709841387.0,1b93mcw,,/r/u_articlebazar/comments/1b93mcw/trending_openai_sora_ai_text_to_video_generator/,u_articlebazar,u/articlebazar,False,not relevant,neutral
[newsletter] The Impact of Text-to-Video Models on Video Production,gradientflow,https://www.reddit.com/r/GradientFlow/comments/1b90g8g/newsletter_the_impact_of_texttovideo_models_on/,1,0,1709832887.0,1b90g8g,"🆕📨 Newsletter 🚀 

\- The future of video production with [**OpenAI**](https://www.linkedin.com/company/42353915/admin/feed/posts/#) Sora & text-to-video models 🎥 

\- [**Mistral AI**](https://www.linkedin.com/company/42353915/admin/feed/posts/#)'s Impact on the AI Landscape 🛑 

\- Challenges facing the suppliers of foundation models & **#LLMs**

👉 https://gradientflow.substack.com/p/navigating-the-future-of-ai-in-the",/r/GradientFlow/comments/1b90g8g/newsletter_the_impact_of_texttovideo_models_on/,GradientFlow,r/GradientFlow,False,relevant,highly relevant
Cyber Briefing - 2024.03.06,CyberMaterial,https://www.reddit.com/r/cybermaterial/comments/1b81w2x/cyber_briefing_20240306/,2,0,1709737395.0,1b81w2x,"&#x200B;

https://reddit.com/link/1b81w2x/video/10445n1zaqmc1/player

**👉 What's trending in cybersecurity today?**

**🚨 #CyberAlerts**

Apple Releases Urgent iOS Fixes to Combat Zero-Day Threats

Source : Apple

GhostSec and Stormous Launch Ransomware Assaults in 15+ Countries

Source : Chetan Raghuprasad via Cisco Talos

Android and Windows RATs Spread Through Fake Online Meetings

Source : Zscaler

VMware Issues Urgent Patches to Address ESXi Sandbox Escape Vulnerabilities

Source : VMware

New WogRAT Malware Utilizes Online Notepad Platform for Malware Storage

Source : Ahnlab Security

**💥 #CyberIncidents**

Meta's Facebook and Instagram Suffer Technical Outage

Source : Clare Duffy via CNN

Fidelity Security Breach Exposes Sensitive Data

Source : Maine Attorney General

OrdiZK Bridge Allegedly Vanishes, Resulting in $1.4M Rug Pull Scam

Source : CHRISTOPHER ROARK via CoinTelegraph

Northeast Orthopedics Cyberattack Exposes 177K Patients

Source : Maine Attorney General

TalentLaunch Subsidiary's Recent Data Breach Affects 119,261

Source : Maine Attorney General

**📢 #CyberNews**

NSA Releases New Zero-Trust Guidance for Network Security Enhancement

Source : NSA

US Sanctions Intellexa Consortium for Predator Spyware

Source : U.S. Department of The Treasury

CrowdStrike Expands Cloud Security Arsenal with Acquisition of Flow Security

Source : CrowdStrike

Madonna Elevates Concert Experience with Text-to-Video Visuals

Source : Matt O’Brien via AP

Microsoft Ends Support for Windows Subsystem for Android

Source : Microsoft

**This Cyber Briefing is sponsored by** **911cyber\[dot\]co** **- Get help from cybersecurity first responders today!**

Find the full stories at cybermaterial\[dot\]com/cyber-briefing/",/r/cybermaterial/comments/1b81w2x/cyber_briefing_20240306/,cybermaterial,r/cybermaterial,False,relevant,less relevant
"Entering the Multimodal AI Era, 4 Core Leading Stocks Compete for Emerging Tracks",DawdenFawdeunt,https://www.reddit.com/r/WallStreetbetsELITE/comments/1b7tk3k/entering_the_multimodal_ai_era_4_core_leading/,1,1,1709709017.0,1b7tk3k,"Since OpenAI launched GPT-4V, it has given big models unprecedented image semantic understanding capabilities. Nowadays, the new direction of AI is multimodal, and the industry generally believes that “AI multimodal” is the next explosive track.

&#x200B;

https://preview.redd.it/ankmb1nnynmc1.png?width=1000&format=png&auto=webp&s=e386a80db6cce6b4b88150767e97ccbd32c2bc91

**AI enters the multimodal era**

Multimodal models can handle multiple data inputs, and their design is more complex, which may require integrating the outputs of multiple sub-networks. This design enables multimodal models to capture interactions and correlations between different data sources, providing multidimensional information for the task.

Multimodality enables the model to access more data and can process content such as text, audio, images, video, and industrial domain code.

Big Model has not only entered a new phase in deep model training, but also innovated in multimodal fusion. In the future, multimodality will also help the model to recognize richer scenarios in practical applications, meet the important needs of AI technology, and realize the continuous development of general AI.

**Multimodal AI**

Looking at the global AI industry, large technology companies are transforming their traditional AI solution business models, shifting their focus to the research and development of AI macrolanguage models, and further exploring the cutting-edge field of multimodal macrolanguage models in depth.

**Google**

In the previous Google Developers Conference, Google once again had the opportunity to prove itself to the public. At the conference, the self-developed PaLM large language model was released, as well as a series of AI evolution of its products, and Gemini became the industry’s first native multimodal large model.

Gemini 1.0 is said to have native multimodal capabilities, capable of processing video, audio, images, text and code and other forms of content, and better performance than the existing “patchwork” multimodal big model. From the examples in Google’s official technical documentation, Gemini is not only capable of bimodal conversions (e.g., text-to-graph or text-to-video) but can also handle complex tasks that require multimodal conversions.

**Microsoft**

Sources point out that the multimodal capability of Microsoft’s GPT-4 is the most important point of its improvement over GPT-3. In fact, before GPT-4, Microsoft’s KOSMOS-1 achieved very surprising results in the multimodal direction.KOSMOS-1 is a multimodal model that can take images and text as input and generate a piece of text.KOSMOS-1 is used in multimodal conversations, multimodal QA as well as multimodal reasoning, OCR, looking at map computing, and image digit recognition for example.

KOSMOS-1 can be regarded as the predecessor of GPT-4 in terms of multimodal capability, and its two important theoretical foundations of chain of thought and emergent capability are also the most basic theoretical foundations of GPT-4. There is also a certain possibility that GPT-4 may have borrowed some of the ideas from KOSMOS-1.

**Apple**

Jan. 30 (Bloomberg) — Recently, a developer discovered technical code related to large models in the Developer Preview Beta of iOS 17.4. The code suggests that Apple is working on a new version of Siri powered by big models, and the code shows that the new version includes a private framework called “SiriSummarization,” which calls on OpenAI’s ChatGPT API.

The addition of this framework could make Siri even better at processing information and answering questions. Meanwhile, the code also shows that Apple is testing four different AI multimodal models, including their own “Ajax”, as well as two large external models, OpenAIGPT and Google’s Flan-T5.

**Meta**

Meta introduced AnyMAL a powerful language model capable of understanding multiple modal signals and generating textual responses (including image, video, audio and IMU motion sensor data). Recently, Meta officially announced the launch of a new open-source big model, Code Llama 70B.

Code Llama 70B is reported to have an accuracy of 53% in the HumanEval benchmark test, exceeding the 48.1% of the GPT-3.5 and approaching the 67% of the GPT-4 reported in an OpenAI paper, making it the highest-rated open-source big model to date.

Code Llama is based on Llama 2, which helps developers create code from prompts and debug manually written code. According to Meta, Code Llama, 70B can handle more content and better help developers deal with problems encountered during the multimodal era of AI.

**Conclusion**

Overall, big models are moving towards multimodality, which may become the new entrance of future interaction. Multimodal capabilities will become an important focus of future competition among the giants in the AI field, and in the future, the development of multimodal capabilities will bring more possibilities for the wide application of AI technology.",/r/WallStreetbetsELITE/comments/1b7tk3k/entering_the_multimodal_ai_era_4_core_leading/,WallStreetbetsELITE,r/WallStreetbetsELITE,False,relevant,highly relevant
"""Nuggets of Gold"" - AI Music edited with AI text-to-video",JJSmith1987,https://v.redd.it/6hbi4wz0lnmc1,2,0,1709704487.0,1b7saei,,/r/AI_Music/comments/1b7saei/nuggets_of_gold_ai_music_edited_with_ai/,AI_Music,r/AI_Music,True,relevant,neutral
Scalable image and video generation models and the coming shitstorm of corporations selling our collective output back to us,Amazing_Painter_7692,https://www.reddit.com/r/aiwars/comments/1b7jsot/scalable_image_and_video_generation_models_and/,11,63,1709680319.0,1b7jsot,"The [Stable Diffusion 3 paper](https://stability.ai/news/stable-diffusion-3-research-paper) came out today and demonstrated virtually unlimited scaling of text-to-image and text-to-video models. OpenAI reported the same thing for video models with Sora. What does that mean? In basic terms, the more compute you throw at the model, the better it gets, without any obvious limitation.

This is the quality that brought LLMs into the realm of ChatGPT, obliterating countless creative industries involving writing overnight.

If you are a commercial visual artist, it is difficult for me to see a future in which you are not using generative AI in some capacity. I think I would appeal to the antagonists of the technology to consider supporting open development of open models. We are now in the position where even if we could somehow stop generative models in our own countries from being trained on the inputs that we helped craft, we can't stop every country from doing so. And the generative output of these lawless models will be kosher to be used as training data for any new generative model over here.

I think that this is a pandora's box that has been opened and the best outcome that could be achieved is that at least artists have access to the models that have been trained on their collective output, and can use them free of charge to create their own comics, animated TV shows on YouTube, and so on. The worst case outcome seems that we will end up with these tools constrained to mega-corporations that derivatives of our own art back to us.

Many in the open source community in generative art dedicate their own time to making tools available to everyone, free of charge and with zero expectation of profit. It might seem like anathema to consider changing your sentiments about this technology, but the outcome wherein most anti-AI people just end up as useful idiots promoting corporate regulation of the technologies trained using their own works is much worse.",/r/aiwars/comments/1b7jsot/scalable_image_and_video_generation_models_and/,aiwars,r/aiwars,False,relevant,highly relevant
[2403.02332] UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control,ninjasaid13,https://arxiv.org/abs/2403.02332,1,0,1709654568.0,1b78stv,,/r/ninjasaid13/comments/1b78stv/240302332_unictrl_improving_the_spatiotemporal/,ninjasaid13,r/ninjasaid13,False,not relevant,highly relevant
CMV: Generated by the user AI entertainment will not replace human generated content,Subtleiaint,https://www.reddit.com/r/changemyview/comments/1b73ppy/cmv_generated_by_the_user_ai_entertainment_will/,16,88,1709640614.0,1b73ppy,"The recent reveal of the Sora text to video generator has sparked a new discussion on the future of home entertainment with one view being that, in the future, users will simply type in a prompt (such as 'show me a high fantasy TV show staring Clark Gable and Marilyn Monroe involving a quest to find lost treasure) and a bespoke high quality TV show or film will be generated for them. My view is that this will never happen.

Sora can currently generate high quality video, up to a minute long, with no sound based on simple prompts. Bar some minor flaws the videos look amazing and it is reasonable to believe that it won't be long until longer videos, without the flaws and with full audio (including speech) will be possible. However there is still a huge leap from better Sora videos to quality entertainment and that leap will be much harder than all the progress AI has made so far, so hard in fact that I don't believe it will ever be made.

The first problem is can AI create a complex narrative with multiple characters that is not only compelling but also logical? The answer to that is no and there's no reason to think it can or will be able to. AI is not creative, it's predictive, it doesn't come up with an idea and then expand on that idea it simply shows you something that it thinks relates to your prompt, it's an illusion of creativity.

The next issue is performance and staging, AI doesn't know what makes entertainment good, it only knows what entertainment looks like. This means it will never understand creativity in a way that a human will and will be unable to produce something that a human can.

Next up is quality control, if an AI made bespoke content for a million different users the range of quality would range dramatically with much of the content unwatchable, no entertainment platform can succeed if much of its content isn't, at least, competently made.

That's my view at the moment, I'd be interested to know if anyone can point towards AI currently creating, or working towards creating, viable entertainment.",/r/changemyview/comments/1b73ppy/cmv_generated_by_the_user_ai_entertainment_will/,changemyview,r/changemyview,False,relevant,highly relevant
Update Announcement (05/03/2024),SeaArt_Ai,https://www.reddit.com/r/SeaArt_Ai/comments/1b71ek3/update_announcement_05032024/,5,1,1709631657.0,1b71ek3,"Dear SeaArt.AI user, 

We're glad to inform you that our platform has been updated with a series of exciting new features and optimizations, all aimed at bringing you with unparalleled creation experience!

💥 **Important Update: The whole new feature - Canvas is available now!** The Canvas(beta) is finally open to all users after several updates! You can enter the Canvas(beta) page and experience the new creation flow by tapping the drop-down option beside Generate in the navigation bar!

1. The first **Creator Incentive Program** starts: all creators can obtain official incentives by publishing **AI character, models, LoRA templates, and Canvas**! (Check the event page for detailed rules.)

2. Updates on **Creation Flow** to enable more convenient advanced creation:

2.1. New ControlNet types: Recolor and IP-Adapter for SD1.5; Recolor, IP-Adapter, and Revision for SDXL.

2.2. Added that creations will be sorted by new tags when selecting models/LoRAs.

2.3. Added ""My LoRA"" and ""Recently Used"" tabs to the SDXL popup.

2.4. Added a ""2K"" option in the Img2Img/ControlNet - Basic Settings - Image Mode.

2.5. Added that LCM is available for Txt2Img, Img2Img, and ControlNet.

2.6. Added that LCM is available for SD1.5 & SDXL.

2.7. Added that SDXL is available for Img2Img and ControlNet.

2.8. Added permissions for users to adjust some parameters in the Advanced Config.

2.9. Added the WebP image format in the Remove Background.

3. **Swift AI Updates**

3.1. Optimized the UI layout.

3.2. Added the Text-to-Video tool.

🌊 Other Updates 🎨

1. Added an ""**iShow"" tab** on the home page.

2. Added a top navigation bar to the detailed pages of works and models.

3. Added a blog page.

✨We will keep going all out to provide you with a better product experience. Thanks for your support for SeaArt.AI all the time! If you have any questions, please feel free to contact us for customer service.

Thank you!

SeaArt.AI Team

https://preview.redd.it/xey0a32gkhmc1.png?width=1061&format=png&auto=webp&s=1b173b863e8105575868662b67aa205eb84b4409",/r/SeaArt_Ai/comments/1b71ek3/update_announcement_05032024/,SeaArt_Ai,r/SeaArt_Ai,False,relevant,highly relevant
Revolutionizing Creativity: Sora's AI-Powered Text-to-Video Magic Unveiled!,zan-max,https://youtu.be/vKs3PP1DuDk,1,0,1709620029.0,1b6yk8w,,/r/Youtube_Video_posts/comments/1b6yk8w/revolutionizing_creativity_soras_aipowered/,Youtube_Video_posts,r/Youtube_Video_posts,False,not relevant,highly relevant
Pika Labs AI Text to Video Generator (Free),Classic-Friendship44,https://pikalabsai.org/,1,0,1709618443.0,1b6y3vt,,/r/u_Classic-Friendship44/comments/1b6y3vt/pika_labs_ai_text_to_video_generator_free/,u_Classic-Friendship44,u/Classic-Friendship44,False,relevant,neutral
7 Things Claude 3 Does Better Than Gemini And ChatGPT,soubhagya_sahu,https://www.reddit.com/r/aitoolonline/comments/1b6py71/7_things_claude_3_does_better_than_gemini_and/,1,0,1709595334.0,1b6py71,"Recently Anthropic, the company behind the chatbot Claude [announced](https://www.anthropic.com/news/claude-3-family) its Claude 3 model family, which is the most advanced model for cognitive tasks beating GPT4 and [Gemini](https://aitoolonline.com/bard-is-now-known-as-gemini-lunched-gemini-advanced).

Claude 3 has 3 most powerful models in the model family. They are Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. (In the ascending order of capability)

These models are trained to offer more powerful performance with accuracy, speed, and optimization cost.

Opus and Sonnet are now available to use in Claude Chatbot and the Claude API. Haiku will be available soon.

Here is a chart to understand these 3 models with their intelligence and cost per million tokens.

[ ](https://preview.redd.it/z3advsqfkemc1.jpg?width=2200&format=pjpg&auto=webp&s=d997bdb6cd76edc291177d7f1f231c731c45d595)

You can use Sonnet for free, which is the default model for Claude Chatapp.

If you want to try the most powerful model Claude 3 Opus, you can subscribe to the pro plan which costs $20/month.

Let’s look at some of the things that Claude 3 models can do better than other models like GPT 4 of ChatGPT and Gemini. 

## Things Claude 3 Does Better Than Others

### 1. Better Performance In Benchmarks

[ ](https://preview.redd.it/gmwjplmikemc1.jpg?width=901&format=pjpg&auto=webp&s=0d3fa21bdd2bdaf216c7130fe359679bc3720062)

Claude 3’s most powerful model, Opus outperforms the most common evaluation benchmarks in AI systems. It does better than GPT-4, GPT-3.5, Gemini 1.0 Ultra, and Gemini 1.0 Pro.

The opus model gives better undergraduate-level expert knowledge (MMLU), graduate-level expert reasoning (GPQA), basic mathematics (GSM8K), and more.

It does a good job of handling complex tasks and records near-human levels of fluency.

But the other models like Sonnet and Haiku are slightly less capable in comparison to [GPT-4](https://aitoolonline.com/chatgpt-new-features-gpt-4-turbo-gpt-store-custom-gpts-devday). But when I test the Sonnet model it gives better results than their previous model.

### 2. Faster Response Time

The Claude 3 models are capable of responding to live customer chats, auto completions, and data extraction tasks. It gives the results quickly and in real-time.

Between these models, Haiku, claims to be the fastest and most cost-effective model on the market for its category.

It can read an information and data-intense research paper with graphs and charts in less than three seconds.

### 3. Improved Multi-Lingual Capabilities

All Claude 3 models are improved in conversing in non-English languages like Spanish, Japanese, and French. It shows the increase in capabilities in analysis, forecasting, content creation, and code generation.

### 4. Improved Vision Capabilities

The Claude 3 models are improved in terms of vision capabilities with other leading models.

The models can process a wide range of charts, graphs, and diagrams. You can use this feature to analyze your wide variety of data from PDFs, flowcharts, and presentation slides.

Here is a chart of Claude 3 models and other leading models compared.

As you can see the most effective model is Gemini 1.0 Ultra beating other models like Claude 3 Opus, Sonnet, Haiku, GPT-4V, and Gemini 1.0 Pro. The Claude 3 models are powerful if you consider these leading models.

[ ](https://preview.redd.it/4rpj87fkkemc1.jpg?width=1024&format=pjpg&auto=webp&s=8f728270f4f647f52ee5316c018e146e24c9092c)

### 5. Improved Accuracy And Less Hallucination

In my experience, the previous Claude model hallucinated a lot but after using the latest Claude 3 Sonnet, I can say it improved a lot. It understands the prompts and the results I want.

As Claude 3 Claims Opus gives a twofold improvement in accuracy compared to the previous model, Claude 2.1. In addition to more correctness in Claude 3 models they are going to add reference materials to verify their answers.

Here is a chart of comparison between Claude 3 Opus and Claude 2.1 on accuracy.

[ ](https://preview.redd.it/ymto24imkemc1.jpg?width=1024&format=pjpg&auto=webp&s=31726472b0cdbe993ff7571054141ac62eb4b0e2)

### 6. Long-Term Memory And Recall

The Claude 3 models can process inputs exceeding 1 million tokens and Opus achieved near-perfect recall with over 99% accuracy.

### 7. Reduced Biases

The biases in the models are an ongoing problem.

However, Claude 3 shows fewer biases than their previous models according to the Bias Benchmark for Question Answering (BBQ).

The company is making an effort to reduce biases and give the answer with neutral responses in their models.

## Closing Note

The new Claude 3 models are powerful and helpful, use them for your use. You can use these new model on their chat app.

The sonnet model is free to use you can give it a try and tell how it compares to the previous model.

Claude 3 surely raised the bar for improvement in AI chatbots and forced to make improvements in other Chatbots.

It sure beats Gemini and ChatGPT in some cases and getting better day by day. You should give it a try and share your opinion.

Read More:

* [How To Generate UI Designs For Free Using Galileo AI](https://aitoolonline.com/how-to-generate-ui-designs-for-free-using-galileo-ai)
* [OpenAI’s Text-To-Video Generator “Sora”: Here 10 Most Impressive Videos](https://aitoolonline.com/openais-text-to-video-generator-sora-here-10-most-impressive-videos)
* [OpenAI’s Guide To Prompt Engineering: 10 Tips To Know About ChatGPT](https://aitoolonline.com/openais-guide-to-prompt-engineering)
* [How To Access Dalle 3 For Free Using Bing Image Creator](https://aitoolonline.com/access-dalle-3-for-free-using-bing-image-creator)",/r/aitoolonline/comments/1b6py71/7_things_claude_3_does_better_than_gemini_and/,aitoolonline,r/aitoolonline,False,relevant,highly relevant
Sora: la nuova intelligenza artificiale di Open AI,Specific_Special_157,https://www.reddit.com/r/universofacile/comments/1b6kwnk/sora_la_nuova_intelligenza_artificiale_di_open_ai/,1,0,1709583312.0,1b6kwnk,"Sora è la nuova intelligenza artificiale di Open AI. Essa è in grado di generare video da un testo (text to video), ma al momento è solamente prodotto di ricerca. Scopriamola insieme.
Open AI presenta Sora
Open AI si è ormai affermata come leader del settore dell’intelligenza artificiale

https://www.enkey.it/2024/03/04/sora-la-nuova-intelligenza-artificiale-di-open-ai/",/r/universofacile/comments/1b6kwnk/sora_la_nuova_intelligenza_artificiale_di_open_ai/,universofacile,r/universofacile,False,relevant,neutral
AI voice over videos on YouTube and Sora OpenAI training,zahreela_saanp,https://www.reddit.com/r/developersIndia/comments/1b6g57l/ai_voice_over_videos_on_youtube_and_sora_openai/,1,1,1709571972.0,1b6g57l,"I got into stable diffusion and the likes recently trying to train some LoRAs and I have been noticing a low-effort trend on YouTube for the past 7-8 months where a video short with AI voice over describing what is happening in the video garners lakhs of views and likes.

Recently, OpenAI announced Sora - the text-to-video AI model. And then it hit me, could it be possible that people are being asked to caption videos and put them up on YouTube? e.g. If I ever tasked to create huge dataset of videos with captions, this seems like the perfect way to do it. Crowdsource the captions, do some sentiment analysis based on the views or likes or comments and then use that to train my AI video model.

Is this known in the tech circles or am I just going cuckoo?",/r/developersIndia/comments/1b6g57l/ai_voice_over_videos_on_youtube_and_sora_openai/,developersIndia,r/developersIndia,False,relevant,highly relevant
[Business] - Chinese researchers aim to ‘reproduce’ OpenAI’s text-to-video model Sora with help from open source community | South China Morning Post,AutoNewspaperAdmin,https://www.scmp.com/tech/tech-trends/article/3254107/chinese-researchers-aim-reproduce-openais-text-video-model-sora-help-open-source-community?utm_source=rss_feed,1,0,1709558345.0,1b6ap5v,,/r/AutoNewspaper/comments/1b6ap5v/business_chinese_researchers_aim_to_reproduce/,AutoNewspaper,r/AutoNewspaper,False,relevant,highly relevant
Template that leverages OpenAI's latest APIs (Including RAG & Persistent Memory),madkimchi,https://www.reddit.com/r/ChatGPTPro/comments/1b67ujx/template_that_leverages_openais_latest_apis/,2,9,1709548573.0,1b67ujx,"I'd like to share with you a template app I've been working on for the last few weeks, that includes working examples of the latest OpenAI Apis. Similarly to previous project, I've given it a name from the periodic table of Elements: **Titanium:**   [https://github.com/athrael-soju/Titanium](https://github.com/athrael-soju/Titanium)

# What is Titanium?

Titanium is a modern web application built with Next.js, leveraging the latest OpenAI APIs to offer an advanced Generative and Conversational AI experience. It's still pretty much a prototype, but I think it's a good start. Here's a list of some of the features:

* Multi-user Authentication using next-auth, including a custom CredentialProvider for guest accounts.✅
* Customizable, Multipurpose Assistants with File Upload support. Also supports complete deletion of all Assistant related data.✅
* Vision via 'gpt-4-vision-preview'. Currently supports Image Analysis for multiple urls. File uploads may come later, but not a priority.✅
* Text to Speech (TTS), Supporting tts-1, tts-1-hd and all available voice models.✅
* Speech to Text (STT), available via button toggle in the input chat box.✅
* Retrieval Augmented Generation (RAG), Using advanced document parsing by Unstructured.io API, ada-003 Embeddings by OpenAI and Pinecone Serverless for fast and efficient indexing & retrieval.✅

Some of the features I'm working on include:

* Persistent multi-user memory.✅
   * NoSQL Based.✅
   * Vector Based.✅
* Image Generation via DALLE-3.🚧
* Video
   * Video to Text (VTT) - For use in RAG based applications. 🚧
   * Text to Video (TTV) - As per latest reveal from OpenAI's latest Diffusion Model, called [Sora](https://openai.com/sora).🚧

And the obligatory:

* Bug fixes and performance improvements.🐛
* Refactor the spaghetti.🍝

I hope you could find it useful. Let me know if you have any questions/suggestions, or even if you're interested in collaborating.",/r/ChatGPTPro/comments/1b67ujx/template_that_leverages_openais_latest_apis/,ChatGPTPro,r/ChatGPTPro,False,relevant,highly relevant
New Community Listing Onboarding!,ConsequenceOld7093,https://www.reddit.com/r/DeFianceApp/comments/1b663t1/new_community_listing_onboarding/,2,1,1709541678.0,1b663t1,"Sora AI community page is now live on [https://DeFiance.app/project/Sora\_ai](https://DeFiance.app/project/Sora_ai).   


$SORA is now listed on #DeFianceSwap.   


Sora AI is a meme token project imitating OpenAI's new text-to-video generating AI called SORA, it has nothing to do with OpenAI's SORA.   


Learn more at: [https://users.DeFiance.app](https://users.DeFiance.app).   
\#SoraAI #memecoins #DeFiants #Crypto #Capture #Bullish #DeFi #DeFianceApp   


https://preview.redd.it/1f37b9d35amc1.png?width=1920&format=png&auto=webp&s=b991c1b0adf79352db777414c2d11aac06e74ba5",/r/DeFianceApp/comments/1b663t1/new_community_listing_onboarding/,DeFianceApp,r/DeFianceApp,False,relevant,neutral
Seeking honest opinions.,vigneshvp,https://i.redd.it/m8ata8ijl8mc1.png,4,8,1709522990.0,1b60r6r,"What do you think of this app icon?
for a Text-to-Video app.
",/r/iosapps/comments/1b60r6r/seeking_honest_opinions/,iosapps,r/iosapps,False,not relevant,highly relevant
"Pictory AI - Creating professional videos has never been easier, thanks to the power of AI Video Editor. With AI Video Editor, small businesses and social media teams can quickly and easily create high-quality videos.",bamodop,https://www.youtube.com/watch?v=tmhscMtLeRU,1,0,1709510483.0,1b5wd6n,,/r/bendigoaerial/comments/1b5wd6n/pictory_ai_creating_professional_videos_has_never/,bendigoaerial,r/bendigoaerial,False,relevant,highly relevant
"Will a feature whereby generative AI """"scans/analyzes an uploaded 100-pages-worth-of-text file ""online or via a generative AI App"""" & generates an at least 5 minute long video that's in accordance with what the text file's text paints"" launch soon?",Jeff_Chileno,https://i.redd.it/t8ysmypyx4mc1.jpeg,1,0,1709478738.0,1b5jf08,"I've heard of the generative AI called Sora, which is extremely new, and can generate videos based on a few sentences that a person types up,… but I haven't yet heard of a generative AI that can generate videos based on a one-shot 100-pages-worth-of-text file. If the generative AI called ""Sora"" ever becomes able to do that, plenty of books can simply be scanned/analyzed by ""Sora"" and have generated videos made that are in accordance with the ""world and/or story"" of those books. Would be awesome, but I've never seen it done by any AI yet.

https://www.linkedin.com/pulse/openai-unveils-sora-generative-ai-text-to-video-david-cronshaw-ppllc

https://theconversation.com/openais-new-generative-tool-sora-could-revolutionize-marketing-and-content-creation-223806",/r/QuestionEverythingNow/comments/1b5jf08/will_a_feature_whereby_generative_ai/,QuestionEverythingNow,r/QuestionEverythingNow,False,relevant,highly relevant
"Will a feature whereby generative AI """"scans/analyzes an uploaded 100-pages-worth-of-text file ""online or via a generative AI App"""" & generates an at least 5 minute long video that's in accordance with what the text file's text paints"" launch soon?",Jeff_Chileno,https://i.redd.it/1f7rk3esx4mc1.jpeg,1,0,1709478679.0,1b5je7a,"I've heard of the generative AI called Sora, which is extremely new, and can generate videos based on a few sentences that a person types up,… but I haven't yet heard of a generative AI that can generate videos based on a one-shot 100-pages-worth-of-text file. If the generative AI called ""Sora"" ever becomes able to do that, plenty of books can simply be scanned/analyzed by ""Sora"" and have generated videos made that are in accordance with the ""world and/or story"" of those books. Would be awesome, but I've never seen it done by any AI yet.

https://www.linkedin.com/pulse/openai-unveils-sora-generative-ai-text-to-video-david-cronshaw-ppllc

https://theconversation.com/openais-new-generative-tool-sora-could-revolutionize-marketing-and-content-creation-223806",/r/QuestionAnswerCentral/comments/1b5je7a/will_a_feature_whereby_generative_ai/,QuestionAnswerCentral,r/QuestionAnswerCentral,False,relevant,highly relevant
How To Generate UI Designs For Free Using Galileo AI,soubhagya_sahu,https://www.reddit.com/r/aitoolonline/comments/1b5ak7w/how_to_generate_ui_designs_for_free_using_galileo/,2,0,1709447853.0,1b5ak7w,"In this world of AI and you can do really complex and time taking tasks in seconds. With the help of AI tools like Galileo AI, you can design website mockups in a few seconds.

Galileo AI is an AI design tool that helps users to generate website designs, and app design mockups in seconds. It also adds AI-generated graphics and images for your convenience and use.

This AI tool is an incredible feature where you don’t have to spend hours of time coding or designing web apps or mockups.

But you have to make some changes for your own use and polish the design for your use case. 

## What Is Galileo AI? 

Galileo AI is a generative AI tool tool that generates UI design with text and image prompts.

It is the best tool for designers and builders. You can use this tool to generate mobile app design, website design, web app design, and any pages you want to design for your website or app.

Then you will able to export this to Figma to customize according to your use cases.

Then you can use the code and design that was generated by Galileo AI. You can try Galileo AI for free with 200 credits and can export up to 3 designs to Figma.

With these 200 credits, you can generate up to 20 designs.

## Features Of Galileo AI

You can take the design to the next step using this tool. Galelio AI made it possible to use text and image prompts to create UI designs to use in websites and apps.

1. Text to UI – Galileo AI makes it possible to generate UI designs from text prompts. You can give prompts to the tools and it will generate an awesome design and you can tweak and alter the design as you like.
2. Image to UI – Galileo AI also makes it possible to generate UI designs from an image. You can use a visual to give guidance for design style, elements, and the type of design you want to generate. 

## Pricing Of Galileo AI

Here is the pricing structure of Galileo AI.

1. Standard Plan – $19/month with 1200 credits per month (approx. 120 designs generated and edits)
2. Pro Plan – $39/month with 3000 credits/month (approx. 300 designs generation and edits)
3. Free Plan – 200 Credits and can export 3 designs to Figma. You can generate up to 20 designs.

The Pro Plan gives you the option to use private mode for your designs which is only visible to you but in the standard plan, the design you have generated will be shown in the explore feed. Both plans give you the general commercial terms of use.

They have clear instructions: only standard and pro plan subscribers and enterprise customers are allowed to use any designs they generate from Galileo for commercial use.

## How To Use Galileo AI To Generate UI Designs

Using Galileo AI is very easy, follow these instructions to learn more about how you can use it.

* Go to [usegalileo.ai](http://usegalileo.ai/) and sign up.
* Then describe your design in detail. 
* You can choose to generate a design from “**Text to UI”** or “Image to UI”.
* Select “**Text to UI**” to generate design from text prompts. 
* Select “Image **to UI**” to generate the design from text prompts. 

You can upload an image, and then you can tell prompts. it will generate the desired results.

You can make changes to designs after the result if you want any changes.

This tool will generate the first iteration of designs then you have to make changes according to your use case or design requirement to make it useful.

You can copy the design to the Figma board for further work.

## Conclusion

Galileo AI is useful for design assistants and for generating initial design mockups. This tool will save lots of time for indie hackers, occasional UI designers, and professional UI designers.

So if you have an app, website, or other digital product needing a beautiful professional design, take Galileo AI for a spin today. In mere minutes, you can generate beautiful designs to bring your concepts to life and appeal to modern users.

More:

[OpenAI’s Text-To-Video Generator “Sora”: Here 10 Most Impressive Videos](https://aitoolonline.com/openais-text-to-video-generator-sora-here-10-most-impressive-videos) 

[How To Create AI Images With Google Gemini](https://aitoolonline.com/how-to-create-ai-images-with-gemini) 

[How To Access Dalle 3 For Free Using Bing Image Creator](https://aitoolonline.com/access-dalle-3-for-free-using-bing-image-creator)",/r/aitoolonline/comments/1b5ak7w/how_to_generate_ui_designs_for_free_using_galileo/,aitoolonline,r/aitoolonline,False,relevant,highly relevant
"How AI Will Change Movies, Games and Music Industry",managebiznow,https://www.reddit.com/r/u_managebiznow/comments/1b4yq8n/how_ai_will_change_movies_games_and_music_industry/,1,0,1709413522.0,1b4yq8n,"The article from AimROI discusses the transformative impact of AI on the entertainment industries, including movies, games, and music. It highlights how AI innovations, like OpenAI's text-to-video product and Google's Genie, are redefining creativity, accelerating production processes, and making content creation more accessible. 

For more detailed insights, visit the article directly [How AI Will Change Movies, Games and Music Industry](https://aimroi.com/technology/how-ai-completely-change-entertainment-industries-movies-games-music/)",/r/u_managebiznow/comments/1b4yq8n/how_ai_will_change_movies_games_and_music_industry/,u_managebiznow,u/managebiznow,False,not relevant,highly relevant
In your opinion what are the best A.I. tools you use and for what? ,mahkahdamian,https://www.reddit.com/r/ArtificialInteligence/comments/1b4wpxe/in_your_opinion_what_are_the_best_ai_tools_you/,33,27,1709408398.0,1b4wpxe,"Been embracing A.I. way more now and even learning about prompt engineering. I'm a CS Major with an Associates degree and even thinking about heading down that direction. Ive been trying around with a lot of things and different ChatBots. But in your opinion what's the best or favorite for each category like ChatBots, Text to video, A.I. art, etc?

",/r/ArtificialInteligence/comments/1b4wpxe/in_your_opinion_what_are_the_best_ai_tools_you/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
"Mind-Blowing AI tools, 10x Crypto Treasures , Text to video tools ""Beyo...",THEABLAB,https://youtube.com/watch?v=x0Q66m4H_Yo&si=HivnwGB5ExkUaLur,1,0,1709402614.0,1b4ufk5,,/r/u_THEABLAB/comments/1b4ufk5/mindblowing_ai_tools_10x_crypto_treasures_text_to/,u_THEABLAB,u/THEABLAB,False,relevant,highly relevant
"Konec videí, jak je známe dnes? O krásné záběry i věrohodné zvuky se postará AI!",SvetAndroida,https://i.redd.it/804bxxpbdylc1.jpeg,1,0,1709399156.0,1b4t28y,"
 	Schopnosti umělé inteligence se rozvíjejí působivým tempem
 	Krásná videa dovede tvořit model Sora od OpenAI
 	Chybí jim ale zvukové efekty, působivé řešení je však za rohem

Mnozí konspirátoři jsou přesvědčeni, že umělá inteligence ovládne svět, a nedávné oznámení modelu Sora od společnosti OpenAI je v jejich myšlenkách dost možná utvrdilo. Americká organizace, stojící za populárním konverzačním nástrojem ChatGPT nebo generátorem obrázku DALL-E, představila službu, která hravě dokáže vytvořit působivé video na základě textového příkazu.

I záběry, na které by za normálních okolností byly potřebné celé filmové štáby, a to nepočítaje všechny herce, zvládne Sora vyplivnout v rozlišení FullHD s maximálním ohledem na detaily scén, komplexitu kamerových záběrů, nebo dokonce lidské emoce. Abyste dostali hezké video s japonskou tématikou, bohatě postačí i prompt kratší 50 slov. Viz příspěvek níže.

Introducing Sora, our text-to-video model.
Sora can create videos of up to 60 seco... celý článek si můžete přečíst na https://www.svetandroida.cz/elevenlabs-generator-zvuku-ai/?feed_id=33388&_unique_id=65e35c72bd3b0&utm_source=Reddit&utm_medium=krf&utm_campaign=FS%20Poster",/r/androidCZSK/comments/1b4t28y/konec_videí_jak_je_známe_dnes_o_krásné_záběry_i/,androidCZSK,r/androidCZSK,False,relevant,highly relevant
weekly roundup for week of 2/23-3/1,sub-roundup,https://www.reddit.com/r/editors/comments/1b4owud/weekly_roundup_for_week_of_22331/,1,0,1709388017.0,1b4owud,"### Most Upvoted Posts

1. [Won a 50k grant for my organization with an edit I did. (267 🡅)](/r/editors/comments/18ze3pp/won_a_50k_grant_for_my_organization_with_an_edit/)
2. [My first feature film edit is going to theaters (218 🡅)](/r/editors/comments/19a881c/my_first_feature_film_edit_is_going_to_theaters/)
3. [Sora (199 🡅)](/r/editors/comments/1asnrw8/sora/)
4. [Just hit version 100 of a 30 second edit (193 🡅)](/r/editors/comments/1b39yyj/just_hit_version_100_of_a_30_second_edit/)
5. [Leaving the industry... (184 🡅)](/r/editors/comments/1b2j78q/leaving_the_industry/)

### Most Commented Posts

1. [Leaving the industry... (272 💬)](/r/editors/comments/1b2j78q/leaving_the_industry/)
2. [What's up with all the Adobe hate? (235 💬)](/r/editors/comments/1ajplgd/whats_up_with_all_the_adobe_hate/)
3. [OpenAI announces Sora today, introducing their photorealistic text-to-video product (229 💬)](/r/editors/comments/1arrmbi/openai_announces_sora_today_introducing_their/)
4. [Sora (201 💬)](/r/editors/comments/1asnrw8/sora/)
5. [Editors, what are some common mistakes you've noticed in amateur film editing? (183 💬)](/r/editors/comments/1ahw9tn/editors_what_are_some_common_mistakes_youve/)",/r/editors/comments/1b4owud/weekly_roundup_for_week_of_22331/,editors,r/editors,False,relevant,less relevant
Chinese state TV airs AI-developed cartoon series amid Sora frenzy,FpRhGf,https://www.reddit.com/r/singularity/comments/1b3tspr/chinese_state_tv_airs_aideveloped_cartoon_series/,27,3,1709298303.0,1b3tspr,"[link](https://www.scmp.com/tech/policy/article/3253390/state-tv-airs-chinas-first-ai-developed-cartoon-series-sora-sets-frenzy-text-video-generation-tech)  
 Excerpt:

>State broadcaster China Media Group has started airing the country’s first cartoon series made with the help of generative artificial intelligence (GenAI) services, including text-to-video tools similar to [OpenAI’s Sora](https://www.scmp.com/tech/big-tech/article/3253034/openais-sora-pours-cold-water-chinas-ai-dreams-text-video-advancements-prompt-more-soul-searching?module=inline&pgtype=article), as the government calls on state-owned firms to pilot the use of AI in their businesses.

>The 26-episode series, *Qianqiu Shisong*, which debuted on Monday, features some of the most fabled classical Chinese poetry and their backstories, with each instalment lasting around 7 minutes, according to a statement published on the WeChat account of the Shanghai Artificial Intelligence Laboratory (SAIL).


Apparently it's an educational animated series geared towards elementary school students. This is the [closest](https://www.takungpao.com/news/232108/2024/0227/946154.html) I've found to an explanation on the process. Seems like they use text prompts to generate images for the characters and background. Then they select the character image, type a prompt for the motion and the same model will animate the character.

Someone uploaded the first 4 episodes [here](https://youtube.com/playlist?list=PLsw2iU9xmpfbnjQdwWVhgM2E0N9oP1_J7&si=KoXEx_g5SCtacbsn) and it's being updated daily. I tried looking into the comments on other sites, and the reception seems to mainly be dunking it for the AI and cheap-looking animation, or just thinking it's a nice educational content for kids.",/r/singularity/comments/1b3tspr/chinese_state_tv_airs_aideveloped_cartoon_series/,singularity,r/singularity,False,relevant,highly relevant
What is Sora AI? Everything to know about OpenAI's text-to-video tool - BBC Science Focus Magazine,tw_bot,https://news.google.com/atom/articles/CBMiOmh0dHBzOi8vd3d3LnNjaWVuY2Vmb2N1cy5jb20vZnV0dXJlLXRlY2hub2xvZ3kvb3BlbmFpLXNvcmHSAQA?oc=5&hl=en-US&gl=US&ceid=US:en,1,0,1709293006.0,1b3s7c1,,/r/tomorrowsworld/comments/1b3s7c1/what_is_sora_ai_everything_to_know_about_openais/,tomorrowsworld,r/tomorrowsworld,False,relevant,neutral
Impact Of Sora AI on Content Service Platform,Business_trendss,https://www.reddit.com/r/u_Business_trendss/comments/1b3nsq7/impact_of_sora_ai_on_content_service_platform/,1,0,1709275407.0,1b3nsq7,"The emergence of [Sora AI](https://openai.com/sora) has significantly transformed the landscape of Content Service Platforms (CSPs), offering a new paradigm in how content is managed, distributed, and consumed. This AI-driven technology has not only streamlined operations for businesses but also enhanced the user experience for consumers. In this blog, we'll delve into the impact of Sora AI on [Content Service Platforms](https://quadrant-solutions.com/market-research/market-share-content-service-platform-2022-worldwide-2412), exploring both its benefits and challenges. 

**What is Sora AI?** 

According to the OpenAI website, Sora is an advanced AI model designed to transform brief text descriptions into elaborate, high-definition video clips that can last up to a minute. This technology is named ""Sora,"" a Japanese term for ""sky,"" symbolizing its boundless potential and innovative capabilities. Sora excels in producing videos of high visual fidelity that precisely follow the provided text prompts, enabling the generation of both realistic and imaginative scenes from mere descriptions. 

OpenAI has released sample videos claimed to be directly generated by Sora, without any alterations. These samples include vivid scenarios such as ""A litter of golden retriever puppies playing in the snow, with their heads popping out,"" showcasing the model's ability to create highly realistic and dynamic scenes. The quality, texture, scene dynamics, camera movements, and consistency of these videos are so refined that it becomes challenging to distinguish between footage created by Sora and actual recorded video. 

**How to Access Sora?** 

Upon its release, [Sora Ai](https://medium.com/@dhumalsonali85/impact-of-sora-ai-on-content-service-platform-eedea857848c) is anticipated to provide a seamless user experience, encouraging in-depth exploration of its capabilities. Users might be able to access Sora through various means, such as a bespoke platform, integration with current OpenAI services, or via specific apps crafted to maximize its text-to-video conversion potential.

As OpenAI unveils further information regarding Sora's incorporation and application usage, the precise methods of accessing it will be more clearly defined. 

**How Does Sora Work?** 

According to OpenAI's technical documentation, Sora is primarily engineered to interpret written material, transforming it into a sequence of images and videos that effectively communicate the desired narrative or message. This operation utilizes advanced AI methodologies that analyze the text to identify core themes, emotions, and narrative components, subsequently translating these elements into visual representations. 

This efficient conversion of text into visual content likely utilizes advancements in natural language processing, machine learning, and computer vision technologies to ensure a fluid transition. 

**Impact of Sora AI on Content Service Platform** 

The impact of AI technologies like Sora AI on Content Service Platforms (CSPs) can be substantial, transforming the way content is managed, distributed, and consumed. Here are several key points that highlight the impact of such AI on CSPs: 

**Enhanced Personalization**: Sora AI can analyze user behavior, preferences, and interaction data to deliver highly personalized content recommendations. This improves user engagement and satisfaction by ensuring users find content that is relevant to their interests more efficiently. 

**Automated Content Curation**: AI can automate the process of content curation, categorizing and tagging content based on its characteristics and user interactions. This reduces the manual effort required to manage large volumes of content and improves the discoverability of content within the platform. 

**Improved Content Creation:** With AI, CSPs can leverage natural language generation (NLG) to create content automatically. This can be particularly useful for generating reports, summaries, or even news articles based on data inputs, significantly reducing the time and resources needed for content creation. 

**Content Optimization:** AI algorithms can analyze content performance data to identify what types of content are most engaging or effective. This insight can guide content creators in optimizing their content strategies for better engagement and ROI. 

**Enhanced Security and Compliance:** AI can help in identifying and managing sensitive or copyrighted content, ensuring that the content distributed through the platform adheres to legal and regulatory standards. This is crucial for maintaining the trust and safety of the platform. 

**Efficient Search and Discovery:** AI-powered search functionalities can go beyond keyword matching to understand the context and intent behind user queries. This leads to more accurate and relevant search results, enhancing the user experience on the platform. 

**Predictive Analytics:** By analyzing user data and content interaction patterns, Sora AI can provide predictive insights regarding future content trends and user behaviors. This allows CSPs to proactively adapt their strategies to meet anticipated demands. 

[**Click Here for More**](https://quadrant-solutions.com/talk-to-analyst)

**Challenges and Ethical Considerations** 

While the applications of Sora AI are vast and varied, its adoption comes with challenges and ethical considerations. Issues related to data privacy, security, and the potential for bias in AI algorithms are of paramount concern. Ensuring transparency, accountability, and fairness in AI applications is crucial to build trust and mitigate potential negative impacts on society. 

**Conclusion** 

The impact of Sora AI on [Content Service Platforms](https://quadrant-solutions.com/market-research/market-share-content-service-platform-2022-worldwide-2412) is profound, offering transformative benefits in personalization, content management, and creation. However, as with any technological advancement, it is crucial to navigate the challenges with careful consideration. By addressing concerns related to data privacy, content quality, and bias, CSPs can harness the power of Sora AI to not only enhance their operations but also deliver exceptional experiences to their users. As the technology continues to evolve, the future of content service platforms looks promising, with Sora AI leading the way in innovation and efficiency. ",/r/u_Business_trendss/comments/1b3nsq7/impact_of_sora_ai_on_content_service_platform/,u_Business_trendss,u/Business_trendss,False,relevant,highly relevant
March 2024 What Are You Working On?,marc-kd,https://www.reddit.com/r/ada/comments/1b3fref/march_2024_what_are_you_working_on/,17,33,1709251322.0,1b3fref,"Welcome to the monthly [r/ada](https://www.reddit.com/r/ada/) *What Are You Working On?* post.

Share  here what you've worked on during the last month. Anything goes:  concepts, change logs, articles, videos, code, commercial products, etc, so long as it's related to Ada. From snippets to theses, from text to  video, feel free to let us know what you've done or have ongoing.

Please stay on topic of course--items not related to the Ada programming language will be deleted on sight!

[Previous ""What Are You Working On"" Posts](https://old.reddit.com/r/ada/search/?q=title%3A%22What+Are+You+Working+On%22&sort=new&restrict_sr=on&t=all)",/r/ada/comments/1b3fref/march_2024_what_are_you_working_on/,ada,r/ada,False,relevant,highly relevant
DeepMind’s New AI Makes Games From Scratch!,ZenTheMC,https://www.reddit.com/r/Asmongold/comments/1b39mjn/deepminds_new_ai_makes_games_from_scratch/,0,0,1709236620.0,1b39mjn,"**It's happening, boys! AI, text to video game. Open source google research! New Model.**

[**Youtube Video Link**](https://youtu.be/-ZSVkjukC1U?si=svwjoTvELHVUHXdc)

https://preview.redd.it/svo1gvplyklc1.png?width=1253&format=png&auto=webp&s=ed284e3ce5b4d9b98a5bdc0a5a3f1a14c56ed1eb",/r/Asmongold/comments/1b39mjn/deepminds_new_ai_makes_games_from_scratch/,Asmongold,r/Asmongold,False,relevant,neutral
LTX-Studio: A REAL AI-Powered Filmmaking Studio,saffronfan,https://www.reddit.com/r/ArtificialInteligence/comments/1b37kpx/ltxstudio_a_real_aipowered_filmmaking_studio/,2,1,1709231665.0,1b37kpx,"**Lightricks** launched [LTX Studio](https://ltx.studio/?utm_source=www.theedge.so&utm_medium=newsletter&utm_campaign=github-launches-enterprise-copilot), an AI tool to help creators make short films just by typing prompts. But it faces stiff competition in generative video. ([Source](https://ltx.studio/?utm_source=www.theedge.so&utm_medium=newsletter&utm_campaign=github-launches-enterprise-copilot))

**How It Works**

* Users describe a story idea and LTX Studio makes a script, storyboard, and characters.
* The AI lets you customize scenes, shot styles, settings, and characters; you can even edit duration, camera angles, and effects for each shot.
* This comes right after [Tyler Perry halts his $800M](https://www.hollywoodreporter.com/business/business-news/tyler-perry-ai-alarm-1235833276/?utm_source=www.theedge.so&utm_medium=newsletter&utm_campaign=github-launches-enterprise-copilot) film studio because of OpenAI's Sora

**Ambitious Capabilities**

* It aims to generate high-quality, realistic short films from text prompts.
* Promises to transform how movies are made through AI.
* Faces competition from OpenAI, Google, Meta, and startups in text-to-video space. Specifically, OpenAI's Sora

**PS:** Get the latest AI developments, tools, and use cases by joining one of the [fastest growing AI newsletters.](https://www.theedge.so/subscribe) Join **15000+ professionals getting smarter in AI.**",/r/ArtificialInteligence/comments/1b37kpx/ltxstudio_a_real_aipowered_filmmaking_studio/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
Sora alternatives?,Kindly_Chip_6413,https://www.reddit.com/r/OpenAI/comments/1b3777c/sora_alternatives/,2,3,1709230738.0,1b3777c,I need text to video that actually lets me use it (I do not mean those crappy ‘text to video’ browser apps that make your PowerPoint presentation),/r/OpenAI/comments/1b3777c/sora_alternatives/,OpenAI,r/OpenAI,False,relevant,highly relevant
"VisualVaultAI Review Bonus OTOs – World’s First MidJourney & Canva Killer That Creates Unique AI Art Images & Videos In 60 Seconds FLAT Millions Of DFY Video Templates With Reseller License Create Stunning Graphics, Blog Articles, All AI Content, blog posts, Text to Image, Text to Video",Agreeable_Income3763,https://www.marketingsharks.com/visualvaultai-review-otos-worlds-first-midjourney-canva-killer-thats-create-unique-ai-art-images-videos-in-60-seconds-flat/,1,0,1709228385.0,1b3682f,,/r/LessContentPublishing/comments/1b3682f/visualvaultai_review_bonus_otos_worlds_first/,LessContentPublishing,r/LessContentPublishing,False,relevant,highly relevant
Synthesia vs. Akool - which is better?,DJsummoner,https://www.reddit.com/r/faceswap/comments/1b30ttf/synthesia_vs_akool_which_is_better/,7,2,1709214829.0,1b30ttf,"I did Heygen vs Akool so I thought i would put this one too.

When comparing Synthesia and Akool, both platforms offer sophisticated AI video capabilities with realistic results. Each has its own set of pros and cons worth considering.

Starting with Synthesia, it boasts industry-friendly templates that are easily customizable, along with realistic facial expressions and head movements. However, its subscription model and limited free trial may be restrictive for non-corporate users, and some users may find the UI crowded and confusing, especially newcomers to AI technology.

On the other hand, Akool provides a user-friendly experience with a simple UI, making it ideal for beginners. It offers a credit-based pricing model, which is attractive for occasional or student users, and its diverse range of tools—from text to video presentations to AI image generation—provides versatility. However, while Akool excels in voice synchronization, its language and translation options are currently more limited compared to Synthesia, and certain features like multi-scene functionality in the Realistic Avatar video builder are still in development.

Ultimately, the choice between Synthesia and Akool depends on individual preferences and specific project needs, with both platforms offering unique advantages and room for improvement.",/r/faceswap/comments/1b30ttf/synthesia_vs_akool_which_is_better/,faceswap,r/faceswap,False,relevant,highly relevant
Hope the AI video editor can help me do my hw.......plz,Interesting-Rub-1897,https://www.reddit.com/r/studyAbroad/comments/1b2vu54/hope_the_ai_video_editor_can_help_me_do_my_hwplz/,0,2,1709196733.0,1b2vu54,"The AI Video Editor Navigation Website. Sometimes, when I'm unsure which AI video editor to use for creating videos, I've found that this website provides a list of tools that are extremely helpful.

As a student, I encounter numerous tasks and challenges, one of which is creating high-quality videos. Whether it's for school projects, social media content, or personal creativity, video production is a task that I often come across. However, lacking professional video editing skills can sometimes hinder the process of video creation. Luckily, there's now a magical place, a place that simplifies and adds fun to video production – that's the AI Video Editor Navigation Site!  
  
Let me explain how convenient this navigation site is. It consolidates various types of AI video editors. Whether you aim to post videos on Twitter, Instagram, or other social media platforms, you can find the necessary editing tools here. The most appealing aspect is its ability to intelligently recognize the platform you've chosen and automatically adjust your video to meet its requirements. This ensures that your video looks flawless across different platforms, providing a sense of reassurance!  
  
Additionally, this website boasts many superb features, such as text-to-video, script-to-video, PPT-to-video, and more. You can submit your creative ideas, well-written scripts, or even the PPT files provided by teachers to the AI video editor.  
  
I am afraid that i can't show it well, so i use chatgpt to help me wrote the introduction above. Hope the website can help u all.",/r/studyAbroad/comments/1b2vu54/hope_the_ai_video_editor_can_help_me_do_my_hwplz/,studyAbroad,r/studyAbroad,False,relevant,highly relevant
Maybe the AI video editor can help smart study........,Interesting-Rub-1897,https://www.reddit.com/r/study/comments/1b2vocn/maybe_the_ai_video_editor_can_help_smart_study/,0,4,1709196047.0,1b2vocn,"The AI Video Editor Navigation Website. Sometimes, when I'm unsure which AI video editor to use for creating videos, I've found that this website provides a list of tools that are extremely helpful.

It consolidates various types of AI video editors. Whether you aim to post videos on Twitter, Instagram, or other social media platforms, i can find the necessary editing tools here.

Additionally, this website boasts many superb features, such as text-to-video, script-to-video, PPT-to-video, and more. You can submit your creative ideas, well-written scripts, or even the PPT files provided by teachers to the AI video editor.

 I am afraid that i can't show it well Hope the website can help us.",/r/study/comments/1b2vocn/maybe_the_ai_video_editor_can_help_smart_study/,study,r/study,False,relevant,highly relevant
Recommand the AI video editor to smart study,Interesting-Rub-1897,https://i.redd.it/hitwid50jhlc1.png,1,1,1709195258.0,1b2vhpo,,/r/studytips/comments/1b2vhpo/recommand_the_ai_video_editor_to_smart_study/,studytips,r/studytips,False,relevant,highly relevant
Read Aloud For Me - AI Dashboard: Access All AI Tools in One App,enoumen,https://www.reddit.com/r/u_enoumen/comments/1b2rt49/read_aloud_for_me_ai_dashboard_access_all_ai/,1,0,1709181924.0,1b2rt49,"# Unlock the power of AI with this AI Dashboard. Access all major AI tools in one seamless app, designed to elevate your productivity and streamline your digital experience

iOs PRO: [https://apps.apple.com/ca/app/read-aloud-for-me-pro-ai-hub/id1600174099](https://apps.apple.com/ca/app/read-aloud-for-me-pro-ai-hub/id1600174099)

iOs: [https://apps.apple.com/ca/app/read-aloud-for-me-top-ai-gpts/id1598647453](https://apps.apple.com/ca/app/read-aloud-for-me-top-ai-gpts/id1598647453)

Windows: [https://apps.microsoft.com/detail/9pm03vfn90l8?hl=en-us&gl=CA](https://apps.microsoft.com/detail/9pm03vfn90l8?hl=en-us&gl=CA)

Web: [https://readaloudforme.com](https://readaloudforme.com/)

Android: [https://play.google.com/store/apps/details?id=com.readaloudforme.app](https://play.google.com/store/apps/details?id=com.readaloudforme.app)

Youtube: [https://youtu.be/4VSgLFTZR90?si=akvWhKAlSn5eTCyV](https://youtu.be/4VSgLFTZR90?si=akvWhKAlSn5eTCyV)

&#x200B;

Access all major AI tools in one seamless app, designed to elevate your productivity and streamline your digital experience. ""Read Aloud For Me"" brings the future of AI directly to your fingertips, merging convenience with innovation. Whether for work, education, or personal enhancement, our app is your gateway to the most advanced AI technologies. Download today and transform the way you interact with AI tools.

Read Aloud For Me PRO is your ultimate, secure, and multilingual speech synthesizer and AI Dashboard app. Transform reading and note-taking with our voice-controlled features. Effortlessly read PDFs, documents, and even photos aloud. Navigate through AI tools, translate texts, documents, and extract text from images in multiple languages with ease. Summarize websites, access leading AI platforms like ChatGPT, Google Gemini, Sora, Mistrial AI, Claude AI and Perplexity within the app. With a commitment to privacy, we ensure a no-tracking, secure user experience.

Elevate your productivity with Read Aloud For Me.

This App Read Aloud For You in your chosen language without tracking you and without collecting your data.

Translate Text For You in your chosen language without collecting your data.

Securely Read, Translate Text, Images, Docs to Speech in your chosen language, Speech Synthesizer using Machine Learning, NLP

This is an application which helps the visually impaired hear text with the help of AI services such as Google AutoML, Amazon Textract, Amazon Comprehend, Amazon Translate and Amazon Polly.

Users enter text or upload a picture of a document, or anything with text, and within a few seconds hear that document in their chosen language.

Application of Machine Learning, Natural Language Processing, Computer Vision

Speech Synthesizer.

Can read text, photos and documents in the following languages:

Afrikaans,

Albanian,

Arabic,

Armenian,

Azerbaijani,

Basque,

Belarusian,

Bulgarian,

Catalan,

Chinese (Simplified),

Chinese (Traditional),

Croatian,

Czech,

Danish,

Dutch,

English,

Estonian,

Filipino,

Finnish,

French,

Galician,

Georgian,

German,

Greek,

Haitian Creole,

Hebrew,

Hindi,

Hungarian,

Icelandic,

Indonesian,

Irish,

Italian,

Japanese,

Korean,

Latvian,

Lithuanian,

Macedonian,

Malay,

Maltese,

Norwegian,

Persian,

Polish,

Portuguese,

Romanian,

Russian,

Serbian,

Slovak,

Slovenian,

Spanish,

Swahili,

Swedish,

Thai,

Turkish,

Ukrainian,

Urdu,

Vietnamese,

Welsh,

Yiddish,

Zulu,

etc.

The AI Dashboard allows you to access the following AI tools without leaving the App:

Read Aloud For Me

Read Aloud - Speech Synthesizer

Take Notes by Voice

Read Photos in my chosen language

Analyze State ID in my chosen language

AI Bot

ChatGPT

Google Gemini

Claude AI

Perplexity AI

Mistrial AI

AI Translators

Translate Text in my chosen language

Translate Docs/PDFs in chosen language

Translate Website in chosen language

Translate Speech in chosen language

AI Speech-to-Text

Convert Speech to Text in chosen Language

AI Text-to-Speech

AI Text to Speech

Text-to-Speech GPT (ElevenLabs)

AI Video Creation

Sora Text-to-Video (OpenAI)

Text-to-Video(Google Luniere)

Image-to-Video(Luniere)

AI Image Creation

Dall E 3 Text-to-Image (OpenAI)

Text-to-Image(Gemini)

Text-to-Image (ImageFx)

Imagine with Meta AI

MidJourney AI

Microsoft Designer Image Creator

Youtube Thumbnail Maestro GPT

AI Voice, Music, Podcast

AI Voice Cloning (ElevenLabs)

AI Voice Changer (ElevenLabs)

Google MusicFX AI

AI Podcast

AI Write For Me

Write For Me with ChatGPT (OpenAI)

Write For Me (GPT)

Google TextFX AI

AI Coding

Code For Me with ChatGPT (OpenAI)

Google Gemini Code AI

Microsoft Copilot AI

PartyRock: Build Your Own Apps

AI Tutor

Tutor Me (Khan Academy)

Ace AWS CCP GPT

Ace AWS SAA GPT

AI Shopping - Product Reviews

Vetted AI - Shopping agent

AI Daily News

Daily AI Chronicle

&#x200B;

https://preview.redd.it/h1phycl9fglc1.png?width=1290&format=png&auto=webp&s=b46226d85006cf6ae6e846757a88f15793fd749c

https://preview.redd.it/h60u1dl9fglc1.png?width=1290&format=png&auto=webp&s=e2cbce20515373273020c8eab2e583027199b3e6

https://preview.redd.it/byc0xcl9fglc1.png?width=1290&format=png&auto=webp&s=9dc4fce305589ce5b4b5f254f57323124d03a968

https://preview.redd.it/pnhwvcl9fglc1.png?width=1290&format=png&auto=webp&s=a8692da6434c1936431296f564fc371f0ffbf485

https://preview.redd.it/wuq49cl9fglc1.png?width=1290&format=png&auto=webp&s=908897489f25f7e6830dd3bc0d55b49025f01047

https://preview.redd.it/ajtmvcl9fglc1.png?width=1290&format=png&auto=webp&s=4aec270f1d66bfe14914a064b22beb7af11d5141

https://preview.redd.it/x7uifdl9fglc1.png?width=1290&format=png&auto=webp&s=89f18ed191a460720b4808e920da5db859972fc9

https://preview.redd.it/9uhn8cl9fglc1.png?width=1290&format=png&auto=webp&s=d5077914b95fb5652e4721293e9f9db306764153

https://preview.redd.it/66224cl9fglc1.png?width=1290&format=png&auto=webp&s=a4acc859414034000e2ba5330d99c1baba117fbd

https://preview.redd.it/xv6nocl9fglc1.png?width=750&format=png&auto=webp&s=d47a3929d9e9a2714211b8513482b993ce4655c0",/r/u_enoumen/comments/1b2rt49/read_aloud_for_me_ai_dashboard_access_all_ai/,u_enoumen,u/enoumen,False,relevant,highly relevant
Have you checked out our blog?📕✏️,8BallShootItAll,https://www.reddit.com/r/8BallShootItAll/comments/1b2fj2j/have_you_checked_out_our_blog/,1,0,1709149923.0,1b2fj2j,"On our website, we have a fun and informative blog where you can learn about 8 Ball Shoot It All!

Catch up on every detail of our recent update and keep enjoying billiards 8 ball 🎱

Read it in between online pool sessions or read it at the real pool table, the choice is yours!

https://preview.redd.it/env9rpe4sdlc1.jpg?width=1080&format=pjpg&auto=webp&s=a398fd7214ef7fb898f582b39b8eb1ab4a5c3512

&#x200B;

Our blog: [https://8ball.in/blogs](https://8ball.in/blogs)

Download on Google Play:

[https://play.google.com/store/apps/details?id=com.square\_enix.pool](https://play.google.com/store/apps/details?id=com.square_enix.pool)

Download on App Store: 

[https://apps.apple.com/app/8-ball-shoot-it-all/id6474727799](https://apps.apple.com/app/8-ball-shoot-it-all/id6474727799)",/r/8BallShootItAll/comments/1b2fj2j/have_you_checked_out_our_blog/,8BallShootItAll,r/8BallShootItAll,False,not relevant,highly relevant
digital art is gonna be the new traditional art.,INGENAREL,https://www.reddit.com/r/Showerthoughts/comments/1b2eq2z/digital_art_is_gonna_be_the_new_traditional_art/,0,4,1709148052.0,1b2eq2z,"think about it. when people made digital art back in the day, a lot of people thought that digital art wasn't 'art'. that you were just typing some buttons in a computer. but as the time pass, digital art is considered art now but also good traditional art has also become rare, expensive, and sought after.

&#x200B;

and now with chatgpt, dall-e, sora (text to video ai if you've been under a rock) and google's new genie (text to 2d platformer games), ai is progressing faster and faster than ever. we're hitting the new technological growth curve. and now digital art made by real people is gonna be the new sought after and expensive thing now.",/r/Showerthoughts/comments/1b2eq2z/digital_art_is_gonna_be_the_new_traditional_art/,Showerthoughts,r/Showerthoughts,False,relevant,highly relevant
Not so well known ComfyUI community Custom Nodes + thoughts on OpenAI Sora and Gemini 1.5,Ak47shays,https://youtu.be/njuG70D4MzE,1,0,1709147727.0,1b2el5v,,/r/comfyui/comments/1b2el5v/not_so_well_known_comfyui_community_custom_nodes/,comfyui,r/comfyui,False,not relevant,less relevant
Heygen vs. Akool - which is better?,DJsummoner,https://www.reddit.com/r/faceswap/comments/1b246hv/heygen_vs_akool_which_is_better/,8,8,1709120854.0,1b246hv,"When it comes to cutting-edge image and video AI platforms, both HeyGen and Akool offer compelling features alongside their own set of pros and cons worth considering.  


Starting with HeyGen, its versatility stands out, catering to a wide range of industry use cases with its library avatars, instant avatars, and voice cloning capabilities. Despite its crowded UI and relatively steep learning curve, HeyGen excels in providing sophisticated editing tools and vast translation options, allowing users to reach diverse audiences effortlessly. However, some users may find its subscription model and add-on fees limiting, particularly for non-corporate users, and its talking photos feature may not always deliver the level of realism expected.

On the other hand, Akool boasts a user-friendly interface, making it accessible for beginners, alongside a flexible pay-as-you-go pricing model that appeals to occasional or student users. With features like text-to-video conversion and AI image generation, Akool offers versatility and innovation. Despite its current limitations in translation options and absence of full-body animations, Akool shines in producing realistic talking photos and providing a seamless viewing experience with no additional fees.

Ultimately, choosing between HeyGen and Akool depends on individual preferences and project requirements, with both platforms showcasing strengths in different areas while leaving room for improvement.",/r/faceswap/comments/1b246hv/heygen_vs_akool_which_is_better/,faceswap,r/faceswap,False,relevant,highly relevant
Sora AI :Text to video Generation !! prompt: ships in a coffee !,Abject_Station_5690,https://v.redd.it/9wexk8mfnalc1,1,0,1709112094.0,1b21wk3,,/r/SORA_CHATGPT/comments/1b21wk3/sora_ai_text_to_video_generation_prompt_ships_in/,SORA_CHATGPT,r/SORA_CHATGPT,True,relevant,neutral
Which adversarial prompts would you be using to test Sora text-to-video a red team member?,artxpromotion,https://www.reddit.com/r/OpenAI/comments/1b1qib6/which_adversarial_prompts_would_you_be_using_to/,3,1,1709076199.0,1b1qib6,I certainly understand the need to thoroughly test something like this to flag inappropriate and unsafe content. However... I think it would be wild to test an uncensored AI model like this to push the limits.,/r/OpenAI/comments/1b1qib6/which_adversarial_prompts_would_you_be_using_to/,OpenAI,r/OpenAI,False,relevant,highly relevant
Sora Ai Text to Video Tool: #SoraAI #OpenAI #TextToVideo #Ai #artificialintelligence #videotool #movieproduction #videoedits #videoediting #aiedit #editing #synthesia #colossyan #deepbrain #aitools #aivideo #thiscrowd,thisCrowd,https://v.redd.it/121zif9ys6lc1,1,0,1709065419.0,1b1lylr,,/r/u_thisCrowd/comments/1b1lylr/sora_ai_text_to_video_tool_soraai_openai/,u_thisCrowd,u/thisCrowd,True,relevant,neutral
Generative AI replace game engines ?,Altruistic_Ad_5474,https://i.redd.it/h0l1d95qp6lc1.jpeg,110,109,1709064336.0,1b1li3p,Guys considering how good generative AI is getting do you think one day as hardware becomes more and more powerful this tech can replace game engines or change the way they work? like Imagine an open world game with limitless possibilities everything rendered and controlled by the help of AI in real time,/r/OpenAI/comments/1b1li3p/generative_ai_replace_game_engines/,OpenAI,r/OpenAI,False,not relevant,highly relevant
[D] Will they be open sourced,CornerGlittering2745,https://www.reddit.com/r/MachineLearning/comments/1b1j4hj/d_will_they_be_open_sourced/,0,0,1709058619.0,1b1j4hj,"According to you when do you think text -to-video model like ""SORA"" could become completely open-sourced or will they never be made?

As its highly possible that people will use it for all not legal purposes (just what i read online could be different).

What do you people think about this topic in general that should model with high generative capability made open sourced or not.

Another reason why company would not do it is for the money they can make by creating a subscription on these which is likely.

Share your thoughts.

Thanks:)",/r/MachineLearning/comments/1b1j4hj/d_will_they_be_open_sourced/,MachineLearning,r/MachineLearning,False,relevant,highly relevant
"[World] - State TV airs China’s first AI-developed cartoon series, as Sora sets off frenzy for text-to-video generation tech | South China Morning Post",AutoNewspaperAdmin,https://www.scmp.com/tech/policy/article/3253390/state-tv-airs-chinas-first-ai-developed-cartoon-series-sora-sets-frenzy-text-video-generation-tech?utm_source=rss_feed,1,0,1709043524.0,1b1cxnq,,/r/AutoNewspaper/comments/1b1cxnq/world_state_tv_airs_chinas_first_aideveloped/,AutoNewspaper,r/AutoNewspaper,False,not relevant,less relevant
[World] - ‘Shining on international stage’: Chinese developers of OpenAI’s text-to-video generator Sora receive acclaim at home | South China Morning Post,AutoNewspaperAdmin,https://www.scmp.com/tech/tech-trends/article/3253371/shining-international-stage-chinese-developers-openais-text-video-generator-sora-receive-acclaim?utm_source=rss_feed,1,0,1709032746.0,1b19h6t,,/r/AutoNewspaper/comments/1b19h6t/world_shining_on_international_stage_chinese/,AutoNewspaper,r/AutoNewspaper,False,not relevant,neutral
"Ein Merkmal in Sora, das das OpenAI-Team nicht gezeigt hat und vielleicht eine Weile nicht veröffentlichen wird, ist die Fähigkeit, Videos aus einem einzigen Bild oder einer Sequenz von Bildern zu generieren.",Webglobic_tech,https://i.redd.it/l7hk3tqmz3lc1.jpeg,7,0,1709031346.0,1b19483,,/r/KI_Welt/comments/1b19483/ein_merkmal_in_sora_das_das_openaiteam_nicht/,KI_Welt,r/KI_Welt,False,not relevant,less relevant
"Ein Merkmal in Sora, das das OpenAI-Team nicht gezeigt hat und vielleicht eine Weile nicht veröffentlichen wird, ist die Fähigkeit, Videos aus einem einzigen Bild oder einer Sequenz von Bildern zu generieren.",Webglobic_tech,https://i.redd.it/nadhgi34z3lc1.jpeg,1,0,1709031176.0,1b192qk,,/r/u_Webglobic_tech/comments/1b192qk/ein_merkmal_in_sora_das_das_openaiteam_nicht/,u_Webglobic_tech,u/Webglobic_tech,False,not relevant,less relevant
"OpenAI's recent Sora text-to-video tech has blown China away, 'cold water' on their AI dreams",GanacheNegative1988,https://www.tweaktown.com/news/96449/openais-recent-sora-text-to-video-tech-has-blown-china-away-cold-water-on-their-ai-dreams/index.html,12,3,1708996329.0,1b0ystj,,/r/AMD_Stock/comments/1b0ystj/openais_recent_sora_texttovideo_tech_has_blown/,AMD_Stock,r/AMD_Stock,False,not relevant,highly relevant
Google releases “text to video game” AI model. Is the future generative gaming? :O,marcmar11,https://v.redd.it/tensk6diizkc1,1,3,1708977158.0,1b0qv9q,"Text to GTA 7 but in my hometown and I’m the main character.

Text below by Rowan Cheung on [Twitter](https://x.com/rowancheung/status/1762153658307989536?s=46)

“Google DeepMind just dropped 'Genie', an AI that can generate interactive video games.

This is a huge deal.

Genie is trained on 200,000 hours of unsupervised public internet gaming videos and can generate video games from a single prompt or image.

But here's what's insane:

Despite not being trained on action or text annotations, the foundation model can determine who the main character is and enable a user to control that character in the generated world.

It does this through its Latent Action Model, Video Tokenizer, and Dynamics Model (will go more in-depth on this in tomorrow's newsletter for those interested).

And for those asking, yes, it's research-only and not publicly available (here come the Google memes), and it does come with some limitations, like only currently creating games at 1FPS.

But this is the worst AI will ever be.

Anyone will be able to create their own entirely imagined virtual worlds soon, and that's a wild sentence to say out loud.”",/r/videogames/comments/1b0qv9q/google_releases_text_to_video_game_ai_model_is/,videogames,r/videogames,True,relevant,highly relevant
Google releases “text to video game” model. Is the future generative gaming? :O,marcmar11,https://v.redd.it/asg3ifseczkc1,0,0,1708975107.0,1b0q06k,"Text to “GTA 7 but in my hometown and I’m the main character”

Text from Rowan Cheung’s [Tweet](https://x.com/rowancheung/status/1762153658307989536?s=46)

“Google DeepMind just dropped 'Genie', an AI that can generate interactive video games.

This is a huge deal.

Genie is trained on 200,000 hours of unsupervised public internet gaming videos and can generate video games from a single prompt or image.

But here's what's insane:

Despite not being trained on action or text annotations, the foundation model can determine who the main character is and enable a user to control that character in the generated world.

It does this through its Latent Action Model, Video Tokenizer, and Dynamics Model (will go more in-depth on this in tomorrow's newsletter for those interested).

And for those asking, yes, it's research-only and not publicly available (here come the Google memes), and it does come with some limitations, like only currently creating games at 1FPS.

But this is the worst AI will ever be.

Anyone will be able to create their own entirely imagined virtual worlds soon, and that's a wild sentence to say out loud.”
",/r/ChatGPT/comments/1b0q06k/google_releases_text_to_video_game_model_is_the/,ChatGPT,r/ChatGPT,True,relevant,highly relevant
This was made with the text to video program OpenAI is making.,-_Anonymous__-,https://v.redd.it/zqqn99groykc1,2,2,1708967158.0,1b0mmvu,,/r/EliteEden/comments/1b0mmvu/this_was_made_with_the_text_to_video_program/,EliteEden,r/EliteEden,True,relevant,neutral
I need to make a Birthday gift with ai,Neurophysiopatology,https://www.reddit.com/r/ArtificialInteligence/comments/1b0jssa/i_need_to_make_a_birthday_gift_with_ai/,1,2,1708960261.0,1b0jssa,"Hello, i need to make a Birthday gift for my brother. But mainstream site have limits for what i need to do. Basically i need a Text to video with voice clone an old Dead Journalist that interview my brother. Is that possible? He Is the pref Journalist of him. If you can help me, very thanks! ",/r/ArtificialInteligence/comments/1b0jssa/i_need_to_make_a_birthday_gift_with_ai/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
What is Sora AI? Everything to know about OpenAI's text-to-video tool - BBC Science Focus,Additional_Zebra_861,https://www.sciencefocus.com/future-technology/openai-sora,1,1,1708945574.0,1b0exy7,,/r/ChatGPT/comments/1b0exy7/what_is_sora_ai_everything_to_know_about_openais/,ChatGPT,r/ChatGPT,False,relevant,neutral
To early to be worried ,thelastsandwich,https://i.redd.it/8omhg1zdvwkc1.jpeg,9,3,1708945168.0,1b0etq6,,/r/internettoday/comments/1b0etq6/to_early_to_be_worried/,internettoday,r/internettoday,False,relevant,less relevant
OpenAI's Sora: Revolutionizing Content Creation with Text-to-Video Generation,MediumHomeland,https://www.reddit.com/r/techtuuv/comments/1b098z8/openais_sora_revolutionizing_content_creation/,1,0,1708923417.0,1b098z8,"In the ever-evolving realm of artificial intelligence, OpenAI's innovative tool, Sora, emerges as a transformative force in multimedia content creation. Sora, a state-of-the-art text-to-video generator, empowers creators to turn written narratives into compelling video content with ease.

https://preview.redd.it/nknbod6p2vkc1.jpg?width=843&format=pjpg&auto=webp&s=15d5b4cb4aa8f6324ff44c47f78081a1476d3b96

As a part of the new generation of AI-driven creative tools, Sora is designed to emulate and amplify human creativity. Users can provide textual descriptions to the tool, which Sora then interprets to generate engaging video sequences. This technology is particularly adept at constructing detailed scenes, with the capability to produce videos that are up to one minute long, showcasing the potential to revolutionize storytelling and content production across various platforms.

&#x200B;

[Click for Details](https://techtuv.com/openai-text-to-video-generation-tool-sora/)",/r/techtuuv/comments/1b098z8/openais_sora_revolutionizing_content_creation/,techtuuv,r/techtuuv,False,relevant,highly relevant
"AI and You: OpenAI's Sora Previews Text-to-Video Future, First Ivy League AI Degree - CNET",tw_bot,https://news.google.com/atom/articles/CBMidWh0dHBzOi8vd3d3LmNuZXQuY29tL3RlY2gvY29tcHV0aW5nL2FpLWFuZC15b3Utb3BlbmFpcy1zb3JhLXByZXZpZXdzLXRleHQtdG8tdmlkZW8tZnV0dXJlLWZpcnN0LWl2eS1sZWFndWUtYWktZGVncmVlL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en,1,0,1708922936.0,1b093tu,,/r/tomorrowsworld/comments/1b093tu/ai_and_you_openais_sora_previews_texttovideo/,tomorrowsworld,r/tomorrowsworld,False,not relevant,neutral
How realistic is it to get the Season 8 we all wanted using AI?,samosama,https://www.reddit.com/r/gameofthrones/comments/1b06bnj/how_realistic_is_it_to_get_the_season_8_we_all/,0,19,1708914220.0,1b06bnj,"With the release of OpenAI Sora (text to video automation), anything seems possible. Maybe soon, or in a few years, one could feed Season 8 source material into an advanced tech/diffusion model like Sora, and create the version we all wanted.",/r/gameofthrones/comments/1b06bnj/how_realistic_is_it_to_get_the_season_8_we_all/,gameofthrones,r/gameofthrones,False,relevant,highly relevant
Hollywood Faces TOTAL Annihilation: Sora AI Text-to-Video Program Causes...,NighCitySolo,https://youtube.com/watch?v=fIPjVm40B1U&si=_Oxyz4SyRDqKqzIc,1,0,1708900934.0,1b01d5b,,/r/u_NighCitySolo/comments/1b01d5b/hollywood_faces_total_annihilation_sora_ai/,u_NighCitySolo,u/NighCitySolo,False,not relevant,neutral
Tiktok thought Sora's F1 video is real,xiikjuy,https://i.redd.it/atudetul8rkc1.png,225,14,1708877531.0,1azrkuz,,/r/ChatGPT/comments/1azrkuz/tiktok_thought_soras_f1_video_is_real/,ChatGPT,r/ChatGPT,False,relevant,neutral
"Roundup: OpenAI's Sora, a revolutionary text-to-video tool?",finphil,https://philaverse.substack.com/p/roundup-openais-sora,2,0,1708815929.0,1az8a1w,,/r/AILinksandTools/comments/1az8a1w/roundup_openais_sora_a_revolutionary_texttovideo/,AILinksandTools,r/AILinksandTools,False,not relevant,neutral
Release Latte! Open source version of openAI Sora text-to-video,balianone,https://v.redd.it/z2sanf82c9kc1,6,4,1708660357.0,1axqy0t,,/r/StableDiffusion/comments/1axqy0t/release_latte_open_source_version_of_openai_sora/,StableDiffusion,r/StableDiffusion,True,not relevant,neutral
"Scary good or just plain scary? OpenAI's text-to-video tool amazes some, worries others",chiquisea,https://www.kuow.org/stories/openai-s-text-to-video-tool-amazes-some-worries-others,7,10,1708733865.0,1aygea9,,/r/technology/comments/1aygea9/scary_good_or_just_plain_scary_openais/,technology,r/technology,False,not relevant,less relevant
"Since Sora's busy converting text to video now, here's a new protagonist for the Kingdom Hearts series.",MurasakiYugata,https://i.redd.it/70brj3mo67kc1.jpeg,12,1,1708634200.0,1axgzq3,,/r/dalle2/comments/1axgzq3/since_soras_busy_converting_text_to_video_now/,dalle2,r/dalle2,False,relevant,neutral
"What happens when you input the entire Infinite Jest into Sora, the upcoming AI text-to-video generator?",Rostamann,https://www.reddit.com/r/InfiniteJest/comments/1axjpbz/what_happens_when_you_input_the_entire_infinite/,0,26,1708640698.0,1axjpbz,,/r/InfiniteJest/comments/1axjpbz/what_happens_when_you_input_the_entire_infinite/,InfiniteJest,r/InfiniteJest,False,not relevant,neutral
"AI and You: OpenAI's Sora Previews Text-to-Video Future, First Ivy League AI Degree",Agile-Bed-5277,https://www.cnet.com/tech/computing/ai-and-you-openais-sora-previews-text-to-video-future-first-ivy-league-ai-degree/,2,0,1708588687.0,1ax114j,,/r/techphonenews/comments/1ax114j/ai_and_you_openais_sora_previews_texttovideo/,techphonenews,r/techphonenews,False,not relevant,neutral
Open AI SORA: A Text-to-Video Generator Revolutionizing Content Creation,NickwilliamsS,/r/u_NickwilliamsS/comments/1avc0jr/open_ai_sora_a_texttovideo_generator/,2,0,1708421535.0,1avdp03,,/r/SoraAi/comments/1avdp03/open_ai_sora_a_texttovideo_generator/,SoraAi,r/SoraAi,False,not relevant,highly relevant
"Oxen.AI's Paper Club zoom call tomorrow will cover Lumiere, Google's text to video diffusion model (Friday 10:00 - 10:45 AM pacific time)",ReluOrTanh,https://www.reddit.com/r/learnmachinelearning/comments/1ax4hc4/oxenais_paper_club_zoom_call_tomorrow_will_cover/,11,2,1708602260.0,1ax4hc4,"[Oxen.AI](https://Oxen.AI)'s Paper Club zoom call tomorrow will cover Lumiere, Google's text to video diffusion model.  Oxen's CEO, Greg Schoeninger,  is both freaking smart and great at plain-speak. He leads the call. (Hey Greg are you on reddit? )

This AI Paper Club is the most fun hour of my workweek.  The questions, insights, and hijinx in the call chat give me an upper for the rest of the day.   

By the end of the 45 min session,  I feel like I understand a paper well enough to add it to my Swiss Army knife of tools ""art of the possible"" understanding to deploy in the right moment. 

For this Friday, am totally psyched to hear Greg's take on Lumiere and see what the other attendees say and ask who run amok in the chat.

Next Friday, March 1, is Medusa with guest host Daniel Varoli-Vasilevskiy.

To signup for the zoom call (approval is required):  
[https://www.oxen.ai/community?utm\_source=paper\_club\_flyer](https://www.oxen.ai/community?utm_source=paper_club_flyer)

Not posting this officially for Oxen.  My views are my own.  I am simply grateful they provide a fun and easy way for me to digest 1 paper foundational to AI each Friday during lunch.   The Oxen team who helps with chat and Q&A, Ben, Scott, Eric, Adam, and my buddy Team Intern Mathi ;-) are also awesome.    They have created something special with this call.  I respect and admire what they have done and how they they did it.

Plus I've met some awesome people I'm pairing with on new projects via their Discord  
[https://discord.com/invite/s3tBEn7Ptg](https://discord.com/invite/s3tBEn7Ptg)

&#x200B;",/r/learnmachinelearning/comments/1ax4hc4/oxenais_paper_club_zoom_call_tomorrow_will_cover/,learnmachinelearning,r/learnmachinelearning,False,relevant,highly relevant
AI tokens are experiencing a surge following the introduction of Sora text-to-video generator by OpenAI,Multiplatform_com,https://multiplatform.ai/ai-tokens-are-experiencing-a-surge-following-the-introduction-of-sora-text-to-video-generator-by-openai/,3,2,1708455178.0,1avphyz,,/r/Multiplatform_AI/comments/1avphyz/ai_tokens_are_experiencing_a_surge_following_the/,Multiplatform_AI,r/Multiplatform_AI,False,not relevant,neutral
Sora AI a Text-to-video Model from OpenAI,pinoygamerph,https://www.youtube.com/watch?v=zmua2f98CsA,2,0,1708293984.0,1au6csn,,/r/pinoygamer/comments/1au6csn/sora_ai_a_texttovideo_model_from_openai/,pinoygamer,r/pinoygamer,False,not relevant,neutral
"Last year, in an interview Sam Altman said that GPT-5 will have text-to-video creation. Guess SORA will be part of ChatGPT premium membership.",twobytesgiven,https://www.reddit.com/r/ChatGPT/comments/1awhyzo/last_year_in_an_interview_sam_altman_said_that/,3,3,1708536813.0,1awhyzo,,/r/ChatGPT/comments/1awhyzo/last_year_in_an_interview_sam_altman_said_that/,ChatGPT,r/ChatGPT,False,not relevant,highly relevant
Text-to-video AI will let us replay our dreams and nightmares.,Like_Fahrenheit,https://www.reddit.com/r/Dreams/comments/1atqg1z/texttovideo_ai_will_let_us_replay_our_dreams_and/,2,1,1708249291.0,1atqg1z,"There's been a few AI text-to-videos recently, and it occurred to me that that may be an excellent way to replay dreams and nightmares. What do you think? Does it seem like a valuable tool in this regard, or not? I would like to see what you think.",/r/Dreams/comments/1atqg1z/texttovideo_ai_will_let_us_replay_our_dreams_and/,Dreams,r/Dreams,False,relevant,highly relevant
"Tyler Perry says he has halted the $800M expansion of his studio after seeing OpenAI’s text-to-video model Sora. He adds that he just used AI in 2 of his films and that with Sora “I no longer would have to travel to locations. If I wanted to be in the snow in Colorado, it’s text.”",Lonely-Freedom4986,https://www.hollywoodreporter.com/business/business-news/tyler-perry-ai-alarm-1235833276/,1119,269,1708647542.0,1axmhdu,,/r/boxoffice/comments/1axmhdu/tyler_perry_says_he_has_halted_the_800m_expansion/,boxoffice,r/boxoffice,False,relevant,less relevant
SORA - Text to video generation model by OpenAI,rhypple,https://v.redd.it/7b82x6lx7yic1,3,0,1708089806.0,1as8hmk,,/r/sora_videos/comments/1as8hmk/sora_text_to_video_generation_model_by_openai/,sora_videos,r/sora_videos,True,relevant,neutral
OpenAI Sora: An Incredible Text to Video Model That Shocked Everyone,Raqmedia,https://www.raqmedia.com/openai-sora-text-to-video-model/,2,4,1708116138.0,1asj0xg,,/r/u_Raqmedia/comments/1asj0xg/openai_sora_an_incredible_text_to_video_model/,u_Raqmedia,u/Raqmedia,False,relevant,less relevant
what do you guys think of OpenAI's new text to video generator Sora?,SpiritMan112,https://www.reddit.com/r/decadeology/comments/1astxf5/what_do_you_guys_think_of_openais_new_text_to/,2,2,1708146291.0,1astxf5,"While its not out yet, I think Sora is about to change everything. Literally, it's looks very real compared to AI generated videos in early 2023. Sora has a 60 second capacity. It does has some hand and slight facial errors, but its way smoother, clearer, and advanced than old AI videos from last year that gave us nightmares. When its released, I really think its going to change social media, the internet, the film industry, and even animation. I also believe its going to be used a lot in politics negatively. But, I think VFX, animators, and maybe even actors are going to be in danger now. I think text to video generators will have bigger and longer capacities in the near future. ",/r/decadeology/comments/1astxf5/what_do_you_guys_think_of_openais_new_text_to/,decadeology,r/decadeology,False,relevant,less relevant
"OpenAI Unveils Sora, an AI-Powered Text-to-Video Generator Capable of Creating One-Minute-Long Clips",Diwonuso,https://www.gadgets360.com/ai/news/openai-unveils-ai-text-to-video-generator-sora-can-create-1-minute-long-clips-5068085#pfrom=home-popular,2,0,1708221410.0,1atipcx,,/r/techphonenews/comments/1atipcx/openai_unveils_sora_an_aipowered_texttovideo/,techphonenews,r/techphonenews,False,not relevant,highly relevant
Prediction: What if OpenAI Fyre Fested us on Sora's capabilities?,JayRidders,https://www.reddit.com/gallery/1azbqcc,23,10,1708825159.0,1azbqcc,,/r/ChatGPT/comments/1azbqcc/prediction_what_if_openai_fyre_fested_us_on_soras/,ChatGPT,r/ChatGPT,False,not relevant,neutral
"“Yes… it will make porn,” sighs OpenAI CEO introducing text-to-video feature",AhFalafel,https://www.reddit.com/r/onionheadlines/comments/1asjbye/yes_it_will_make_porn_sighs_openai_ceo/,8,0,1708116890.0,1asjbye,,/r/onionheadlines/comments/1asjbye/yes_it_will_make_porn_sighs_openai_ceo/,onionheadlines,r/onionheadlines,False,not relevant,neutral
Looks Like Midjourney Level Text To Video Coming From OpenAI - SORA : Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&midjourney,6,4,1708053291.0,1arygav,,/r/midjourney/comments/1arygav/looks_like_midjourney_level_text_to_video_coming/,midjourney,r/midjourney,False,not relevant,highly relevant
"Introducing Sora AI, OpenAI’s new text-to-video tool",kkingfisherr,https://techbriefly.com/2024/02/16/how-to-use-openai-sora-ai/,2,0,1708073629.0,1as43o6,,/r/TechBriefly/comments/1as43o6/introducing_sora_ai_openais_new_texttovideo_tool/,TechBriefly,r/TechBriefly,False,not relevant,neutral
What Is SORA? Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&SoraAi,3,0,1708266662.0,1atvcl2,,/r/SoraAi/comments/1atvcl2/what_is_sora_unbelieve_new_text_to_video_ai_model/,SoraAi,r/SoraAi,False,not relevant,neutral
What are the current open-source projects for text-to-video ?,Bloortis,https://www.reddit.com/r/SoraAi/comments/1as5lpb/what_are_the_current_opensource_projects_for/,4,0,1708079999.0,1as5lpb,"Sora looks pretty cool but we don't have much info on it (apart from their ""tech report"").   

Since I don't know much about the text-to-video scene, I was  wondering if there was some open-source projects I can have a look at !   ",/r/SoraAi/comments/1as5lpb/what_are_the_current_opensource_projects_for/,SoraAi,r/SoraAi,False,relevant,highly relevant
"This video was generated by Sora, the new text-to-video model by OpenAI: Very Retro Pikmin Dancing in a Blue Car",Blue_Pikmin25,https://i.redd.it/58j6s7ea59jc1.gif,13,2,1708222136.0,1atiy27,,/r/Pikmin/comments/1atiy27/this_video_was_generated_by_sora_the_new/,Pikmin,r/Pikmin,False,relevant,neutral
"Sora AI, the first text to video model from OpenAI",mishalobdell,/r/SoraAI_Content_Create/comments/1arr8l5/sora_ai_the_first_text_to_video_model_from_openai/,2,1,1708033752.0,1arr9y3,,/r/ChatGPT/comments/1arr9y3/sora_ai_the_first_text_to_video_model_from_openai/,ChatGPT,r/ChatGPT,False,relevant,neutral
OpenAI Unveils Sora: Text-to-Video AI Model Creates Movie-Like Scenes,ProlificWriter3,https://www.reddit.com/r/1ATHStudio/comments/1as4n29/openai_unveils_sora_texttovideo_ai_model_creates/,2,0,1708075944.0,1as4n29,"OpenAI unveils Sora, a generative AI model that creates high-resolution video from text prompts, demonstrating impressive capabilities in generating movie-like scenes with multiple characters and motions.",/r/1ATHStudio/comments/1as4n29/openai_unveils_sora_texttovideo_ai_model_creates/,1ATHStudio,r/1ATHStudio,False,relevant,highly relevant
Video Generated By OpenAI Sora! New OpenAI Text-To-Video Model,st1275857,https://www.youtube.com/watch?v=KiP05mehXFQ,2,0,1708074877.0,1as4dzx,This entire video is generated by OpenAI sora. It incredible how good it is. RIP to videographer. What do you guys think? ,/r/learnmachinelearning/comments/1as4dzx/video_generated_by_openai_sora_new_openai/,learnmachinelearning,r/learnmachinelearning,False,relevant,highly relevant
Video: OpenAI Unveils Next-Level Generative AI Text-to-Video Tool,PostNewsEra,https://www.reddit.com/r/FightingFakeNews/comments/1aruduz/video_openai_unveils_nextlevel_generative_ai/,2,0,1708041595.0,1aruduz,Video: OpenAI Unveils Next-Level Generative AI Text-to-Video Tool [https://www.infowars.com/posts/video-openai-unveils-next-level-generative-ai-text-to-video-tool/](https://www.infowars.com/posts/video-openai-unveils-next-level-generative-ai-text-to-video-tool/),/r/FightingFakeNews/comments/1aruduz/video_openai_unveils_nextlevel_generative_ai/,FightingFakeNews,r/FightingFakeNews,False,not relevant,neutral
Latest Erupting concern: Sora,MircroDesk,https://www.reddit.com/r/MircroDesk/comments/1ayqikm/latest_erupting_concern_sora/,2,1,1708766850.0,1ayqikm,"OpenAI’s new generative Sora tool has sparked lively technology discussions over the past week, generating both enthusiasm and concern among fans and critics.

Sora is a text-to-video model that significantly advances the integration of deep learning, natural language processing and computer vision to transform textual prompts into detailed and coherent life-like video content.  


But....  
 What about deep fake? Isn't it a matter of time when anything could be produced just by a prompt? Isn't it alarming?Share your insights.",/r/MircroDesk/comments/1ayqikm/latest_erupting_concern_sora/,MircroDesk,r/MircroDesk,False,relevant,highly relevant
The tech of text-to-video through AI is incredible. Here is a sample. ,rockntalk,https://v.redd.it/sntjd7egzsic1,19,7,1708026421.0,1aro9np,"Here is a demo video by OpenAI through their new product called Sora that converts text to video of a minute. 

Where are we heading with pace of development? ",/r/IndiaTech/comments/1aro9np/the_tech_of_texttovideo_through_ai_is_incredible/,IndiaTech,r/IndiaTech,True,relevant,neutral
OpenAI unveils text-to-video model and the results are astonishing. Take a look for yourself,EcstadelicNET,https://www.zdnet.com/article/openai-unveils-text-to-video-model-and-the-results-are-astonishing-take-a-look-for-yourself/,5,0,1708100144.0,1ascgq4,,/r/IntelligenceSupernova/comments/1ascgq4/openai_unveils_texttovideo_model_and_the_results/,IntelligenceSupernova,r/IntelligenceSupernova,False,not relevant,neutral
Plai Labs unveils AI-powered text-to-video tool PlaiDay,Athena5000,https://venturebeat.com/games/plai-labs-unveils-ai-powered-text-to-video-tool-plaiday/,3,0,1707979913.0,1ar9h7d,,/r/PlaiDay/comments/1ar9h7d/plai_labs_unveils_aipowered_texttovideo_tool/,PlaiDay,r/PlaiDay,False,not relevant,neutral
"After the text to video model Sora made by OpenAI, a wave of negativity coming from the Anti-AI side",CrazyKittyCat0,https://www.reddit.com/gallery/1asjwlp,31,21,1708118284.0,1asjwlp,,/r/DefendingAIArt/comments/1asjwlp/after_the_text_to_video_model_sora_made_by_openai/,DefendingAIArt,r/DefendingAIArt,False,relevant,less relevant
OpenAI Sora 🔥 New Realistic Text-to-Video Revolutionary AI Shocked The ...,dotcomgeek,https://youtube.com/watch?v=7M0C2JdlkSI&si=oa6nFgTNSFiaKsVq,2,0,1708039665.0,1artnob,,/r/GoogleGeminiAI/comments/1artnob/openai_sora_new_realistic_texttovideo/,GoogleGeminiAI,r/GoogleGeminiAI,False,not relevant,less relevant
#1 Best Video AI Tools For 2024,BiscottiSilent3815,https://www.reddit.com/r/youtube/comments/1aypd4w/1_best_video_ai_tools_for_2024/,2,1,1708762263.0,1aypd4w,"&#x200B;

[#1 Best Video AI Tools For 2024](https://preview.redd.it/v6p2t0zerhkc1.jpg?width=960&format=pjpg&auto=webp&s=ec1154757ec1429b0480cef14932024ee80bfe70)

**Stunn (**[**www.stunnvideo.com**](https://www.stunnvideo.com/)**): The #1 Best AI Video Generator**

**Prompt:** ""Make a video about lasers"": [https://www.youtube.com/watch?v=7eDzSCUfaSA](https://www.youtube.com/watch?v=7eDzSCUfaSA)

* **Powerful Conversion:** Stunn turns text into professional-quality videos by pulling clips from the internet in minutes.
* **Customization:**  Upload custom transcripts, voiceovers, or even videos of yourself  speaking to      incorporate your own touch into the final product.

**Voice-Over Generation**

* **Eleven Labs:** Known for its incredibly realistic, customizable voice-over synthesis. ([https://elevenlabs.io/](https://elevenlabs.io/))

**Transcript Generation**

* **Gemini:** A powerful text-to-speech generator with impressive vocal quality and flexibility. ([ https://gemini.google.com/](https://gemini.google.com/))

**Video Generation**

* **Runway:** A versatile platform with robust text-to-video capabilities, image editing, and other AI-powered creative tools. ([https://runwayml.com/](https://runwayml.com/))
* **Sora:** Generate clips from a Text prompt ([https://openai.com/sora](https://openai.com/sora))   (NOT YET RELEASED)

**Music Generation**

* **Suno:** Compose unique, royalty-free music tracks tailored to your videos using AI. ([https://www.suno.ai/](https://www.suno.ai/))",/r/youtube/comments/1aypd4w/1_best_video_ai_tools_for_2024/,youtube,r/youtube,False,relevant,highly relevant
"OpenAI has launched 'Sora', a text-to-video model that allows users to create videos using prompts.",UnusualWhalesBot,http://twitter.com/1200616796295847936/status/1758195823853375532,22,4,1708030826.0,1arq2jq,,/r/unusual_whales/comments/1arq2jq/openai_has_launched_sora_a_texttovideo_model_that/,unusual_whales,r/unusual_whales,False,relevant,highly relevant
Sora’s growth in the next year,ClassroomOk8582,https://www.reddit.com/r/artificial/comments/1awgfum/soras_growth_in_the_next_year/,3,12,1708533194.0,1awgfum,"With Open Ai revealing their new text to video generation tool named Sora it has caused a massive sesmic wave in the tech industry. When you compare what text-to-video generation looked like a year ago from today and you compare it to what we have today, it’s clear the progress has been exponential. What does text-to-video generation look like a year from today? What will be the consequences of such a tool? I can’t help but imagine that the implications will change pretty much every aspect of our life. ",/r/artificial/comments/1awgfum/soras_growth_in_the_next_year/,artificial,r/artificial,False,relevant,highly relevant
PSA: It looks like OpenAI now has a TikTok account and is posting new Sora videos there,jaketocake,https://i.redd.it/5x2vuf125sjc1.jpeg,525,76,1708452038.0,1avo5u4,,/r/OpenAI/comments/1avo5u4/psa_it_looks_like_openai_now_has_a_tiktok_account/,OpenAI,r/OpenAI,False,not relevant,highly relevant
Open AI Sora - text to video - let’s make some bucks 💰 - looking for a dev,Odd-Tree1041,https://www.reddit.com/r/indiehackers/comments/1arums5/open_ai_sora_text_to_video_lets_make_some_bucks/,6,9,1708042232.0,1arums5,"Hey I’m seasoned product manager and photographer/videographer, I have an app idea to capitalise on Open AI latest update Sora once it’s released, the window of opportunity is small because there will be copy cats once it’s launched but I think it will be lucrative while it lasts as long as the API costs are not too much.

Message me if you’re interested, 50/50 split I’ll focus product ideation, marketing and sales and you development, we can both work on UI.",/r/indiehackers/comments/1arums5/open_ai_sora_text_to_video_lets_make_some_bucks/,indiehackers,r/indiehackers,False,relevant,highly relevant
SORA : Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&aiwars,7,9,1708053363.0,1aryh5x,,/r/aiwars/comments/1aryh5x/sora_unbelieve_new_text_to_video_ai_model_by/,aiwars,r/aiwars,False,not relevant,neutral
Huh... weird that they won't let you talk about their proprietary new technology. Why do y'all think they'd do this?,Raptorex2000,https://i.redd.it/x5xl9z0rlkjc1.png,50,35,1708360871.0,1aurrkv,,/r/ArtistHate/comments/1aurrkv/huh_weird_that_they_wont_let_you_talk_about_their/,ArtistHate,r/ArtistHate,False,not relevant,less relevant
SORA : Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&Youtube_Video_posts,3,0,1708052951.0,1arycaq,,/r/Youtube_Video_posts/comments/1arycaq/sora_unbelieve_new_text_to_video_ai_model_by/,Youtube_Video_posts,r/Youtube_Video_posts,False,not relevant,neutral
Anon weighs in on Google’s Gemini,RightThisHemingway,https://i.redd.it/jtdmr72j17kc1.jpeg,959,42,1708632455.0,1axg7y8,,/r/4chan/comments/1axg7y8/anon_weighs_in_on_googles_gemini/,4chan,r/4chan,False,not relevant,neutral
"Wow, bros are so considerate about disabled people as always!",WonderfulWanderer777,https://i.redd.it/6mm4e2avr0kc1.jpeg,50,9,1708556639.0,1awqd4b,,/r/ArtistHate/comments/1awqd4b/wow_bros_are_so_considerate_about_disabled_people/,ArtistHate,r/ArtistHate,False,not relevant,highly relevant
The Tesla cult is so desperate to have Tesla latch on to the AI hype 😂,GadnukBreakerOfWrlds,https://i.redd.it/uft8j7fc06kc1.png,165,15,1708620087.0,1axaxgi,,/r/EnoughMuskSpam/comments/1axaxgi/the_tesla_cult_is_so_desperate_to_have_tesla/,EnoughMuskSpam,r/EnoughMuskSpam,False,relevant,less relevant
AI News for this week,mmanthony00,https://www.reddit.com/r/AINewsAndTrends/comments/1awrrkx/ai_news_for_this_week/,11,7,1708560099.0,1awrrkx,"This week in AI news has been incredibly great with major announcements!

1. **Gemini 1.5:** Google released Gemini 1.5 Pro, a LLM with a 1 million token context window.
2. **Sora:** OpenAI unveiled its new text-to-video model Sora, which can create realistic 60-minute videos and high-resolution images. 
3. **Chat with RTX:** NVIDIA has introduced Chat with RTX, a locally run AI Chatbot, allowing users to build their own personalized chatbot experience. 
4. **V-JEPA:** Meta released a new AI model that improves training by learning from videos—Yann LeCun's vision of advanced machine intelligence (AMI).
5. **Stable Cascade:** Stability AI released its newest model for image generation.
6. **ChatGPT Memory:** ChatGPT will now have a memory you can turn off with its settings. 
7. **Eleven Labs:** introduced a new feature where you can turn your voice into passive income.",/r/AINewsAndTrends/comments/1awrrkx/ai_news_for_this_week/,AINewsAndTrends,r/AINewsAndTrends,False,relevant,highly relevant
"""if stealing legal why murdering AI thieves is still not """,dookiefoofiethereal,https://i.redd.it/6t215kzxh4kc1.png,44,10,1708624206.0,1axcp1j,,/r/DefendingAIArt/comments/1axcp1j/if_stealing_legal_why_murdering_ai_thieves_is/,DefendingAIArt,r/DefendingAIArt,False,relevant,less relevant
"The new OpenAi text-to-video model ""Sora"" just got announced. Really not sure how to feel about this for the futur of our industry...",emrk30,https://v.redd.it/yyt9p4trmsic1/HLSPlaylist.m3u8?a=1710632614%2CYzdiNDM0OTkzNGUyY2I2NThlYTYzZTNiMjI5N2U1YzIwNGJiMmI2ZTg1ODVkYTY5MjdlYWVmMjBmZDk0MWY2NA%3D%3D&amp;v=1&amp;f=sd,2,4,1708040980.0,1aru5qf,,/r/videography/comments/1aru5qf/the_new_openai_texttovideo_model_sora_just_got/,videography,r/videography,False,relevant,less relevant
Introducing SoraWebui: Your Gateway to Future-Ready Video Creation!,qiayue,https://www.reddit.com/r/SoraAi/comments/1aw12pq/introducing_sorawebui_your_gateway_to_futureready/,7,0,1708484027.0,1aw12pq,"Hey Reddit Community,  


I'm excited to introduce you to SoraWebui, an open-source project revolutionizing the way we create videos online. 

[https://github.com/SoraWebui/SoraWebui](https://github.com/SoraWebui/SoraWebui)

With SoraWebui, you can prepare your website for the future of video creation, even before the official release of the Sora API by OpenAI.

You see, Sora API isn't available yet, but that's where SoraWebui comes in. 

Our project allows you to get ahead of the curve by setting up your website and being ready to integrate the latest open-source code as soon as OpenAI releases the Sora API. 

This means you'll be among the first to offer text-to-video functionality once it becomes available.

To bridge the gap until the official release of the Sora API, we've developed FakeSoraAPI. 

[https://github.com/SoraWebui/FakeSoraAPI](https://github.com/SoraWebui/FakeSoraAPI)

This tool lets you simulate the functionality of the Sora API, enabling you to start developing products and solutions in advance. 

Currently, SoraWebui operates using the Fake Sora API provided by FakeSoraAPI.

So, what's the big deal with SoraWebui? It's all about being prepared ahead of time. 

By leveraging our platform, you'll be ready to unlock the full potential of the Sora model for text-to-video conversion as soon as it's released.

But that's not all! We're continuously working on enhancing SoraWebui. In the future, we plan to support Google login and integrate with Stripe for seamless payment processing.

So, are you ready to be at the forefront of video creation innovation? Check out SoraWebui today and join us on the journey towards the future of content creation.

&#x200B;

[SoraWebui](https://preview.redd.it/i7oei5zprujc1.png?width=2888&format=png&auto=webp&s=57c903f98690b10b86c250f677d111c5cf2d45de)

&#x200B;

[Quick Started](https://preview.redd.it/uvirofbtrujc1.png?width=1832&format=png&auto=webp&s=6b25053e340800c8b81427d8f6bff880b3914146)

[SoraWebui Demo](https://preview.redd.it/mukd2yo3sujc1.png?width=2024&format=png&auto=webp&s=4bca762884206aca1f697041e6d9bacc59bf3b06)

&#x200B;

&#x200B;

&#x200B;",/r/SoraAi/comments/1aw12pq/introducing_sorawebui_your_gateway_to_futureready/,SoraAi,r/SoraAi,False,relevant,highly relevant
Guys we fucked up… It’s coming,Zeeu5,https://i.redd.it/l66yhxmnt7jc1.jpeg,29,5,1708206065.0,1atd4sr,,/r/schizoposters/comments/1atd4sr/guys_we_fucked_up_its_coming/,schizoposters,r/schizoposters,False,not relevant,less relevant
"Open Ai just announced Sora , A text to video generator, its not open to public yet , but we will be soon , be ready my friends",MozartO_O,https://www.reddit.com/r/soragonewild/comments/1artaf3/open_ai_just_announced_sora_a_text_to_video/,2,1,1708038724.0,1artaf3,,/r/soragonewild/comments/1artaf3/open_ai_just_announced_sora_a_text_to_video/,soragonewild,r/soragonewild,False,relevant,highly relevant
"Open Ai just announced Sora , A text to video generator, its not open to public yet , but we will be soon , be ready my friends Join us",MozartO_O,/r/soragonewild/comments/1artaf3/open_ai_just_announced_sora_a_text_to_video/,5,1,1708039148.0,1artgeh,,/r/DalleGoneWild/comments/1artgeh/open_ai_just_announced_sora_a_text_to_video/,DalleGoneWild,r/DalleGoneWild,False,relevant,highly relevant
Yeah sure (chatgpt 3.5),Givanni64,https://i.redd.it/fo8axl4zkbjc1.png,3,1,1708251575.0,1atr0go,,/r/ChatGPT/comments/1atr0go/yeah_sure_chatgpt_35/,ChatGPT,r/ChatGPT,False,not relevant,highly relevant
A Website That Helps You Quickly Find Various Types of AI Video Products,pageone_ai,https://www.reddit.com/r/MakerHunter/comments/1aw4dmp/a_website_that_helps_you_quickly_find_various/,3,3,1708494093.0,1aw4dmp,"\-sugus From Gefei Community

In today's fast-paced digital world, video content reigns supreme. Whether you're a content creator for YouTube, a marketer aiming to capture your audience's attention, or a TikTok enthusiast looking to make your clips stand out, finding the right tools to create, edit, and enhance your videos is crucial. This is where AI video products come into play, revolutionizing the way we create and interact with video content. For those in search of a comprehensive resource to quickly find various types of AI video products, [AI-Video-Editor.AI](http://www.ai-video-editor.ai/) stands out as a beacon in the vast sea of online tools.

## Table of Contents

1. **Introduction to AI Video Products**
2. **Discovering AI video editor Directory&List**
3. **Types of AI Video Products Available**
4. **Why Choose** **AI-Video-Editor.AI**?
5. **Getting Started with AI Video Products**
6. **Conclusion**

## 1. Introduction to AI Video Products

## What Are AI Video Products?

AI video products are tools that leverage artificial intelligence to automate and enhance various aspects of video creation and editing. From converting text into engaging video content to generating scripts and transforming presentations into dynamic videos, AI video editing tools are designed to streamline the content creation process.

## The Evolution of Video Editing Technology

The evolution of video editing technology has been remarkable, transitioning from manual, time-consuming processes to automated, AI-driven solutions. This progress has democratized video production, making it accessible to creators of all skill levels.

## 2. Discovering AI Video Tools Directory&List

## Overview of the Website

it is a comprehensive directory that caters to the needs of individuals looking for AI-powered video editing tools. The website offers a curated list of solutions for various video editing requirements, from text-to-video conversion to specialized tools for social media platforms like YouTube and TikTok.

## How It Simplifies Your Search for AI Video Tools

The platform simplifies the search for AI video tools by categorizing products based on functionality, making it easier for users to find the tool that best fits their needs. Whether you're a professional video editor or a novice, it guides you to the right resources.

## 3. Types of AI Video Products Available

* **Text To Video**: Convert written content into captivating video pieces.
* **Blog To Video**: Transform your blog posts into engaging videos.
* **AI Script Generator**: Generate scripts for your videos using AI.
* **PPT To Video**: Turn PowerPoint presentations into dynamic videos.
* **Solutions for YouTube and TikTok Video Editing**: Specialized tools designed to enhance video content for popular social media platforms.

## 4. Why Choose AI-Video-Editor.AI

## Comprehensive Directory

The website offers a broad selection of AI video editing tools, ensuring you find exactly what you need for your specific video project.

## User-Friendly Interface

With a focus on user experience, the website is designed to be intuitive and easy to navigate, saving you time and effort in your search for the perfect AI video tool.

## Time-Saving Features

The categorization and detailed descriptions of each tool mean you spend less time researching and more time creating high-quality video content.

## 5. Getting Started with AI Video Products

## Tips for Beginners

For those new to AI video products, start with tools that offer templates and guided workflows to familiarize yourself with the process before moving on to more advanced features.

## Maximizing the Use of AI Video Tools

Leverage the capabilities of AI to automate repetitive tasks, focus on creative aspects of video production, and explore different styles and formats to enhance your content.

## 6. Conclusion

In the realm of digital content creation, videos are a powerful tool for storytelling and engaging with your audience. With the help of **AI-Video-Editor.AI**, finding the right AI video product to suit your needs has never been easier. Whether you're looking to convert text to video, generate video scripts, or edit videos for social media platforms, this website is your gateway to a world of innovative video editing solutions. Embrace the future of video content creation with AI video products and elevate your creative projects to new heights.",/r/MakerHunter/comments/1aw4dmp/a_website_that_helps_you_quickly_find_various/,MakerHunter,r/MakerHunter,False,relevant,highly relevant
Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&Corridor,9,3,1708053597.0,1aryjqs,,/r/Corridor/comments/1aryjqs/unbelieve_new_text_to_video_ai_model_by_openai_37/,Corridor,r/Corridor,False,not relevant,highly relevant
Why are they teasing him?,Plumsby,https://i.redd.it/fpmlqyi8ezic1.jpeg,196,33,1708104030.0,1ase2x4,,/r/KingdomHearts/comments/1ase2x4/why_are_they_teasing_him/,KingdomHearts,r/KingdomHearts,False,not relevant,less relevant
Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,6,3,1708053625.0,1aryk48,,/r/deeplearning/comments/1aryk48/unbelieve_new_text_to_video_ai_model_by_openai_37/,deeplearning,r/deeplearning,False,not relevant,highly relevant
Most AI predictions,Maxie445,https://i.redd.it/2osiokg44ajc1.png,6397,239,1708233811.0,1atmh6a,,/r/ChatGPT/comments/1atmh6a/most_ai_predictions/,ChatGPT,r/ChatGPT,False,not relevant,neutral
Problems with OpenAI Sora: Multiple Arms on Human and Cat,edepot,https://www.reddit.com/r/OpenaiSora/comments/1avgqtp/problems_with_openai_sora_multiple_arms_on_human/,2,0,1708432957.0,1avgqtp,"For those who have seen the latest demo from OpenAI Sora, you may have noticed the cat waking the human section. If you pay close attention, you will notice that the cat has multiple arms (paws) and the sleeping woman also has multiple arms. There are 3 arms for both the cat and human, a big problem for text to video applications. Here is a video showing the arm problem in detail: [https://youtu.be/nwnPaJhu94k](https://youtu.be/nwnPaJhu94k)",/r/OpenaiSora/comments/1avgqtp/problems_with_openai_sora_multiple_arms_on_human/,OpenaiSora,r/OpenaiSora,False,relevant,less relevant
SORA : Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - I Am Still Skeptical How This Is Real,CeFurkan,https://www.youtube.com/watch?v=VlJYmHNRQZQ&machinelearningnews,12,0,1708051549.0,1arxvqm,,/r/machinelearningnews/comments/1arxvqm/sora_unbelieve_new_text_to_video_ai_model_by/,machinelearningnews,r/machinelearningnews,False,not relevant,highly relevant
"This set of stunning video clips are not real, they are open AI SORA generated😲",remsondu,https://youtu.be/FdzbXJ1yxcY,2,0,1708196547.0,1at9fmi,,/r/Snorkblot/comments/1at9fmi/this_set_of_stunning_video_clips_are_not_real/,Snorkblot,r/Snorkblot,False,relevant,highly relevant
Aged like milk,Gold-and-Glory,https://i.redd.it/m1n2xq20x7jc1.png,2339,360,1708207281.0,1atdlki,,/r/singularity/comments/1atdlki/aged_like_milk/,singularity,r/singularity,False,not relevant,highly relevant
Validating a GPTs interaction,HospitalSouthern1749,https://www.reddit.com/r/GPTStore/comments/1au01ew/validating_a_gpts_interaction/,3,4,1708278700.0,1au01ew,"I am interested in creating interactive GPTs that ask a series of clarifying or follow-up questions before responding to vague prompts. In particular, I am torn between making the GPT ask a general question at once or having its question broken down into smaller questions for which the GPT waits for the user's answer before it proceeds to ask the next one.

The interaction in this video captures a session where the Text-to-Video GPT guides the user into providing useful answers. But I am concerned the user could drop off the conversation if they feel they have to go through such an interaction.

[https://chat.openai.com/g/g-3I4wk3u89-text-to-video](https://chat.openai.com/g/g-3I4wk3u89-text-to-video)

https://reddit.com/link/1au01ew/video/7p26er1rtdjc1/player",/r/GPTStore/comments/1au01ew/validating_a_gpts_interaction/,GPTStore,r/GPTStore,False,relevant,less relevant
“Photorealistic videos from a few sentences is something only our great-grandkids *might* have”,justausernamehereman,https://i.redd.it/yiy1b1plo6jc1.jpeg,1640,212,1708192259.0,1at7rjo,In light of recent developments from OpenAI and Sora text to video,/r/agedlikemilk/comments/1at7rjo/photorealistic_videos_from_a_few_sentences_is/,agedlikemilk,r/agedlikemilk,False,relevant,highly relevant
OpenAI Sora: The Age Of AI Is Here! - Two Minute Papers,cloudrunner69,https://www.youtube.com/watch?v=nbPbK1xYSNY,19,2,1708098345.0,1asbqb6,,/r/singularity/comments/1asbqb6/openai_sora_the_age_of_ai_is_here_two_minute/,singularity,r/singularity,False,not relevant,neutral
AI films,Classic_essays,https://www.reddit.com/r/LangChain/comments/1aun3bk/ai_films/,2,13,1708348738.0,1aun3bk,"Do you guys think it will be possible to make entire films using AI? I honestly feel like Sora might just be a lightbulb in the text-to-video space. And when that time comes, will films enthusiasts still have the same thrill of going to cinemas? Is it even ethical to ask people to pay for AI generated films?",/r/LangChain/comments/1aun3bk/ai_films/,LangChain,r/LangChain,False,relevant,highly relevant
things are improving.. RAPIDLY,toreachtheapex,https://i.redd.it/esft4wq8luic1.jpeg,772,43,1708045846.0,1arvwtl,,/r/ChatGPT/comments/1arvwtl/things_are_improving_rapidly/,ChatGPT,r/ChatGPT,False,not relevant,neutral
this is terrifying... how far Ai has come in the last year.,Taylorsdank,https://www.tiktok.com/@aiforhumansshow/video/7335933391935737131,3,0,1708106910.0,1asfarq,,/r/xaryu/comments/1asfarq/this_is_terrifying_how_far_ai_has_come_in_the/,xaryu,r/xaryu,False,not relevant,neutral
Has anyone else noticed the massive amount of hate for Sora?,plantsnlionstho,https://www.reddit.com/r/singularity/comments/1au9eri/has_anyone_else_noticed_the_massive_amount_of/,272,575,1708301649.0,1au9eri,"I'm seeing an absolutely enormous amount of hate for text to video and Sora right now. Far more than I've ever seen for image generation. I'll leave some examples below but these videos have hundreds of thousands of likes and make claims like:

>*""Nothing good that could be created with this justifies the evil that will be done""*  
>  
>*-* [https://www.tiktok.com/@allyrooker/video/7336323843558067487](https://www.tiktok.com/@allyrooker/video/7336323843558067487)  
>  
>*""There are no non evil uses for AI videos""*  
>  
>*""If you're creating this stuff, or work for a company that creates this stuff... you are evil, you are a bad person, you are sick in the soul""*  
>  
>\- [https://www.tiktok.com/@mattgrippi/video/7336234661137370410](https://www.tiktok.com/@mattgrippi/video/7336234661137370410)  
>  
>*""It's fundamentally stripping us of our reasons to be alive""*  
>  
>\- [https://www.tiktok.com/@jstoobs/video/7336292039073729838](https://www.tiktok.com/@jstoobs/video/7336292039073729838)

Outside of r/singularity and AI twitter the reactions I've seen have been almost exclusively negative.

I'm interested to hear people's thoughts on this. How would you respond to these people and what do you agree/disagree with?

Edit: [Here's a twitter thread that compiles more of the same type of reactions.](https://twitter.com/AISafetyMemes/status/1759231062646104165)",/r/singularity/comments/1au9eri/has_anyone_else_noticed_the_massive_amount_of/,singularity,r/singularity,False,relevant,less relevant
The worst part of Sora is the video to video,StrapOnDillPickle,https://www.reddit.com/r/vfx/comments/1atm1kf/the_worst_part_of_sora_is_the_video_to_video/,144,161,1708232302.0,1atm1kf,"Check this out. Surprised I haven't seen anybody talk about this out here.

\[edit\] new tiktok link [https://www.tiktok.com/@openai/video/7337000211228708139](https://www.tiktok.com/@openai/video/7337000211228708139)

It's not just text to video. It's video to video. It's seamless transitions. it's plate extensions.

Was nice knowing you all. Jealous of those close to retirement.

The info was hidden inside the Technical Report

[https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)",/r/vfx/comments/1atm1kf/the_worst_part_of_sora_is_the_video_to_video/,vfx,r/vfx,False,relevant,less relevant
Sora from OpenAI to create video from text : It will disrupt many industries,gagan1985,https://www.reddit.com/r/StartUpIndia/comments/1asdxik/sora_from_openai_to_create_video_from_text_it/,13,1,1708103665.0,1asdxik,"Sora is an AI model that can create realistic and imaginative scenes from text instructions.[https://openai.com/sora](https://openai.com/sora)

This will change how we consume media and it will be a menace for YouTube to handle.

Sam Altman announced it yesterday.

https://preview.redd.it/shrapua7czic1.png?width=1080&format=png&auto=webp&s=ecdef4a420b46a9e2355c407902eeef28cf096be

Example

[https://twitter.com/sama/status/1758219575882301608](https://twitter.com/sama/status/1758219575882301608)  


The Creator Economy will be the first to get disrupt.

https://preview.redd.it/umd0tpy0czic1.png?width=1192&format=png&auto=webp&s=2cf84f25c03f31cb816cfc2398fafe46c58e8db5

The Question is,

What will be next opportunities or industries that will get disrupted ?

I am thinking about Video Ads will also be disrupted.Story boarding for movies/TV will also get disrupted.

What else?  


Update: BTW I asked same question to ChatGPT as well.  


https://preview.redd.it/tq7z11g0ezic1.png?width=1594&format=png&auto=webp&s=cda0a9fcbf29ce07d310f871e2ce285deca24e89",/r/StartUpIndia/comments/1asdxik/sora_from_openai_to_create_video_from_text_it/,StartUpIndia,r/StartUpIndia,False,relevant,highly relevant
Well that just sounds like intedimensional cable but with extra steps.,Faaz_Noushad4444,https://i.redd.it/79827rmtt0jc1.png,274,9,1708121411.0,1asl5pd,,/r/rickandmorty/comments/1asl5pd/well_that_just_sounds_like_intedimensional_cable/,rickandmorty,r/rickandmorty,False,not relevant,highly relevant
Is this Ai scenario possible?,Difficult-Eye9960,https://www.reddit.com/r/RandomThoughts/comments/1aujfzn/is_this_ai_scenario_possible/,2,2,1708335354.0,1aujfzn,"I was thinking about new hopefully soon to be released text to video Ai called Sora that is made by Open Ai. Now with Elon Musks neuralchips and METAs and Apples VR headsets and with Ai, would it be possible to be in a dream basically? 

For example if the neuralchip gets advanced enough to translate thoughts to text and that that text to be inputed in Sora who would actually create it and than we would be able to see it through the VR headsets.

This is how it would look like,

for example I think about summoning a tornado that would destroy the gaspump in front of me when i snap my fingers, than the chip converts it to text which gets imputed in Sora which than after I snap my fingers shows the gaspump getting destroyed by the tornado on the VR headset.

How possible is this?
What do you guys think about this?",/r/RandomThoughts/comments/1aujfzn/is_this_ai_scenario_possible/,RandomThoughts,r/RandomThoughts,False,relevant,highly relevant
The future of learning might actually be genuinely fun for kids and adults in near future!,hiccuppinganus,https://www.reddit.com/r/singularity/comments/1augqte/the_future_of_learning_might_actually_be/,24,30,1708324635.0,1augqte,"Imagine 5-10 years from now (hopefully sooner) when text to video is near perfect and is able to give 30+min long videos. Now imagine when a kid is sitting in class whether it be at home on a device or in a school setting and they are forced to learn X they can make the education actually fun by having the A.I make age appropriate videos of a history lesson, math lesson or whatever other X lesson and each video is specifically tailored to that kid or adult that is learning to fully grab there attention and not bore them to sleep. Seriously imagine the implications this has you would be able to teach things to all people even those who are slow at learning and if someone is highly intelligent they can go at there own pace and not have to go at everyone else's pace. I can see a future where the basics of everything will be taught to kids from kindergarten to 6th grade and then after that it will all be specialty teaching and getting kids to all choose what interests them the most and if they don't like it they get to learn a different branch of education the following year or month or week exc... 

I think education will be completely reshaped in 10 years or less and I think one of the new career paths for people will be to have a meeting with a kid for X amount of time to try to discover what type of learning will be best for them and that persons  job will be to make a prompt specifically for them to help the kid get the most out of there education    ",/r/singularity/comments/1augqte/the_future_of_learning_might_actually_be/,singularity,r/singularity,False,relevant,highly relevant
Stop Sora AI: Petition,unideni,https://www.reddit.com/r/teenagers/comments/1au29af/stop_sora_ai_petition/,4,8,1708284088.0,1au29af,"https://chng.it/TcLbj78qp8

OpenAI, the company behind ChatGPT has just announced “Sora”: a text-to-video AI platform. 

Releasing this tool could ruin hundreds of thousands of peoples dreams and LIVES. This can and WILL be used to spread misinformation, violence, s**ual content and other DISTURBING and HORRIBLE things. Yes, OpenAI said it will not allow such things to happen, but who’s stopping the internet? People will find a way to abuse and bypass the restrictions. People will also find other ways to abuse AI.
Imagine in a year you’re in court looking at yourself vandalizing a building illegally. But, that video was made with AI. What is your defense?
We need to stop AI once and for all, before it’s too late.

Any signs are very much appreciated",/r/teenagers/comments/1au29af/stop_sora_ai_petition/,teenagers,r/teenagers,False,relevant,less relevant
I asked Sora to generate this video,gigacored,https://v.redd.it/ynwsfst7h3jc1,76,9,1708153478.0,1asvxhn,"Got early access to Sora. OpenAI's text-to-video model. 

The prompt was 'a panning shot of beautiful Swiss Alps overlooking a lake, it is cloudy afternoon and with a bit of breeze, the waves are calming and the grass is green'",/r/SwitzerlandIsFake/comments/1asvxhn/i_asked_sora_to_generate_this_video/,SwitzerlandIsFake,r/SwitzerlandIsFake,True,relevant,highly relevant
The future is epic awesomesauce!!!!! 🤯🤯🤯,very_bug_like,https://i.redd.it/cxyei35jzzic1.png,232,36,1708111227.0,1ash1v5,,/r/redscarepod/comments/1ash1v5/the_future_is_epic_awesomesauce/,redscarepod,r/redscarepod,False,not relevant,neutral
Metal Gear Solid 2: Sons of Liberty (2001),Suitable-Union-3714,https://i.redd.it/b4havp7t8zic1.jpeg,663,37,1708102298.0,1asdd4g,,/r/Gamingcirclejerk/comments/1asdd4g/metal_gear_solid_2_sons_of_liberty_2001/,Gamingcirclejerk,r/Gamingcirclejerk,False,not relevant,highly relevant
Sora: OpenAI’s Groundbreaking AI for Video Generation,merchbubble,https://medium.com/link/a7e511fee41c,2,0,1708029863.0,1arpola,,/r/MediumApp/comments/1arpola/sora_openais_groundbreaking_ai_for_video/,MediumApp,r/MediumApp,False,not relevant,neutral
It's over.,basedtrump2k16,https://i.redd.it/857yh742psic1.jpeg,335,158,1708022916.0,1armuvg,They put this out there before the proper start of the election cycle on purpose I'm sure. The people in charge of these companies are accelerationists. Evil evil ideology.,/r/redscarepod/comments/1armuvg/its_over/,redscarepod,r/redscarepod,False,not relevant,less relevant
"With Sora in mind, what days in the last five years have you been the most blown away by how fast AI is moving?",CaptainMarkoRamius,https://www.reddit.com/r/OpenAI/comments/1atbjsl/with_sora_in_mind_what_days_in_the_last_five/,22,31,1708201913.0,1atbjsl,"I'm trying to wrap my head around the implications of such high quality text to video coming this quickly after where we were a year ago (or certainly three years ago), which made me think...is seeing Sora the most blown away I have been by the velocity of AI technology development? I think for me it is yesterday, seeing Sora...

I just can't get my head around the fact if it took this little time to get from the initial launch of ChatGPT to Sora, what does that portend re where we will be 3 or 5 years from now. How does this not result in massive societal change in so many ways well before 2030?",/r/OpenAI/comments/1atbjsl/with_sora_in_mind_what_days_in_the_last_five/,OpenAI,r/OpenAI,False,relevant,highly relevant
2024 elections are coming,MetaKnowing,https://i.redd.it/8dh7yi7mf4jc1.png,74,6,1708165074.0,1asysoe,,/r/WhitePeopleTwitter/comments/1asysoe/2024_elections_are_coming/,WhitePeopleTwitter,r/WhitePeopleTwitter,False,not relevant,neutral
AI and shit,Itchy-Hunter-6040,https://www.reddit.com/r/nihonkoku_shoukan/comments/1atc5jm/ai_and_shit/,23,7,1708203501.0,1atc5jm,"So if you guys have heard, OpenAI just released Sora or a Text to Video Generator. And it might be a pretty damn good wag to kickstart an NHS Animated serious lol.

I'm not saying it's gonna happen within this year, from what I've heard, it's expensive. And somewhat bad. However it is in its early stages. With how rapid AI technology has grown over the past three years. I think we can have moderately affordable Sora-ish generators.

And while it may take a while, it's not entirely impossible someone might make an NHS Animated Serious all from AI. And while it is quite awesome it is also quite terrifying how far AI has come in the past few years ",/r/nihonkoku_shoukan/comments/1atc5jm/ai_and_shit/,nihonkoku_shoukan,r/nihonkoku_shoukan,False,relevant,less relevant
How are we going to create social links in an ultra-individualistic society ?,IntroVertu,https://www.reddit.com/r/singularity/comments/1asg7b2/how_are_we_going_to_create_social_links_in_an/,17,33,1708109148.0,1asg7b2,"Communities often gather around **common interests.** Reddit is a good example of a company that has taken advantage of this. We like to discuss the latest film or give our opinions on the latest single. 

Culture is (unlike other topics like politics) a simple **AND** fun way of connecting with people (at parties and elsewhere).

But now, if anyone can create their own music, manga or movie, isn't there a risk that we'll all end up isolated, with nothing in common from a cultural point of view ?

In the space of a few decades, we're going to go from a world where people *gather* at the movie theater to see the same film, to a world where everyone watches and creates *their own* film (in their bedroom via the latest Vision Pro) ?

Human beings are by nature sociable animals, so I'm very curious to see how we're going to adapt to this rise of hyper-personalized content...

&#x200B;

https://preview.redd.it/h2ambkpekzic1.png?width=1176&format=png&auto=webp&s=63060c8dbb61b287ffffab855e7daf3afc139ab9",/r/singularity/comments/1asg7b2/how_are_we_going_to_create_social_links_in_an/,singularity,r/singularity,False,relevant,highly relevant
Recommend Cheap ways to run Comfyui and get good amount of storage,Anonymous__Explorer,https://www.reddit.com/r/comfyui/comments/1av0c5n/recommend_cheap_ways_to_run_comfyui_and_get_good/,4,5,1708380644.0,1av0c5n,"Hey guys, I am beginner and have understood the image generation but now I want to utilise AnimateDiff and want to work on Image to video and text to video models and workflows.   


\> I have been using Kaggle cause it gives 30hour/week GPU access, but problem is I get 19GB of storage which is  not enough for video generation workflows.   


\> I am a university student and have a university email id, which gets me unlimited drive storage. So i tried accessing colab and It does not allows me to run ComfyUI on free tier, planning to buy Pro version, but don't understand the compute units and if they will be sufficient or not. I will pay for these services from my own side gig, and so I am low on budget, like $250 a month only, at very max.   


\> I want to utilise my unlimited drive storage but it seems I can't use it with kaggle, and I saw other applications like Runpod (GPU seems to be decent but what about storage ??) and also Paperspace, but can't understand if it is good cause I have to first pay.   


I have a potato PC, with no GPU. can't run it on this.   


Or maybe I was also thinking of buying external hard - drive with nearly 2PB and to run ComfyUI without GPU, and utilising my external hard - drive.   


Btw my specs for system is :   
OS Name	Microsoft Windows 11 Home Single Language

Version	10.0.22621 Build 22621

Other OS Description 	Not Available

OS Manufacturer	Microsoft Corporation

System Name	\[REDACTED\]

System Manufacturer	HP

System Model	HP Pavilion x360 Convertible 14-dy1xxx

System Type	x64-based PC

System SKU	661X0PA#ACJ

Processor	11th Gen Intel(R) Core(TM) i5-1155G7 @ 2.50GHz, 2496 Mhz, 4 Core(s), 8 Logical Processor(s)

BIOS Version/Date	Insyde F.16, 17-03-2022

SMBIOS Version	3.3

Embedded Controller Version	65.54

BIOS Mode	UEFI

BaseBoard Manufacturer	HP

BaseBoard Product	89BD

BaseBoard Version	65.54

Platform Role	Mobile

Secure Boot State	On

PCR7 Configuration	Elevation Required to View

Windows Directory	C:\\WINDOWS

System Directory	C:\\WINDOWS\\system32

Boot Device	\\Device\\HarddiskVolume1

Locale	United States

Hardware Abstraction Layer	Version = ""10.0.22621.2506""

User Name	\[REDACTED\]

Time Zone	\[REDACTED\]

Installed Physical Memory (RAM)	16.0 GB

Total Physical Memory	15.8 GB

Available Physical Memory	1.69 GB

Total Virtual Memory	31.5 GB

Available Virtual Memory	6.66 GB

Page File Space	15.8 GB

Page File	C:\\pagefile.sys

Kernel DMA Protection	On

Virtualization-based security	Running

Virtualization-based security Required Security Properties	

Virtualization-based security Available Security Properties	Base Virtualization Support, Secure Boot, DMA Protection, UEFI Code Readonly, SMM Security Mitigations 1.0, Mode Based Execution Control, APIC Virtualization

Virtualization-based security Services Configured	

Virtualization-based security Services Running	

Windows Defender Application Control policy	Enforced

Windows Defender Application Control user mode policy	Off

Device Encryption Support	Elevation Required to View

&#x200B;

&#x200B;",/r/comfyui/comments/1av0c5n/recommend_cheap_ways_to_run_comfyui_and_get_good/,comfyui,r/comfyui,False,relevant,highly relevant
Quick F.A.Q Re: Sora,Ok_Huckleberry4418,https://www.reddit.com/r/soravids/comments/1atb4ks/quick_faq_re_sora/,4,0,1708200832.0,1atb4ks," **FAQ:**

1. Is Sora available right now to me?  
A: No, it is not yet widely available.
2. Is there a waitlist or API access?  
A: Not as of Feb 16th, stay tuned!
3. How can I get access to Sora?  
A: Stay tuned! We have not rolled out public access to Sora.

**Source:** [Community OpenAI](https://community.openai.com/t/introducing-sora-our-text-to-video-model/630289)",/r/soravids/comments/1atb4ks/quick_faq_re_sora/,soravids,r/soravids,False,relevant,less relevant
Chinese Internet reaction to OpenAI Sora?,pekeng_pangalan,https://www.reddit.com/r/ADVChina/comments/1aswmqh/chinese_internet_reaction_to_openai_sora/,9,3,1708156238.0,1aswmqh,"With China being the self-professed leaders in AI, how is it reacting to OpenAI's text-to-video model Sora? It seems to be blowing up on X, Youtube, and western media.",/r/ADVChina/comments/1aswmqh/chinese_internet_reaction_to_openai_sora/,ADVChina,r/ADVChina,False,relevant,neutral
Soon this will look like it’s 8Bit compared to what’s about to happen…,Cottoneyeknow,https://youtu.be/ZnezqewjtNo?si=42tqEl5A_rNDmvSz,2,0,1708167796.0,1aszhrp,"With the incoming OpenAi text-to-video on its way, it’s crazy to think that this quality of clip will soon be outdated.",/r/AI_Music/comments/1aszhrp/soon_this_will_look_like_its_8bit_compared_to/,AI_Music,r/AI_Music,False,relevant,neutral
Well?,ActuaryGlittering16,https://www.reddit.com/r/singularity/comments/1as5ps7/well/,30,35,1708080455.0,1as5ps7,"How many pseuds told you text-to-video wouldn’t be here until 2030 at the earliest?

We’re mostly futurists here. I can’t be the only one who is starting to get seriously annoyed by doomers, overly smug skeptics, and people “working in AI, bro” who have severe tunnel vision and no imagination. None of these people know shit. We deal with them 24/7 and they bash everything we say. And when we eventually are proven right, they’re nowhere to be found. 

For the record I think AGI still has a ways to go, and the gap between AGI and ASI/singularity might be larger than we expect. But  it’s very clear that the low hanging fruit like text-to-video will get figured out quickly because lots of smart people are all working together across the world on this technology, and lots of rich people are funding it.",/r/singularity/comments/1as5ps7/well/,singularity,r/singularity,False,relevant,highly relevant
Ich habe selten so Gänsehaut bekommen wie mit den neuen Entwicklungen von Sora und Gemini,traenen,https://www.reddit.com/r/de_EDV/comments/1atsw21/ich_habe_selten_so_gänsehaut_bekommen_wie_mit_den/,318,200,1708258822.0,1atsw21,"Die Fähigkeiten von **Sora** sind jetzt schon atemberaubend, selbst wenn das nur die besten Beispiele von hunderten sind (was ich nicht mal glaube).

Hier haben sicher alle die Beispiele gesehen. Alleine schon das, kann wahnsinnige Auswirkungen haben. Denkt an alle Clips, die wenig Kontext brauchen und unter 1 Min sind. Es gibt subs mit Millionen an Subscribern, die fast nur daraus bestehen. Cinematics for video spiele. Pornoclips. Werbung.

Aber die weiteren Fähigkeiten sind IMO noch ""übler"". Ich beziehe mich hier auf die Beispiele hier:

 [Video generation models as world simulators (openai.com)](https://openai.com/research/video-generation-models-as-world-simulators) 

1. Fotos animieren. Die Möglichkeit das zu tun öffnet so viel (Missbrauchs)potential. Überlegt euch, was man alles damit anstellen kann aus einem Foto einer Person ein praktisch fotorealistisches Video per Text-To-Video zu machen. Eine Woche vor der Wahl ein Video generieren wo ein Politiker einen Welpen tritt? Kein Problem. Natürlich wird openAI das nicht zulassen, aber technisch ist es nun machbar.
2. Videos verlängern. Vielleicht nicht ganz so krass aber auch beeindruckend
3. Videos verbinden. Ich glaube man muss sich nur die Beispiele anschauen. Manche davon haben offensichtlich noch Probleme aber manche sind quasi jetzt schon perfekt.
4. Object permanence. Sora ""weiß"" das wenn ein Objekt nur vorrübergehend verdeckt ist, dass es danach wieder da ist. 
5. Live rendering mit Input. Wenn ich das richtig interpretiere, kann Sora eine Welt live simulieren, in der man grob herumlaufen kann mit z.B. WASD. Die Auswirkungen auf Videospiele sind offensichtlich. Wir machen da jetzt schon große Schritt Richtung Holodeck wo eine fotorealistische Simulation mit Interaktion möglich ist.

In  [vfx for visual effects artists and industry (reddit.com)](https://www.reddit.com/r/vfx/)  ist jetzt schon absolute Weltuntergangsstimmung und ich kann das voll nachvollziehen. Sora kann Videos generieren, die sonst wohl abertausende Euros kosten würden.

Aber gleichzeitig kam auch **Gemini** und das ist fast auf gleichem Niveau was ""Wtf wir leben in der Zukunft"" angeht.

Gemini kann viel viel mehr Inhalt verarbeiten als alles was wir bisher hatten und das multimodal. Mit 10M Tokens sollte das locker die gesamte Bibel verarbeiten können z.B. und Fragen dazu beantworten auf bekanntem ChatGPT Niveau. Wobei zumindest Google sagt, dass Gemini 1.5 besser ist als ChatGPT aber nicht massiv. Und ähnliches mit Video und Audio.

Gib ihm ein 3h Videostream von einer Überwachungskamera und frage ""Wie lange war der Mitarbeiter nicht am Platz?"" und er sollte dir die richtige Antwort geben können. Ich sehe Echtzeit Massenüberwachung mit kompletten Kontext fast schon für technisch machbar. 

Wir sind entweder schon da oder kurz davor, einem LLM eine ganze Codebase zu geben und damit zu arbeiten. Oder immer alle deine E-Mails vom aktuellen Jahr. Oder alle Dokumente einer komplexen Gerichtsverhandlung.

Insbesondere was Coding angeht, wird sich IMO bald Panik breit machen, denn das eingeschränkte Kontextfenster war wirklich die große Limitation. Mit 10M Tokens sollte die ganze Codebase von z.B. Photoshop 1.0 als Input funktionieren.

Ich denke wir befinden uns in den letzten 1-2 Jahren bevor AI sehr große Auswirkungen auf fast alle Personen in der westlichen Welt haben wir alleine schon wegen den Verwerfungen im Job-Bereich.

&#x200B;",/r/de_EDV/comments/1atsw21/ich_habe_selten_so_gänsehaut_bekommen_wie_mit_den/,de_EDV,r/de_EDV,False,relevant,less relevant
The AOE is ALMOST REAL,dxvywrld,https://i.redd.it/wzwxl4vy40jc1.jpeg,62,22,1708113015.0,1ashrmz,,/r/ANRime/comments/1ashrmz/the_aoe_is_almost_real/,ANRime,r/ANRime,False,not relevant,neutral
Why are Open AI releasing (the admittedly excellent) Sora and yet their paid version of Chat GPT 4 gets worse by the fucking week? I'm getting this message for even 300 word output requests,remixedmoon5,https://i.redd.it/s92in0nmrzic1.png,73,30,1708108529.0,1asfyff,"What the fuck are they on? 

Chat GPT 4 is regressing and they pour likely hundreds of millions into a text to video AI? 

Why not get the core product right first? 

Every single day, multiple times, I'm getting ""I'm unable to fulfill this request"" to simple tasks on Chat GPT 4 and it gets worse week on week

Cue the usual Altman White Knights to defend their nonsense ....",/r/ChatGPT/comments/1asfyff/why_are_open_ai_releasing_the_admittedly/,ChatGPT,r/ChatGPT,False,relevant,less relevant
Il circolino dell'hardware pt.2,nandospc,https://www.reddit.com/r/ItalyInformatica/comments/1avc74v/il_circolino_dellhardware_pt2/,24,38,1708415341.0,1avc74v,"Ben approdati in questa nuova rubrica quindicinale, redditors! Siamo al secondo appuntamento per il luogo in cui parlare insieme delle ultime novità relative al mondo dell'Hardware per PC e per raccogliere tutte le richieste di aiuto sul vostro attuale e futuro Computer. Se state rifacendo il PC e vi serve una build, un upgrade e avete dei dubbi, questo è il posto perfetto.

Eccoci con la carrellata di oggi 👇

# I nuovi prodotti di Lian Li

Durante l'ultimo Expo di Lian Li, l'azienda taiwanese ha presentato i suoi nuovi prodotti. Nell'evento si sono susseguiti i pareri del CEO Jameson Chen e degli invitati Justin Robey di RobeyTech, Pedro Valadas di PCMR e Stuart Tonks di GGF Events. Si è parlato principalmente dei nuovi case, come il SUP-01. Si tratta di un case compatto dal volume di 45 Litri dove la GPU è posizionata frontalmente grazie all'uso di un riser pcie 4.0 già preinstallato. Nonostante le dimensioni, potrà ospitare radiatori per aio fino a 360mm e il design dotato di pannelli mesh permette di avere un airflow paragonabile, ad esempio, ad un lancool 216. MSRP di 149.99$.

L'altro Case è il DAN Cases A3. Si tratta di un case Micro ATX che offre un design ""all-mesh"", supporta radiatori per aio fino a 360mm e ha un design desktop, in cui la GPU è posizionata orizzontalmente. Presenta anche una funzione di regolazione per l'alimentatore offrendo compatibilità a psu fino a 140mm. MSRP di 69.99$

Lian Li ha avuto modo di presentare anche le nuove versioni dell'O11 Vision. Parliamo dell'O11 Vision Back Connect e l'O11 Vision Chrome. Il back connect offre compatibilità per le schede madri atx e matx che hanno le connessioni volte verso il lato posteriore, mentre il Chrome è dotato, invece, di pannelli di vetro cromati, che creano un'estetica riflettente. Mr Matt Lee ha già pubblicato una cinematica di una build a riguardo:

* [https://www.youtube.com/@Mr\_Matt\_Lee](https://www.youtube.com/@mr_matt_lee)
* [https://www.youtube.com/results?search\_query=lian+li+o11+vision](https://www.youtube.com/results?search_query=lian+li+o11+vision)

Degna di nota, invece, è stata la presentazione della serie di alimentatori ""EDGE"". Si tratta di alimentatori pensati per adattarsi perfettamente a configurazioni di case ""dual chamber"". Includono una sorta di allungamento del PSU in cui i connettori classici sono rivolti perpendicolarmente al psu stesso, in una sorta di design a ""L"", il che permette di inserire i cavi con più facilità. Nella classica posizione dei connettori, invece, si avrà un hub di connettori USB interni, che permetteranno di alimentare meglio i vari dispositivi del PC e aumentare il numero di header usb a disposizione.

Sono stati presentati, inoltre, anche la nuova scrivania DK-07, con pannelli in alluminio e un monitor oled trasparente della LG da 30"", e il nuovo AIO GALAHAD II LCD SHIFT, che porta con sé la novità di avere i tubi nascosti lungo il bordo del radiatore.

Cosa ne pensate?

**Fonti**: [guru3d](https://www.guru3d.com/story/lian-li-2024-digital-expo-some-intersting-new-products-revealed/), [wccftech](https://wccftech.com/lian-li-unveils-next-gen-pc-cases-cooling-solutions-psus-at-digital-expo-2024/)

# Disponibile AMD ROCm 6.0

Con la sesta release di ROCm, AMD include il supporto alle schede video Radeon PRO W7800 e RX 7900 GRE, che si aggiungono alle RX7900XT, XTX e Radeon Pro W7900 già incluse in ROCm 5.7, che ha incluso anche il supporto a PyTorch. La novità, però, è anche l'annuncio al supporto alle runtime ONNX, il che permetterà agli utenti di poter fare inferenza su una più ampia gamma di sorgenti dati direttamente in locale ed espandere gli strumenti a disposizione in ambito ML. Continua, dunque, la volontà di AMD di rendere più accessibile agli sviluppatori le potenzialità IA dei suoi hardware, che verranno espanse nel tempo come dichiarato da AMD stessa.

È la volta buona per AMD per mettersi un po' più alla pari con nVidia in campo IA?

**Fonti**: [guru3d](https://www.guru3d.com/story/amd-rocm-60-released-adds-support-for-radeon-pro-w7800-rx-7900-gre-gpus/), [videocardz](https://videocardz.com/newz/amd-rocm-6-0-adds-support-for-radeon-rx-7900-gre-and-pro-w7800), [amd](https://community.amd.com/t5/ai/amd-expands-ai-offering-for-machine-learning-development-with/ba-p/662026)

# Arrivato in Italia l'ASUS Zenbook Duo 2024

Presentato durante il CES 2024, parliamo di un nuovo portatile che, all’apparenza, sembrerebbe avere un form factor classico, ma la tastiera è, invece, appoggiata su un secondo monitor che, grazie alla sua apposita cornice e ad un supporto integrato, consente ai due schermi di essere appoggiati orizzontalmente o verticalmente.

I due display da 14″ sono degli OLED con touchscreen, risoluzione 3K e refresh rate di 120Hz. Il tutto rimane contenuto in una soluzione dal peso di soli 1,35Kg. Sarà possibile equipaggiarlo, al massimo, con il nuovo Intel Core Ultra 9 185H, 32GB di RAM DDR5 e 2 TB di storage su SSD NVMe PCIe Gen4.

Il modello in questione, l'UX8406, è già disponibile sullo store ufficiale di ASUS al prezzo di partenza di 2199€. Che ne pensate?

**Fonti**: [tomshw](https://www.tomshw.it/hardware/asus-zenbook-duo-2024-arriva-in-italia-e-si-puo-gia-comprare), [punto-informatico](https://www.punto-informatico.it/asus-zenbook-duo-disponibile-italia/)

# Intel Core i9-14900KS: 24 Core, 32 Thread e Frequenza Max di 6.2Ghz!

Intel ha annunciato il rilascio, per la metà di Marzo, del nuovo processore high end i9-14900KS. Parliamo di una CPU dotata di 24 core e 32 thread, comprensiva di 8 P-Core e 16 E-Core, che incorpora 36MB di Cache L3 e 32MB di Cache L2. Essendo una versione specializzata del classico 14900k, introduce una frequenza massima di ben 6.2ghz, ad un TDP PL1 di 150W con picchi di 410W, per una potenza media di 330W in load, superando di 200Mhz di picco del modello k base.

Le temperature di funzionamento sono altissime, con picchi di 101°C nei test, pertanto serviranno sicuramente soluzioni dissipanti top di gamma e mobo con VRM abbastanza generosi.

Quanto lo trovate utile? In che ambito verrà usato maggiormente? Solo per overclockers?

**Fonti**: [guru3d](https://www.guru3d.com/story/intel-core-i914900ks-processor-unveiled-24-cores-32-threads-up-to-62-ghz-frequency/), [hwupgrade](https://www.hwupgrade.it/news/cpu/intel-core-i9-14900ks-la-cpu-capace-di-toccare-i-6-2-ghz-potrebbe-arrivare-a-meta-marzo_124389.html)

# News Bonus > OpenAI Sora: la nuova frontiera del videomaking con IA?

OpenAi ha introdotto Sora, un nuovo modello ""text-to-video"" che permette di generare video, per ora lunghi fino a un minuto, a partire dai prompt degli utenti. Non è una novità in tutto e per tutto, ma la qualità video mostrata è nettamente più realistica delle tech usate fino ad ora, come Lumiere, Synthesia, HourOne o DeepBrain, per citarne alcuni.

Secondo OpenAI, Sora sarà in grado di generare scene complesse con più personaggi, con specifici tipi di movimento e offrendo dettagli accurati sia dei soggetti che degli sfondi. Il modello usato da Sora, inoltre, sarà in grado non solo di comprendere ciò che l'utente chiede nel prompt, ma anche come queste cose si integrano ed esistono nel mondo fisico.

Siamo all'alba di una nuova frontiera nell'utilizzo delle IA? Come si comporteranno gli hardware casalinghi attuali? Avete visto i video di Sora?

**Fonti**: [OpenAI](https://openai.com/sora), [hwupgrade](https://www.hwupgrade.it/news/web/openai-sora-il-nuovo-modello-ia-realizza-video-credibili-partendo-da-un-prompt-testuale_124430.html)

Uno tra questi componenti sarà all'interno della vostra build? Utilizzerete ROCm 6 o utilizzate le vostre GPU per l'IA? State progettando una nuova build e avete dei dubbi o volete farvela riesaminare con altri punti di vista? Avete BSOD, problemi di compatibilità, errori o altro da risolvere sulla vostra build? A voi la palla :)

**Regole**

* Solo all'interno di questo post andremo in deroga alla regola numero 5 riguardante le richieste di consigli e sarà quindi possibile commentare richiedendo suggerimenti riguardanti l'hardware
* Potete postare anche voi news o rumors sui quali discutere",/r/ItalyInformatica/comments/1avc74v/il_circolino_dellhardware_pt2/,ItalyInformatica,r/ItalyInformatica,False,relevant,highly relevant
We're fast approaching a true point of complete and utter distrust of anything on the internet,RickyWicky,https://www.reddit.com/r/Showerthoughts/comments/1as6olm/were_fast_approaching_a_true_point_of_complete/,1216,148,1708084022.0,1as6olm,"With OpenAI's SORA text to video on the scene, and with how fast this tech is improving in general, within the next few years it's going to be very hard to distinguish fact from fiction online, regardless of the content type. 

The only thing that would have any semblance of validity would be a live stream. But the tech could eventually advance so much that it can synthesize believable video in real time, so that won't last forever. The internet is going to get weirder than it already is. I don't know if I should brace myself or cry. 

It's still a ways off, of course, but it feels like that's where things are headed from this point of view.",/r/Showerthoughts/comments/1as6olm/were_fast_approaching_a_true_point_of_complete/,Showerthoughts,r/Showerthoughts,False,relevant,less relevant
"What's new in e-commerce? 🔥 Week of February 19th, 2024",adventurepaul,https://www.reddit.com/r/ShopifyeCommerce/comments/1aux5pz/whats_new_in_ecommerce_week_of_february_19th_2024/,6,2,1708373301.0,1aux5pz,"Hi [r/ShopifyeCommerce](https://www.reddit.com/r/ShopifyeCommerce/) \- I'm Paul and I follow the e-commerce industry closely for my [Shopifreaks E-commerce Newsletter](https://www.shopifreaks.com/). Each week I post a summary recap of the week's top stories, which I cover in depth in the newsletter. Let's dive in...   
.

1. 65% of executives across North America and Europe believe that B2B e-commerce is broken at their organizations. A study conducted by Forrester Consulting on behalf of Zoovu found that B2B buyers now expect B2B experiences to align with the consumer experiences of their personal lives — quick, convenient, and personalized.  
.
2. 20% of U.S. adults surveyed said they would drop their Amazon Prime membership if another brand offered equivalent benefits. Well, their wish may come true if **Target** can deliver with its new premium membership offering. Internally dubbed “Project Trident” (like the gum), the new membership program, which will be positioned to compete against Amazon Prime and Walmart+, could launch as soon as this year, according to insiders.  
.
3. **Amazon** has become the latest company to argue that the structure of the National Labor Relations Board (NLRB) violates the U.S. Constitution, following in the footsteps of SpaceX and Trader Joe's. Amazon claimed in a recent filing that the labor board's case, which accuses the company of illegally retaliating against unionizing workers, should be dismissed because the board itself is unconstitutional. Amazon says that the NLRB's structure “violates the separation of powers” because administrative law judges and board members are largely insulated from presidential oversight and removal, which impedes the executive power provided in Article II of the Constitution. Amazon attorneys also argues that NLRB proceedings deny the company a trial by a jury and violate its due-process rights under the Fifth Amendment.  
.
4. **Walmart** is in talks to buy **Vizio's TV business** for $2B, but it's not really about the TVs themselves. It's more about what happens on the TVs. The deal would give Walmart access to the customer data collected by Vizio's smart TV platform, as well as to the revenue stream created by serving up personalized ads and taking a cut of subscription fees. This would help Walmart compete against Roku, which last year launched its own line of smart TVs, and Amazon, which is steadily building out its lineup of Fire TVs while expanding ads on the TV's operating system.  
.
5. The EU's new rules under the **Digital Services Act** went into full effect on Saturday for companies of all sizes, setting new legal obligations on thousands of platforms and digital businesses in the region. EU Member States must have empowered their Digital Services Coordinator by then, with the DSA now fully applicable for all entities within its scope (not just for large companies, in which the rules went into effect in August 2023).  
.
6. **Google Bard** officially rebranded to Google Gemini and launched a premium / paid version called Gemini Advanced as well as a mobile app. Gemini Advanced is now available as part of the company's new Google One AI Premium Plan for $19.99/mo and claims to be more capable at highly complex tasks like coding, logical reasoning, following nuanced instructions and collaborating on creative projects.  
.
7. **Google** also internally launched a large language model named **Goose** to assist employees with writing code, which they describe as a “descendant of Gemini” that is “trained on the sum total of 25 years of engineering expertise at Google.” Goose can answer questions around Google-specific technologies, write code using internal tech stacks, and support capabilities such as editing code based on natural language prompts.  
.
8. 5 minutes later, not to be outdone by Google, **OpenAI** unveiled its new text-to-video generation tool called Sora, which it is currently testing in private beta. It's almost as if they had Sora in their back pocket, waiting to pull it out and divert attention away from a Google AI announcement at a moment's notice. Sora is an AI model that can create realistic video scenes from text instructions. OpenAI has not indicated a timeline for when it will be available to the public, but noted that it will be “taking several important safety steps ahead of making Sora available in OpenAI's products” such as combating misinformation, hateful content, and bias within the model.  
.
9. Last week Super Bowl 2024 shattered records becoming the most-watched television event in U.S. history. **X** published its own press release, claiming that the Super Bowl was also one of the biggest events ever on its social media platform, with more than 10B impressions and over 1B video views. But were the numbers it claimed true? According to data provided by CHEQ, a cybersecurity firm that tracks bots and fake users, a whopping 75.85% of traffic from X to its advertisers' websites during the weekend of the Super Bowl was fake! CHEQ founder and CEO Guy Tytunovich told Mashable, “I've never seen anything even remotely close to 50%, not to mention 76%. I'm amazed… I've never, ever, ever, ever seen anything even remotely close.”  
.
10. **Mexico's** antitrust regulator, Cofece, recommended that the government order **Amazon** and **Mercado Libre** to take “corrective measures” to ensure competition in the e-commerce market after determining that the two marketplaces account for 85% of e-commerce sales within the country. These measures would include giving services providers greater transparency and separating streaming services from marketplace membership within the next six months.  
.
11. Every year, Insider Monkey ranks the **top 20 fastest growing publicly traded e-commerce companies** with at least $100M in annual revenue. The top five spots went to PDD Holdings (parent company of Temu and Pinduoduo), Hepsiburada (the first and only EU company to make the list), LightInTheBox (Singaporean marketplace), Global-e Online (helps companies sell internationally), and MercadoLibre (largest marketplace in Latin America).   
.
12. **New York City** is hoping to ease e-commerce freight congestion within the city by shifting the freight transported by more than 6,200 short-haul trucks to barges that will dock at six new maritime shipping hubs on the city's waterfront. The plan, dubbed Blue Highways, aims to design barge landings and access points where e-bikes and small delivery vehicles can pick up packages for last-mile delivery, which it estimates will eliminate 92M miles of truck travel every year.   
.
13. **Temu** sales fell 12.5% MoM in December and 4.8% in January, a sharp drop from the app's growth of more than 50% in mid-2023, according to Bloomberg Second Measure data, while overall U.S. retail sales rose in December. A late-January from Morgan Stanley found that nearly a third of Temu users plan to shop less on the platform over the next three months, ranking only above eBay and Etsy which had weaker outlooks.   
.
14. **China’s consumer prices** fell at their fastest pace in 15 years in January, falling 0.8% compared with a year earlier, signaling deeper deflation and weakening consumer demand. China's economy first entered deflation last summer, with prices falling at a faster pace since then.   
.
15. **Factory owners in China** are claiming that listing their goods on Alibaba no longer guarantees sales like the old days, and they are now listing their products on rival marketplaces like Pinduoduo, JD.com, and Douyin. This new trend is a far cry from the early years when Alibaba's sites were the only option for merchants, leading to factory-lined streets to be named after the company and its founder — like Ali Road and Jack Ma Boulevard.   
.
16. **Waterbucket**, a startup that specializes in dynamic creative optimization for e-commerce merchants, was granted a patent for its approach to dynamically adding “As low as” pricing in visual merchandising ads. The technology dynamically inserts BNPL prices into product images and videos to improve click-thrus by enticing consumers with the lower monthly installment price.   
.
17. **Heather Hurst-McKee,** an e-commerce entrepreneur from El Cajon, California, was sued by several former clients who claim to have lost tens of thousands of dollars after purchasing e-commerce stores from her via Biz Buy Sell. Buyers said that Hurst-McKee included Zoom training sessions in the contract to teach the buyers how to run the stores, but then either ghosted them or offered 10-20 minute worthless sessions. The buyers claimed that the purchased stores also made them no money.   
.
18. **YouTuber Matty Benedetto**, who publishes the Unnecessary Inventions channel, went viral for creating a 50-foot rug inspired by a long CVS receipt. First he re-created a high-resolution digital version of his most recent CVS receipt, and then found a rug manufacturer in China that could reproduce it. The rug arrived about 3 weeks later and now sits in his studio.   
.
19. **The Federal Reserve Bank of New York** reported that it's rare for consumers to use BNPL just once and that about 72% of financially stable users and 89% of financially fragile users (those with credit scores below 620) have made multiple BNPL purchases over the past 12 months. The study found that about 60% of financially fragile consumers use the installment options as many as five times per year.   
.
20. **Consumers shopping online after midnight** are often making riskier transactions and are more likely to default on their loans, according to Affirm CFO Michael Linford, who said that credit delinquencies spike right around 2am. It's like Ted Mosby's mom used to always say, “Nothing good ever happens after 2am.” Last week Affirm reported that 30-day delinquencies on monthly loans held steady from a year earlier at 2.4%, even as total purchase volumes surged 32% during that time.   
.
21. **AMC** notified subscribers of a proposed $8.3M settlement that provides awards to an estimated 6M subscribers of its streaming services in response to allegations that the company illegally shared subscribers' viewing history with Google, Facebook, and X. Sharing that data is in violation of the 1988 Video Privacy Protection Act, which prohibits video service providers from sharing “information which identifies a person as having requested or obtained specific video materials or services from a video tape service provider,” originally passed to protect consumers' viewing habits when renting at Blockbuster and other video rental stores. “Uh oh” said every other streaming platform.   
.
22. **Nvidia** passed Amazon and Alphabet to become the third most valuable company in the U.S., after seeing its share price grow 221% over the past year due to the surge in demand for its GPUs. The company's market cap is now $1.81T compared to Alphabet's $1.78T and Amazon's $1.76T, trailing just Apple and Microsoft in market cap.   
.
23. **Jitterbit**, a cloud-based integration platform that helps e-commerce and SaaS companies automate through pre-built no-code workflows, named Bill Conner as its new President, CEO, and board member, effective immediately. Conner has previously led multiple cybersecurity, SaaS, data networks, and other tech companies including Entrust and SonicWall.   
.
24. Last week I reported that **Shopify** was upping the price of its Shopify Plus subscription from $2,000/month to $2,500/month for merchants on a one-year term, and now some are saying that the price hike could have negative effects on app developers and startups building in its ecosystem. If merchants decide to stick with Shopify after the price hike, it means they will likely look to trim costs elsewhere in their stack stack, which could mean churn in the Shopify app ecosystem.   
.
25. **eBay** publicly announced that its Monthly Community Chat for sellers has been discontinued as the company moves to occasional, not live chats centered around specific updates or events. Moving forward, eBay will host threads in the Buying & Selling forums, and users will have one week to submit their questions. (Remember eBay, the tighter you grip seller conversation, the more you'll lose control of it.)   
.
26. In other **eBay** news, the company told a federal judge last week that it does not sell the items on its platform and therefore the court should dismiss the lawsuit filed against it by the EPA in September over the alleged sale of unlawful pesticides and high-emission car parts. The government argued that eBay is the seller of illegal goods that violate the Clean Air Act and other laws, however, eBay points to Section 230 to say that it shouldn't be responsible for listings published by third parties on its platform.   
.
27. **Online revenue** fell 7% YoY in January against a decline of 3.5% last year, according to the IMRG Online Retail Index. Since the high growth rates seen during the pandemic, e-commerce revenue declined 10% YoY in 2022 and 3% YoY in 2023, with IMRG forecasting 0% YoY growth in 2024.   
.
28. **Misfits Market** started offering e-commerce fulfillment services to companies that sell perishable products directly to consumers. The online grocer is leveraging its five temperature-controlled warehouses and in-house delivery network to provide the new service, known as Fulfilled by Misfits.   
.
29. Some layoffs to report this week… **Nike** is laying off more than 1,600 employees, or about 2% of its workforce, following its December announcement of a cost-savings plan. **Instacart** is laying off 250 workers as part of a restructuring program it says will help “flatten the organization” to focus on core initiatives. **Cisco** is laying off 4,250 employees globally, which represents about 5% of its workforce. And **Mozilla** is laying off around 60 employees as it scales back its investment in products like its VPN, Relay, and Online Footprint Scrubber.   
.
30. **40% of Gen Z consumers** surveyed said they are tired of hearing about Amazon and are actively trying to shop less on the marketplace, with 60% believing Amazon is too powerful. One-fifth of adults said they would drop their Amazon Prime membership if another brand offered equivalent benefits.   
.
31. Speaking of **Amazon**, the company said that it surpassed $8B in cumulative exports from India last year, up from $5B in 2022, and is set to achieve its ambitious target of $20B in exports by 2025. The news comes shortly after Walmart said that it surpassed $30B in sourcing from India in over two decades and is now targeting $10B annually by 2027.   
.
32. **Wix** is launching Wix Capital Accelerator Program, designed to recruit Israel-based pre-seed and seed-stage companies that are creating disruptive business platforms, tools, and management solutions for SMBs. “Hello, uh, we're building a Shopify app…”  
.
33. Plus 8 seed rounds, IPOs, and acquisitions of interest including **Bob W**, a Finland-based marketplace for premium short-term rentals in Europe, raising €40M in a Series B round.  
.

 I hope you found this recap helpful. See you next week!

For more details on each story and sources, see the full edition: [https://www.shopifreaks.com/target-prime-walmart-tvs-openai-text-to-video/](https://www.shopifreaks.com/target-prime-walmart-tvs-openai-text-to-video/)

**What else is new in e-commerce?** Share stories of interesting in the comments below (including in your own business) or on [r/shopifreaks](https://www.reddit.com/r/shopifreaks/).

\-PAUL  
Editor of Shopifreaks E-commerce Newsletter",/r/ShopifyeCommerce/comments/1aux5pz/whats_new_in_ecommerce_week_of_february_19th_2024/,ShopifyeCommerce,r/ShopifyeCommerce,False,relevant,highly relevant
We need a game plan for Sora AI,Aiur-Dragoon,https://www.reddit.com/r/NAFO/comments/1asjgna/we_need_a_game_plan_for_sora_ai/,25,7,1708117216.0,1asjgna,"For context, Open Ai is unveiling a new text to video generator, lightyears ahead of anything that was used in the past. To the critical eye, there's still some artifacts that are a dead giveaway, but most people will be fooled.

&#x200B;

Undoubtably, Vatniks, Tankies, and Fascists will use these tools to try to sway more people to their side. What can we do to combat this?",/r/NAFO/comments/1asjgna/we_need_a_game_plan_for_sora_ai/,NAFO,r/NAFO,False,relevant,less relevant
New GPT : Video Prompter,neulvo,https://www.reddit.com/r/GPTStore/comments/1as0t2a/new_gpt_video_prompter/,3,3,1708060879.0,1as0t2a,"for upcoming surge of the text-to-video models, I made a GPT that helps generating a text-to-video prompt. It can advise you brainstorming and helps you generate a short video in the near future.

link : [https://chat.openai.com/g/g-fsw7QjBTg-video-prompter](https://chat.openai.com/g/g-fsw7QjBTg-video-prompter)",/r/GPTStore/comments/1as0t2a/new_gpt_video_prompter/,GPTStore,r/GPTStore,False,relevant,highly relevant
“Without having spiritual discernment” Babe wake up! a new Priesthood DLC was released with new special powers!!,Oldmelloyellow,https://i.redd.it/ryf3b398zyic1.jpeg,18,4,1708098978.0,1asbzez,"Mormons are now able to use their Uber special priesthood powers to tell if something is AI or not, incredible!! ",/r/exmormon/comments/1asbzez/without_having_spiritual_discernment_babe_wake_up/,exmormon,r/exmormon,False,relevant,highly relevant
Sam Fennah Being Spot On About The Devastating Consequences Of Gen AI Yet Again,Artist_On_The_Brink,https://www.reddit.com/gallery/1aswhxg,59,10,1708155703.0,1aswhxg,,/r/ArtistHate/comments/1aswhxg/sam_fennah_being_spot_on_about_the_devastating/,ArtistHate,r/ArtistHate,False,not relevant,less relevant
OpenAI favoring Sora over Dall-E,MysteriousPayment536,https://www.reddit.com/r/ChatGPT/comments/1as70xm/openai_favoring_sora_over_dalle/,6,5,1708085206.0,1as70xm,"Sora is the ChatGPT moment for text to video, it can do text to video & image to video. But it can also do text to image according to OpenAI ([https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)) 

## ""Image generation capabilities

Sora is also capable of generating images. We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.""  


I think OpenAI will kill Dall-e in favor for Sora: 

&#x200B;

1. Saving of resources to run Dall-e 3 and Sora parallel for the API and ChatGPT users
2. Sora can generate images of 2048x2048. Which is higher than Dall-e 3, SDXL and on pair with Midjourney v6
3. More data for future improvements
4. More money for OpenAI because of the hype  


Thoughts??",/r/ChatGPT/comments/1as70xm/openai_favoring_sora_over_dalle/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
Will software engineering be replaced by AI in next 20 years.,nishadastra,https://www.reddit.com/r/developersIndia/comments/1as85uj/will_software_engineering_be_replaced_by_ai_in/,67,130,1708088850.0,1as85uj,"Recently OpenAI launched Sora. This is a revolutionary text to video Creator. This has the potential to ruin 3D animation, cinematography. 
I am though more focused on my profession as Software Engineer. 
What is the future you see for this field? 
I don't see long before companies would Upload their entire codebase on AI platform where it will perform maintanenece, testing, enhancement etc",/r/developersIndia/comments/1as85uj/will_software_engineering_be_replaced_by_ai_in/,developersIndia,r/developersIndia,False,relevant,less relevant
Why did OpenAI name it as Sora?,chomacrubic,https://www.reddit.com/r/SoraAi/comments/1asdffd/why_did_openai_name_it_as_sora/,3,2,1708102456.0,1asdffd,"Here is the guess from Google Gemini:

Unfortunately, OpenAI hasn't explicitly stated the specific reasoning behind naming their text-to-video tool ""Sora."" However, we can speculate based on potential connections and interpretations:

**1. Meaningful connections:**

* **Sky/Soaring:** ""Sora"" translates to ""sky"" in Japanese, potentially referencing the limitless creative possibilities and freedom the tool offers. It could also allude to the tool's ability to create scenes in various settings, including the sky.
* **Character Inspiration:** ""Sora"" is the name of the protagonist in the video game ""Kingdom Hearts,"" known for his adventurous spirit and ability to travel between worlds. This could subtly represent the tool's ability to create diverse and fantastical worlds within videos.

**2. Interpretations:**

* **Innovation:** ""Sora"" sounds futuristic and innovative, aligning with the cutting-edge nature of the text-to-video technology.
* **Simplicity:** The short, two-syllable word is easy to remember and pronounce, making it accessible to a broad audience.
* **Neutrality:** It avoids specific cultural or linguistic connotations, ensuring its appeal to a global audience.

Ultimately, the true meaning behind the name ""Sora"" likely lies with OpenAI itself. We can appreciate the positive interpretations and potential connections, but the definitive answer remains undisclosed.",/r/SoraAi/comments/1asdffd/why_did_openai_name_it_as_sora/,SoraAi,r/SoraAi,False,relevant,highly relevant
Scary Times,Ok-Vegetable-8720,https://www.reddit.com/r/TransparencyforTVCrew/comments/1asjcvv/scary_times/,3,7,1708116954.0,1asjcvv,"https://openai.com/sora


https://www.theverge.com/2024/2/15/24074151/openai-sora-text-to-video-ai

Anybody else feel slightly threatened by the fast advancement of Video AI?

If it can look this good in Feb 2024, what will the capabilities be in a years time? Two years? Five years? Scary stuff.",/r/TransparencyforTVCrew/comments/1asjcvv/scary_times/,TransparencyforTVCrew,r/TransparencyforTVCrew,False,not relevant,less relevant
OpenAI Research: Video generation models as world simulators,aurumvexillum,https://openai.com/research/video-generation-models-as-world-simulators,45,23,1708051408.0,1arxu18,"I'm seeing numerous reposts of Sora's text-to-video samples, which are impressive in their own right, and showcase what is undoubtedly a massive leap forward for generative video models. However, the full range of the model's capabilities — outlined within the technical report — is truly remarkable. ",/r/artificial/comments/1arxu18/openai_research_video_generation_models_as_world/,artificial,r/artificial,False,relevant,highly relevant
This is exciting!,haaphboil,https://v.redd.it/lrff0yzejuic1,30,16,1708045236.0,1arvp8c,"With this OpenAI’s SORA text-to-video model, anyone can be a storyteller.

Anyone can be rajamouli!
Cinema definition eh maripotadi!
What do you think? Am I overreacting? What are your thoughts?",/r/Ni_Bondha/comments/1arvp8c/this_is_exciting/,Ni_Bondha,r/Ni_Bondha,True,not relevant,highly relevant
Thoughts on Sora from OpenAi?,GameControl21,https://www.reddit.com/r/CryptoMars/comments/1asjuol/thoughts_on_sora_from_openai/,3,2,1708118148.0,1asjuol,"I haven't done a lot of investing into these memecoins, but Sora seems like it is the next 100x.   


Sora is this new text-to-video AI generator from OpenAI. It seems very high-tech and futuristic.   


There is this new Sora token, that I think is fairly early. Anybody have recommendations?  


Here is the BSC Scan:  [Sora (SORA) Token Tracker | BscScan](https://bscscan.com/token/0x45530c45b96baf883c45dc0a0e46c1652a49099c) 

Should I just drop $500?",/r/CryptoMars/comments/1asjuol/thoughts_on_sora_from_openai/,CryptoMars,r/CryptoMars,False,not relevant,highly relevant
"I primi video creati con Sora, un nuovo strumento di OpenAI per generare filmati con l’intelligenza artificiale",zio_otio,https://www.ilpost.it/flashes/open-ai-presenta-sora-video/,40,82,1708038307.0,1art4n9,"Poche ore fa OpenAI ha presentato Sora, il suo nuovo modello text-to-video in grado di creare video realistici a partire da istruzioni testuali. Maggiori dettagli sul sito di OpenAI https://openai.com/sora",/r/italy/comments/1art4n9/i_primi_video_creati_con_sora_un_nuovo_strumento/,italy,r/italy,False,relevant,highly relevant
What should I expect from Sora as a 3d artist??,3D3Dmods,https://www.reddit.com/r/singularity/comments/1asdzvh/what_should_i_expect_from_sora_as_a_3d_artist/,8,13,1708103827.0,1asdzvh,"Hey everyone, I have been a 3d modeler for the past five years; the moment I saw the notification saying ""openAI launches text to video model,"" My first thought was how good the 3d models or animated characters, if any. Then I went to the website and saw that each and every video was beyond my expectations!! But the 3d rendered or 3d modeled stuff like the bunny playing with a candle. I want to hear opinions on what you guys think about its 3d model rendering capabilities. ",/r/singularity/comments/1asdzvh/what_should_i_expect_from_sora_as_a_3d_artist/,singularity,r/singularity,False,relevant,highly relevant
Google lumiere vs OpenAi Sora Comparison -By Gemini,Yazzdevoleps,https://i.redd.it/5t231u96rvic1.jpeg,10,1,1708059952.0,1as0j1h,,/r/Bard/comments/1as0j1h/google_lumiere_vs_openai_sora_comparison_by_gemini/,Bard,r/Bard,False,not relevant,neutral
What should I expect from Sora as a 3d artist??,3D3Dmods,https://www.reddit.com/r/ArtificialInteligence/comments/1ase01y/what_should_i_expect_from_sora_as_a_3d_artist/,7,19,1708103838.0,1ase01y,"Hey everyone, I have been a 3d modeler for the past five years; the moment I saw the notification saying ""openAI launches text to video model,"" My first thought was how good the 3d models or animated characters, if any. Then I went to the website and saw that each and every video was beyond my expectations!! But the 3d rendered or 3d modeled stuff like the bunny playing with a candle. I want to hear opinions on what you guys think about its 3d model rendering capabilities. ",/r/ArtificialInteligence/comments/1ase01y/what_should_i_expect_from_sora_as_a_3d_artist/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
AI Weekly Rundown (February 11 to February 19),RohitAkki,https://www.reddit.com/r/ArtificialInteligence/comments/1auriad/ai_weekly_rundown_february_11_to_february_19/,4,1,1708360258.0,1auriad,"Major AI announcements from Google, IBM, ElevenLabs, OpenAI, Reddit and more.  


* **DeepSeekMath advances math reasoning abilities in LLMs**  
\- DeepSeek AI has developed a new 7 bn parameter AI model called DeepSeekMath that specializes in mathematical reasoning. After training on a 120 bn token math corpus, it achieves state-of-the-art performance on English and Chinese math benchmarks, even rivaling commercial models like GPT-4.
* **Google introduces localllm for local LLM access and has open-sourced Magika**  
\- Google has introduced localllm, an open-source tool that allows developers to run LLMs locally on CPUs instead of GPUs. This provides easy access to efficient LLMs within Cloud Workstations, enhancing productivity and cost efficiency.  
\- Magika helps accurately detect binary and textual file types. Under the hood, Magika employs a custom, highly optimized deep-learning model, enabling precise file identification within milliseconds, even when running on a CPU.
* **Apple is reportedly working on AI updates to Spotlight and Xcode**  
\- AI chatbot-like search features for Spotlight search could let iOS and macOS users make natural language requests, like with ChatGPT, to get weather reports or operate features deep within apps. Apple has also expanded internal testing of new generative AI features for its Xcode programming software and plans to release them to third-party developers this year. 
* **IBM researchers manipulate live phone calls**  
\- IBM researchers have demonstrated how multiple AI services can intercept and alter live phone calls. By processing audio through speech recognition and natural language models, they swapped real bank account details with AI-generated fakes. It shows how AI could enable large-scale deception and privacy violations.
* **More agents = more performance**  
\- Tecent Research team’s new research suggests boosting language model performance by adding more agents. They use a ""sampling-and-voting"" method, where the input task is run multiple times through a language model with several agents to generate various results. These results are then subjected to majority voting to determine the most reliable result. 
* **SoftBank’s founder is seeking about $100 billion for an AI chip venture**  
\- ​​Masayoshi Son envisions creating a company that can complement the chip design unit Arm Holdings Plc. The AI chip venture is code-named Izanag and will compete with Nvidia and supply semiconductors essential for AI.
* **Reddit signs AI content licensing deal ahead of IPO**  
\- It has signed a contract allowing a company (unnamed) to train its AI models on its content. Reddit told prospective investors in its IPO that it had signed the deal, worth about $60 million on an annualized basis, earlier this year.
* **Google DeepMind’s MC-ViT enables long-context video understanding**  
\- Google DeepMind introduces memory consolidated vision transformer (MC-ViT) that effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos.
* **ElevenLabs’ AI voice cloning lets you turn your voice into passive income**  
\- The voice cloning program allows all voice-over artists to create professional clones, share them with the Voice Library community, and earn rewards/royalty every time soundbite is used. 
* **Nvidia launches offline AI chatbot trainable on local data**   
\- Nvidia has released ""Chat with RTX"", an AI chatbot you can customize by training it on your private local data, including PDFs, videos, links, and more formats.
* **ChatGPT can now remember conversations**   
\- OpenAI is releasing a memory feature for ChatGPT, allowing it to remember details from previous conversations to provide more helpful and personalized responses. Users control what's remembered and can delete memories anytime. 
* **Cohere launches open-source 101-language AI model**   
\- Cohere for AI has launched Aya, a new open-source LLM supporting 101 languages. With a dataset covering lesser-resourced languages, Aya is making AI accessible for overlooked cultures by tackling data outside English training content.
* **Apple develops Keyframer: A web-based generative AI animation tool using LLM**  
\- Apple's Keyframer is a prototype generative AI animation tool that animates 2D images based on text descriptions. It uses OpenAI's GPT-4 as its base model to generate CSS code for animation.  
Stability AI introduced Stable Cascade- It is a new text-to-image model released under a non-commercial license for non-commercial use only. It integrates three distinct models (Stages A, B, and C) for the hierarchical compression of images in a highly compressed latent space, offering significant computational efficiency.
* **OpenAI and Microsoft disrupted the activities of 5 state-affiliated threat actors**  
\- OpenAI and Microsoft collaborated to identify and terminate accounts associated with threat actors from China, Iran, North Korea, and Russia who were using OpenAI's services for malicious purposes, including generating content for phishing campaigns and conducting open-source research.
* **OpenAI launches text-to-video model**  
\- OpenAI unveiled Sora, a generative AI model that creates (60-sec) video from text or a still image. Sora can generate 1080p movie-like scenes with multiple characters, different types of motion, and background details. Sora is currently in research preview.
* **Google announces Gemini 1.5 with 1 million tokens!**   
\- This is a breakthrough in long-context understanding, as it is the longest context window of any large-scale foundation model yet. It’ll make the model’s output more consistent, relevant, and useful. Gemini 1.5 Pro with up to 1M tokens is now available to a limited group of developers in Google AI Studio and in private preview on VertexAI.
* **Meta is releasing V-JEPA, a method for teaching machines to understand and model the physical world by watching videos**  
\-  V-JEPA is a non-generative model that learns by predicting missing or masked parts of a video in an abstract representation space. This work is an important milestone for advancing machine intelligence (AMI).
* And there was more…   
\- Perplexity partners with Vercel to bring AI search to apps  
\- Mistral quietly released a new model in testing called ‘next’  
\- Volkswagen sets up “AI Lab” to speed up its AI development initiatives  
\- Tech giants use AI to monitor employee messages  
\- Disney harnesses AI to bring contextual ads to streaming  
\- Microsoft hints at a more helpful Copilot in Windows 11  
\- NVIDIA CEO Jensen Huang advocates for each country’s sovereign AI  
\- Google to invest €25 million in Europe to uplift AI skills  
\- NVIDIA surpasses Amazon and Alphabet in market value   
\- Microsoft might develop an AI upscaling feature for Windows 11  
\- Fandom rolls out controversial generative AI features  
\- NVIDIA presents OpenMathInstruct-1, a 1.8 million math instruction tuning dataset  
\- Nous Research released 1M-Entry 70B Llama-2 model with advanced steerability   
\- Otter launches AI meeting buddy that can catch up on meetings  
\- ElevenLabs announced expansion for its speech-to-speech in 29 languages  
\- Airbnb plans to build ‘most innovative AI interfaces ever  
\- Amazon develops BASE TTS text-to-speech model with emergent abilities  
\- Nokia unveils AI assistant for industrial workers  
\- Slack adds AI-fueled search and summarization to the platform  
\- Apple readies an AI-based code completion tool to rival GitHub Copilot  
\- Google announces free AI cyber tools to strengthen online security  
\- OpenAI is reportedly developing AI web search to directly compete with Google  
\- Microsoft pledges $3.44bn to boost Germany's AI industry amid economic challenges  
\- Penn Engineering launches first Ivy League undergraduate major degree in AI

More detailed breakdown of these news and innovations in the [daily newsletters](https://theaiedge.substack.com/).",/r/ArtificialInteligence/comments/1auriad/ai_weekly_rundown_february_11_to_february_19/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
ملاحظة ممتازة,InterviewIcy101,https://i.redd.it/33904tr0eyic1.png,5,1,1708091897.0,1as97k1,,/r/EgyptExTomato/comments/1as97k1/ملاحظة_ممتازة/,EgyptExTomato,r/EgyptExTomato,False,not relevant,neutral
Why do the stills in the videos look better than the images that DALL-E create?,davesalias,https://www.reddit.com/r/SoraAi/comments/1arq0ph/why_do_the_stills_in_the_videos_look_better_than/,21,2,1708030702.0,1arq0ph,"Not just as a text to video tool, but also just the image quality by itself looks insane in this model, I assume it's just completely different architecture, but I wish they would release a txt2img tools as well with this.",/r/SoraAi/comments/1arq0ph/why_do_the_stills_in_the_videos_look_better_than/,SoraAi,r/SoraAi,False,relevant,highly relevant
What should I expect??,3D3Dmods,https://www.reddit.com/r/SoraAi/comments/1asdek9/what_should_i_expect/,3,4,1708102395.0,1asdek9,"Hey everyone, I have been a 3d modeler for the past 5 years; the moment I saw the notification saying ""openAI launches text to video model,"" My first thought was how good the 3d models or animated characters, if any. Then I went to the website and saw that each and every video was beyond my expectations!! But the 3d rendered or 3d modeled stuff like the bunny playing with a candle. I want to hear opinions on what you guys think about its 3d model rendering capabilities. ",/r/SoraAi/comments/1asdek9/what_should_i_expect/,SoraAi,r/SoraAi,False,relevant,highly relevant
OpenAI Sora and new forms of spectacle,Avethle,https://www.reddit.com/r/Situationism/comments/1ass86j/openai_sora_and_new_forms_of_spectacle/,6,2,1708140786.0,1ass86j,"I feel like new situationist theory is gonna drop real soon. I don't want to speculate too hard before we see new observable phenomena in society caused by text-to-video because I'll probably be wrong. But I do feel like this marks a shift in the political economy behind the spectacle. The spectacle proliferated in the 20th century as the development of the material base of society created media on which moments can be pseudocyclically replayed. But it used to be that the spectacle was driven by the aggregate unconscious behavior of society. I made a post a while back about [in the last decade, instead of you directly spectating, your path on the terrain of the spectacle has been managed for you by the algorithm](https://www.reddit.com/r/GenZSituationist/comments/10bvokv/the_digital_spectacle/). But I mean now, even the imagination of the spectacular images is going to be done you by machine. This is actually crazy. Could the same technology that the new spectacle is based on also be detourned by using new tactics to disrupt the spectacle? I don't know.",/r/Situationism/comments/1ass86j/openai_sora_and_new_forms_of_spectacle/,Situationism,r/Situationism,False,relevant,highly relevant
What should I expect from Sora as a 3d artist??,3D3Dmods,https://www.reddit.com/r/aiArt/comments/1ase0mx/what_should_i_expect_from_sora_as_a_3d_artist/,2,2,1708103879.0,1ase0mx,"Hey everyone, I have been a 3d modeler for the past five years; the moment I saw the notification saying ""openAI launches text to video model,"" My first thought was how good the 3d models or animated characters, if any. Then I went to the website and saw that each and every video was beyond my expectations!! But the 3d rendered or 3d modeled stuff like the bunny playing with a candle. I want to hear opinions on what you guys think about its 3d model rendering capabilities. ",/r/aiArt/comments/1ase0mx/what_should_i_expect_from_sora_as_a_3d_artist/,aiArt,r/aiArt,False,relevant,highly relevant
"E-commerce Industry News Recap 🔥 Week of Feb 19th, 2024",adventurepaul,https://www.reddit.com/r/ecommerce/comments/1aux49x/ecommerce_industry_news_recap_week_of_feb_19th/,20,1,1708373206.0,1aux49x,"Hi r/ecommerce \- I'm Paul and I follow the e-commerce industry pretty closely for my Shopifreaks E-commerce Newsletter. Each week I post a summary recap of the week's top stories, which I cover in depth (with sources) in the newsletter. Let's dive in...  
.

1. 65% of executives across North America and Europe believe that B2B e-commerce is broken at their organizations. A study conducted by Forrester Consulting on behalf of Zoovu found that B2B buyers now expect B2B experiences to align with the consumer experiences of their personal lives — quick, convenient, and personalized.  
.
2. 20% of U.S. adults surveyed said they would drop their Amazon Prime membership if another brand offered equivalent benefits. Well, their wish may come true if **Target** can deliver with its new premium membership offering. Internally dubbed “Project Trident” (like the gum), the new membership program, which will be positioned to compete against Amazon Prime and Walmart+, could launch as soon as this year, according to insiders.  
.
3. **Amazon** has become the latest company to argue that the structure of the National Labor Relations Board (NLRB) violates the U.S. Constitution, following in the footsteps of SpaceX and Trader Joe's. Amazon claimed in a recent filing that the labor board's case, which accuses the company of illegally retaliating against unionizing workers, should be dismissed because the board itself is unconstitutional. Amazon says that the NLRB's structure “violates the separation of powers” because administrative law judges and board members are largely insulated from presidential oversight and removal, which impedes the executive power provided in Article II of the Constitution. Amazon attorneys also argues that NLRB proceedings deny the company a trial by a jury and violate its due-process rights under the Fifth Amendment.  
.
4. **Walmart** is in talks to buy **Vizio's TV business** for $2B, but it's not really about the TVs themselves. It's more about what happens on the TVs. The deal would give Walmart access to the customer data collected by Vizio's smart TV platform, as well as to the revenue stream created by serving up personalized ads and taking a cut of subscription fees. This would help Walmart compete against Roku, which last year launched its own line of smart TVs, and Amazon, which is steadily building out its lineup of Fire TVs while expanding ads on the TV's operating system.  
.
5. The EU's new rules under the **Digital Services Act** went into full effect on Saturday for companies of all sizes, setting new legal obligations on thousands of platforms and digital businesses in the region. EU Member States must have empowered their Digital Services Coordinator by then, with the DSA now fully applicable for all entities within its scope (not just for large companies, in which the rules went into effect in August 2023).  
.
6. **Google Bard** officially rebranded to Google Gemini and launched a premium / paid version called Gemini Advanced as well as a mobile app. Gemini Advanced is now available as part of the company's new Google One AI Premium Plan for $19.99/mo and claims to be more capable at highly complex tasks like coding, logical reasoning, following nuanced instructions and collaborating on creative projects.  
.
7. **Google** also internally launched a large language model named **Goose** to assist employees with writing code, which they describe as a “descendant of Gemini” that is “trained on the sum total of 25 years of engineering expertise at Google.” Goose can answer questions around Google-specific technologies, write code using internal tech stacks, and support capabilities such as editing code based on natural language prompts.  
.
8. 5 minutes later, not to be outdone by Google, **OpenAI** unveiled its new text-to-video generation tool called Sora, which it is currently testing in private beta. It's almost as if they had Sora in their back pocket, waiting to pull it out and divert attention away from a Google AI announcement at a moment's notice. Sora is an AI model that can create realistic video scenes from text instructions. OpenAI has not indicated a timeline for when it will be available to the public, but noted that it will be “taking several important safety steps ahead of making Sora available in OpenAI's products” such as combating misinformation, hateful content, and bias within the model.  
.
9. Last week Super Bowl 2024 shattered records becoming the most-watched television event in U.S. history. **X** published its own press release, claiming that the Super Bowl was also one of the biggest events ever on its social media platform, with more than 10B impressions and over 1B video views. But were the numbers it claimed true? According to data provided by CHEQ, a cybersecurity firm that tracks bots and fake users, a whopping 75.85% of traffic from X to its advertisers' websites during the weekend of the Super Bowl was fake! CHEQ founder and CEO Guy Tytunovich told Mashable, “I've never seen anything even remotely close to 50%, not to mention 76%. I'm amazed… I've never, ever, ever, ever seen anything even remotely close.”  
.
10. **Mexico's** antitrust regulator, Cofece, recommended that the government order **Amazon** and **Mercado Libre** to take “corrective measures” to ensure competition in the e-commerce market after determining that the two marketplaces account for 85% of e-commerce sales within the country. These measures would include giving services providers greater transparency and separating streaming services from marketplace membership within the next six months.  
.
11. Every year, Insider Monkey ranks the **top 20 fastest growing publicly traded e-commerce companies** with at least $100M in annual revenue. The top five spots went to PDD Holdings (parent company of Temu and Pinduoduo), Hepsiburada (the first and only EU company to make the list), LightInTheBox (Singaporean marketplace), Global-e Online (helps companies sell internationally), and MercadoLibre (largest marketplace in Latin America).   
.
12. **New York City** is hoping to ease e-commerce freight congestion within the city by shifting the freight transported by more than 6,200 short-haul trucks to barges that will dock at six new maritime shipping hubs on the city's waterfront. The plan, dubbed Blue Highways, aims to design barge landings and access points where e-bikes and small delivery vehicles can pick up packages for last-mile delivery, which it estimates will eliminate 92M miles of truck travel every year.   
.
13. **Temu** sales fell 12.5% MoM in December and 4.8% in January, a sharp drop from the app's growth of more than 50% in mid-2023, according to Bloomberg Second Measure data, while overall U.S. retail sales rose in December. A late-January from Morgan Stanley found that nearly a third of Temu users plan to shop less on the platform over the next three months, ranking only above eBay and Etsy which had weaker outlooks.   
.
14. **China’s consumer prices** fell at their fastest pace in 15 years in January, falling 0.8% compared with a year earlier, signaling deeper deflation and weakening consumer demand. China's economy first entered deflation last summer, with prices falling at a faster pace since then.   
.
15. **Factory owners in China** are claiming that listing their goods on Alibaba no longer guarantees sales like the old days, and they are now listing their products on rival marketplaces like Pinduoduo, JD.com, and Douyin. This new trend is a far cry from the early years when Alibaba's sites were the only option for merchants, leading to factory-lined streets to be named after the company and its founder — like Ali Road and Jack Ma Boulevard.   
.
16. **Waterbucket**, a startup that specializes in dynamic creative optimization for e-commerce merchants, was granted a patent for its approach to dynamically adding “As low as” pricing in visual merchandising ads. The technology dynamically inserts BNPL prices into product images and videos to improve click-thrus by enticing consumers with the lower monthly installment price.   
.
17. **Heather Hurst-McKee,** an e-commerce entrepreneur from El Cajon, California, was sued by several former clients who claim to have lost tens of thousands of dollars after purchasing e-commerce stores from her via Biz Buy Sell. Buyers said that Hurst-McKee included Zoom training sessions in the contract to teach the buyers how to run the stores, but then either ghosted them or offered 10-20 minute worthless sessions. The buyers claimed that the purchased stores also made them no money.   
.
18. **YouTuber Matty Benedetto**, who publishes the Unnecessary Inventions channel, went viral for creating a 50-foot rug inspired by a long CVS receipt. First he re-created a high-resolution digital version of his most recent CVS receipt, and then found a rug manufacturer in China that could reproduce it. The rug arrived about 3 weeks later and now sits in his studio.   
.
19. **The Federal Reserve Bank of New York** reported that it's rare for consumers to use BNPL just once and that about 72% of financially stable users and 89% of financially fragile users (those with credit scores below 620) have made multiple BNPL purchases over the past 12 months. The study found that about 60% of financially fragile consumers use the installment options as many as five times per year.   
.
20. **Consumers shopping online after midnight** are often making riskier transactions and are more likely to default on their loans, according to Affirm CFO Michael Linford, who said that credit delinquencies spike right around 2am. It's like Ted Mosby's mom used to always say, “Nothing good ever happens after 2am.” Last week Affirm reported that 30-day delinquencies on monthly loans held steady from a year earlier at 2.4%, even as total purchase volumes surged 32% during that time.   
.
21. **AMC** notified subscribers of a proposed $8.3M settlement that provides awards to an estimated 6M subscribers of its streaming services in response to allegations that the company illegally shared subscribers' viewing history with Google, Facebook, and X. Sharing that data is in violation of the 1988 Video Privacy Protection Act, which prohibits video service providers from sharing “information which identifies a person as having requested or obtained specific video materials or services from a video tape service provider,” originally passed to protect consumers' viewing habits when renting at Blockbuster and other video rental stores. “Uh oh” said every other streaming platform.   
.
22. **Nvidia** passed Amazon and Alphabet to become the third most valuable company in the U.S., after seeing its share price grow 221% over the past year due to the surge in demand for its GPUs. The company's market cap is now $1.81T compared to Alphabet's $1.78T and Amazon's $1.76T, trailing just Apple and Microsoft in market cap.   
.
23. **Jitterbit**, a cloud-based integration platform that helps e-commerce and SaaS companies automate through pre-built no-code workflows, named Bill Conner as its new President, CEO, and board member, effective immediately. Conner has previously led multiple cybersecurity, SaaS, data networks, and other tech companies including Entrust and SonicWall.   
.
24. Last week I reported that **Shopify** was upping the price of its Shopify Plus subscription from $2,000/month to $2,500/month for merchants on a one-year term, and now some are saying that the price hike could have negative effects on app developers and startups building in its ecosystem. If merchants decide to stick with Shopify after the price hike, it means they will likely look to trim costs elsewhere in their stack stack, which could mean churn in the Shopify app ecosystem.   
.
25. **eBay** publicly announced that its Monthly Community Chat for sellers has been discontinued as the company moves to occasional, not live chats centered around specific updates or events. Moving forward, eBay will host threads in the Buying & Selling forums, and users will have one week to submit their questions. (Remember eBay, the tighter you grip seller conversation, the more you'll lose control of it.)   
.
26. In other **eBay** news, the company told a federal judge last week that it does not sell the items on its platform and therefore the court should dismiss the lawsuit filed against it by the EPA in September over the alleged sale of unlawful pesticides and high-emission car parts. The government argued that eBay is the seller of illegal goods that violate the Clean Air Act and other laws, however, eBay points to Section 230 to say that it shouldn't be responsible for listings published by third parties on its platform.   
.
27. **Online revenue** fell 7% YoY in January against a decline of 3.5% last year, according to the IMRG Online Retail Index. Since the high growth rates seen during the pandemic, e-commerce revenue declined 10% YoY in 2022 and 3% YoY in 2023, with IMRG forecasting 0% YoY growth in 2024.   
.
28. **Misfits Market** started offering e-commerce fulfillment services to companies that sell perishable products directly to consumers. The online grocer is leveraging its five temperature-controlled warehouses and in-house delivery network to provide the new service, known as Fulfilled by Misfits.   
.
29. Some layoffs to report this week… **Nike** is laying off more than 1,600 employees, or about 2% of its workforce, following its December announcement of a cost-savings plan. **Instacart** is laying off 250 workers as part of a restructuring program it says will help “flatten the organization” to focus on core initiatives. **Cisco** is laying off 4,250 employees globally, which represents about 5% of its workforce. And **Mozilla** is laying off around 60 employees as it scales back its investment in products like its VPN, Relay, and Online Footprint Scrubber.   
.
30. **40% of Gen Z consumers** surveyed said they are tired of hearing about Amazon and are actively trying to shop less on the marketplace, with 60% believing Amazon is too powerful. One-fifth of adults said they would drop their Amazon Prime membership if another brand offered equivalent benefits.   
.
31. Speaking of **Amazon**, the company said that it surpassed $8B in cumulative exports from India last year, up from $5B in 2022, and is set to achieve its ambitious target of $20B in exports by 2025. The news comes shortly after Walmart said that it surpassed $30B in sourcing from India in over two decades and is now targeting $10B annually by 2027.   
.
32. **Wix** is launching Wix Capital Accelerator Program, designed to recruit Israel-based pre-seed and seed-stage companies that are creating disruptive business platforms, tools, and management solutions for SMBs. “Hello, uh, we're building a Shopify app…”  
.
33. Plus 8 seed rounds, IPOs, and acquisitions of interest including **Bob W**, a Finland-based marketplace for premium short-term rentals in Europe, raising €40M in a Series B round.  
.

I hope you found this recap helpful. See you next week!  
  
PAUL  
Editor of Shopifreaks E-Commerce Newsletter  
  
PS: If I missed any big news this week, please share in the comments.

&#x200B;",/r/ecommerce/comments/1aux49x/ecommerce_industry_news_recap_week_of_feb_19th/,ecommerce,r/ecommerce,False,relevant,highly relevant
GPT-4 Week 4. The rise of Agents and the beginning of the Simulation era,lostlifon,https://www.reddit.com/r/ChatGPT/comments/12o29gl/gpt4_week_4_the_rise_of_agents_and_the_beginning/,3945,428,1681636411.0,12o29gl,"Another big week. Delayed a day because I've been dealing with a terrible flu

&#x200B;

* Cognosys - a web based version of AutoGPT/babyAGI. Looks so cool \[[Link](https://www.cognosys.ai/)\]
* Godmode is another web based autogpt. Very fun to play with this stuff \[[Link](https://godmode.space/)\]
* HyperWriteAI is releasing an AI agent that can basically use the internet like a human. In the example it orders a pizza from dominos with a single command. This is how agents will run the internet in the future, or maybe the present? Announcement tweet \[[Link](https://twitter.com/mattshumer_/status/1646234077798727686?s=20)\]. Apply for early access here \[[Link](https://app.hyperwriteai.com/earlyAccess)\]
* People are already playing around with adding AI bots in games. A preview of whats to come \[[Link](https://twitter.com/DeveloperHarris/status/1647134796886441985)\]
* Arxiv being transformed into a podcast \[[Link](https://twitter.com/yacineMTB/status/1646591643989037056?s=20)\]
* AR + AI is going to change the way we live, for better or worse. lifeOS runs a personal AI agent through AR glasses \[[Link](https://twitter.com/bryanhpchiang/status/1645501260827885568)\]
* AgentGPT takes autogpt and lets you use it in the browser \[[Link](https://agentgpt.reworkd.ai/)\]
* MemoryGPT - ChatGPT with long term memory. Remembers past convos and uses context to personalise future ones \[[Link](https://twitter.com/rikvk01/status/1645847481601720321)\]
* Wonder Studios have been rolling out access to their AI vfx platform. Lots of really cool examples I’ll link here \[[Link](https://twitter.com/WonderDynamics/status/1644376317595615233)\] \[[Link](https://twitter.com/nickfloats/status/1645113516808892418)\] \[[Link](https://twitter.com/DonAllenIII/status/1644053830118813697)\] \[[Link](https://twitter.com/ABAOProductions/status/1645435470145376259)\] \[[Link](https://twitter.com/ABAOProductions/status/1645451762134859776)\] \[[Link](https://twitter.com/ActionMovieKid/status/1644776744614785027)\] \[[Link](https://twitter.com/rpnickson/status/1644669313909964804)\] \[[Link](https://twitter.com/eLPenry/status/1643931490290483201)\]
* Vicuna is an open source chatbot trained by fine tuning LLaMA. It apparently achieves more than 90% quality of chatgpt and costs $300 to train \[[Link](https://vicuna.lmsys.org/)\]
* What if AI agents could write their own code? Describe a plugin and get working Langchain code \[[Link](https://twitter.com/NicolaeRusan/status/1644120508173262853)\]. Plus its open source \[[Link](https://github.com/hey-pal/toolkit-ai)\]
* Yeagar ai - Langchain Agent creator designed to help you build, prototype, and deploy AI-powered agents with ease \[[Link](https://github.com/yeagerai/yeagerai-agent)\]
* Dolly - The first “commercially viable”, open source, instruction following LLM \[[Link](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\]. You can try it here \[[Link](https://huggingface.co/spaces/RamAnanth1/Dolly-v2)\]
* A thread on how at least 50% of iOs and macOS chatgpt apps are leaking their private OpenAI api keys \[[Link](https://twitter.com/cyrilzakka/status/1646532570597982208?s=20)\]
* A gradio web UI for running LLMs like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA. Open source and free \[[Link](https://github.com/oobabooga/text-generation-webui)\]
* The Do Anything Machine assigns an Ai agent to tasks in your to do list \[[Link](https://twitter.com/thegarrettscott/status/1645918390413066240)\]
* Plask AI for image generation looks pretty cool \[[Link](https://twitter.com/plask_ai/status/1643632016389226498?s=20)\]
* Someone created a chatbot that has emotions about what you say and you can see how you make it feel. Honestly feels kinda weird ngl \[[Link](https://www.meetsamantha.ai/)\]
* Use your own AI models on the web \[[Link](https://twitter.com/mathemagic1an/status/1645478246912229412)\]
* A babyagi chatgpt plugin lets you run agents in chatgpt \[[Link](https://twitter.com/skirano/status/1646582731629887503)\]
* A thread showcasing plugins hackathon (i think in sf?). Some of the stuff is pretty in here is really cool. Like attaching a phone to a robodog and using SAM and plugins to segment footage and do things. Could be used to assist people with impairments and such. makes me wish I was in sf 😭 \[[Link](https://twitter.com/swyx/status/1644798043722764288)\] robot dog video \[[Link](https://twitter.com/swyx/status/1645237585885933568)\]
* Someone created KarenAI to fight for you and negotiate your bills and other stuff \[[Link](https://twitter.com/imnotfady/status/1646286464534159360?s=20)\]
* You can install GPT4All natively on your computer \[[Link](https://twitter.com/BrianRoemmele/status/1646714552602460160?s=20)\]
* WebLLM - open source chat bot that brings LLMs into web browsers \[[Link](https://mlc.ai/web-llm/)\]
* AI Steve Jobs meets AI Elon Musk having a full on unscripted convo. Crazy stuff \[[Link](https://twitter.com/forever_voices/status/1644607758107279361)\]
* AutoGPT built a website using react and tailwind \[[Link](https://twitter.com/SullyOmarr/status/1644160222733406214)\]
* A chatbot to help you learn Langchain JS docs \[[Link](https://www.supportguy.co/chatbot/UMFDPPIGugxNPhSXj1KR)\]
* An interesting thread on using AI for journaling \[[Link](https://twitter.com/RunGreatClasses/status/1645111641602682881)\]
* Build a Chatgpt powered app using Bubble \[[Link](https://twitter.com/vince_nocode/status/1645112081069359104)\]
* Build a personal, voice-powered assistant through Telegram. Source code provided \[[Link](https://twitter.com/rafalwilinski/status/1645123663514009601)\]
* This thread explains the different ways to overcome the 4096 token limit using chains \[[Link](https://twitter.com/wooing0306/status/1645092115914063872)\]
* This lads creating an open source rebuild of descript, a video editing tool \[[Link](https://twitter.com/michaelaubry/status/1646005905371299840?s=20)\]
* DesignerGPT - plugin to create websites in ChatGPT \[[Link](https://twitter.com/skirano/status/1645555893902397440)\]
* Get the latest news using AI \[[Link](https://twitter.com/clusteredbytes/status/1645033582144913409)\]
* Have you seen those ridiculous balenciaga videos? This thread explain how to make them \[[Link](https://twitter.com/ammaar/status/1645146599772020738)\]
* GPT-4 plugin to generate images and then edit them \[[Link](https://twitter.com/skirano/status/1645162581424844804)\]
* How to animate yourself \[[Link](https://twitter.com/emmabrokefree/status/1644848135141982208)\]
* Baby-agi running on streamlit \[[Link](https://twitter.com/dory111111/status/1645043491066740736)\]
* How to make a Space Invaders game with GPT-4 and your own A.I. generated textures \[[Link](https://twitter.com/icreatelife/status/1644934708084502529)\]
* AI live coding a calculator app \[[Link](https://twitter.com/SullyOmarr/status/1645087016823173124)\]
* Someone is building Apollo - a chatgpt powered app you can talk to all day long to learn from \[[Link](https://twitter.com/localghost/status/1646243856336420870?s=20)\]
* Animals use reinforcement learning as well \[[Link](https://twitter.com/BrianRoemmele/status/1645069408883314693)\]
* How to make an AI aging video \[[Link](https://twitter.com/icreatelife/status/1645115713479225345)\]
* Stable Diffusion + SAM. Segment something then generate a stable diffusion replacement. Really cool stuff \[[Link](https://twitter.com/1littlecoder/status/1645118363562135553)\]
* Someone created an AI agent to do sales. Just wait till this is integrated with Hubspot or Zapier \[[Link](https://twitter.com/ompemi/status/1645083062986846209)\]
* Someone created an AI agent that follows Test Driven Development. You write the tests and the agent then implements the feature. Very cool \[[Link](https://twitter.com/adamcohenhillel/status/1644836492294905856)\]
* A locally hosted 4gb model can code a 40 year old computer language \[[Link](https://twitter.com/BrianRoemmele/status/1644906247311986689)\]
* People are adding AI bots to discord communities \[[Link](https://twitter.com/davecraige/status/1643514607150194688)\]
* Using AI to delete your data online \[[Link](https://twitter.com/jbrowder1/status/1644814314908565504)\]
* Ask questions over your files with simple shell commands \[[Link](https://twitter.com/jerryjliu0/status/1644728855704518657)\]
* Create 3D animations using AI in Spline. This actually looks so cool \[[Link](https://spline.design/ai)\]
* Someone created a virtual AI robot companion \[[Link](https://twitter.com/zoan37/status/1644679778316742657)\]
* Someone got gpt4all running on a calculator. gg exams \[[Link](https://twitter.com/BrianRoemmele/status/1644321318001868801)\] Someone also got it running on a Nintendo DS?? \[[Link](https://twitter.com/andriy_mulyar/status/1644408478834860034)\]
* Flair AI is a pretty cool tool for marketing \[[Link](https://twitter.com/mickeyxfriedman/status/1644038459613650944)\]
* A lot of people have been using Chatgpt for therapy. I wrote about this in my last newsletter, it’ll be very interesting to see how this changes therapy as a whole. An example of someone whos been using chatgpt for therapy \[[Link](https://twitter.com/Kat__Woods/status/1644021980948201473)\]
* A lot of people ask how can I use gpt4 to make money or generate ideas. Here’s how you get started \[[Link](https://twitter.com/emollick/status/1644532127793311744)\]
* This lad got an agent to do market research and it wrote a report on its findings. A very basic example of how agents are going to be used. They will be massive in the future \[[Link](https://twitter.com/SullyOmarr/status/1645205292756418562)\]
* Someone made a plugin that gives access to the shell. Connect this to an agent and who knows wtf could happen \[[Link](https://twitter.com/colinfortuner/status/1644532707249012736)\]
* Someone made an app that connects chatgpt to google search. Pretty neat \[[Link](https://heygpt.chat/)\]
* Somebody made a AI which generates memes just by taking a image as a input \[[Link](https://www.memecam.io/)\]
* This lad made a text to video plugin \[[Link](https://twitter.com/chillzaza_/status/1644031140779421696)\]
* Why only talk to one bot? GroupChatGPT lets you talk to multiple characters in one convo \[[Link](https://twitter.com/richardfreling/status/1646179656775925767?s=20)\]
* Build designs instantly with AI \[[Link](https://twitter.com/Steve8708/status/1643050860396834816)\]
* Someone transformed someone dancing to animation using stable diffusion and its probably the cleanest animation I’ve seen \[[Link](https://www.reddit.com/r/StableDiffusion/comments/12i9qr7/i_transform_real_person_dancing_to_animation/)\]
* Create, deploy, and iterate code all through natural language. Man built a game with a single prompt \[[Link](https://twitter.com/dylanobu/status/1645308940878749697)\]
* Character cards for AI roleplaying \[[Link](https://twitter.com/Teknium1/status/1645147324480630784)\]
* IMDB-LLM - query movie titles and find similar movies in plain english \[[Link](https://github.com/ibiscp/LLM-IMDB)\]
* Summarize any webpage, ask contextual questions, and get the answers without ever leaving or reading the page \[[Link](https://www.browsegpt.one/)\]
* Kaiber lets you restyle music videos using AI \[[Link](https://twitter.com/icreatelife/status/1645270393291194368)\]. They also have a vid2vid tool \[[Link](https://twitter.com/TomLikesRobots/status/1645502724404903943)\]
* Create query boxes with text descriptions of any object in a photo, then SAM will segment anything in the boxes \[[Link](https://huggingface.co/spaces/ngthanhtinqn/Segment_Anything_With_OWL-ViT)\]
* People are giving agents access to their terminals and letting them browse the web \[[Link](https://twitter.com/lobotomyrobot/status/1645209135728979969)\]
* Go from text to image to 3d mesh to video to animation \[[Link](https://twitter.com/icreatelife/status/1645236879892045826)\]
* Use SAM with spatial data \[[Link](https://github.com/aliaksandr960/segment-anything-eo)\]
* Someone asked autogpt to stalk them on the internet.. \[[Link](https://twitter.com/jimclydego/status/1646139413150433281?s=20)\]
* Use SAM in the browser \[[Link](https://twitter.com/visheratin/status/1645811764460761089)\]
* robot dentitsts anyone?? \[[Link](https://twitter.com/HowThingsWork_/status/1640854930561933318)\]
* Access thousands of webflow components from a chrome extension using ai \[[Link](https://www.compo.ai/)\]
* AI generating designs in real time \[[Link](https://twitter.com/Steve8708/status/1645186455701196800)\]
* How to use Langchain with Supabase \[[Link](https://blog.langchain.dev/langchain-x-supabase/)\]
* Iris - chat about anything on your screen with AI \[[Link](https://twitter.com/ronithhh/status/1645649290193416193)\]
* There are lots of prompt engineering jobs being advertised now lol \[[Link](https://twitter.com/AiBreakfast/status/1645581601408172033)\]. Just search in google
* 5 latest open source LLMs \[[Link](https://twitter.com/TheTuringPost/status/1645404011300790272)\]
* Superpower ChatGPT - A chrome extension that adds folders and search to ChatGPT \[[Link](https://chrome.google.com/webstore/detail/superpower-chatgpt/amhmeenmapldpjdedekalnfifgnpfnkc)\]
* Terence Tao the best mathematician alive used gpt4 and it saved him a significant amount of tedious work \[[Link](https://mathstodon.xyz/@tao/110172426733603359)\]
* This lad created an AI coding assistant using Langchain for free in notebooks. Looks great and is open source \[[Link](https://twitter.com/pictobit/status/1646925888271835149?s=20)\]
* Someone got autogpt running on an iPhone lol \[[Link](https://twitter.com/nathanwchan/status/1646194627756830720?s=20)\]
* Run over 150,000 open-source models in your games using a new Hugging Face and Unity game engine integration. Use SD in a unity game now \[[Link](https://github.com/huggingface/unity-api)\]
* Not sure if I’ve posted here before but [nat.dev](http://nat.dev/) lets you race AI models against each other \[[Link](https://accounts.nat.dev/sign-in?redirect_url=https%3A%2F%2Fnat.dev%2F)\]
* A quick way to build LLM apps - an open source UI visual tool for Langchain \[[Link](https://github.com/FlowiseAI/Flowise)\]
* A plugin that gets your location and lets you ask questions based on where you are \[[Link](https://twitter.com/BenjaminDEKR/status/1646044007959523329?s=20)\]
* The plugin OpenAI was using to assess the security of other plugins is interesting \[[Link](https://twitter.com/rez0__/status/1645861607010979878?s=20)\]
* Breakdown of the team that built gpt4 \[[Link](https://twitter.com/EMostaque/status/1646056127883513857?s=20)\]
* This PR attempts to give autogpt access to gradio apps \[[Link](https://github.com/Significant-Gravitas/Auto-GPT/pull/1430)\]

# News

&#x200B;

* Stanford/Google researchers basically created a mini westworld. They simulated a game society with agents that were able to have memories, relationships and make reflections. When they analysed the behaviour, they measured to be ‘more human’ than actual humans. Absolutely wild shit. The architecture is so simple too. I wrote about this in my newsletter yday and man the applications and use cases for this in like gaming or VR and basically creating virtual worlds is going to be insane (nsfw use cases are scary to even think about). Someone said they cant wait to add capitalism and a sense of eventual death or finite time and.. that would be very interesting to see. Link to watching the game \[[Link](https://reverie.herokuapp.com/arXiv_Demo/#)\] Link to the paper \[[Link](https://arxiv.org/pdf/2304.03442.pdf)\]
* OpenAI released an implementation of Consistency Models. We could actually see real time image generation with these (from my understanding, correct me if im wrong). Link to github \[[Link](https://github.com/openai/consistency_models)\]. Link to paper \[[Link](https://arxiv.org/abs/2303.01469)\]
* Andrew Ng (cofounder of Google Brain) & Yann LeCun (Chief AI scientist at Meta) had a very interesting conversation about the 6 month AI pause. They both don’t agree with it. A great watch \[[Link](https://www.youtube.com/watch?v=BY9KV8uCtj4)\]. This is a good twitter thread summarising the convo \[[Link](https://twitter.com/alliekmiller/status/1644392058860208139)\]
* LAION proposes to openly create ai models like gpt4. They want to build a publicly funded supercomputer with \~100k gpus to create open source models that can rival gpt4. If you’re wondering who they are - the director of LAION is a research group leader at a centre with one of the largest high performance computing clusters in Europe. These guys are legit \[[Link](https://www.heise.de/news/Open-source-AI-LAION-proposes-to-openly-replicate-GPT-4-a-public-call-8785603.html)\]
* AI clones girls voice and demands ransom from mum. She doesnt doubt the voice for a second. This is just the beginning for this type of stuff happening. I have no idea how we’re gona solve this problem \[[Link](https://nypost.com/2023/04/12/ai-clones-teen-girls-voice-in-1m-kidnapping-scam/?utm_source=reddit.com)\]
* Stability AI, creators of stable diffusion are burning through a lot of cash. Perhaps they’ll be bought by some other company \[[Link](https://www.semafor.com/article/04/07/2023/stability-ai-is-on-shaky-ground-as-it-burns-through-cash)\]. They just released SDXL, you can try it here \[[Link](https://beta.dreamstudio.ai/generate)\] and here \[[Link](https://huggingface.co/spaces/RamAnanth1/stable-diffusion-xl)\]
* Harvey is a legalAI startup making waves in the legal scene. They’ve partnered with PWC and are backed by OpenAI’s startup fund. This thread has a good breakdown \[[Link](https://twitter.com/ai__pub/status/1644735555752853504)\]
* Langchain released their chatgpt plugin. People are gona build insane things with this. Basically you can create chains or agents that will then interact with chatgpt or other agents \[[Link](https://github.com/langchain-ai/langchain-aiplugin)\]
* Former US treasury secretary said that ChatGPT has ""a great opportunity to level a lot of playing fields"" and will shake up the white collar workforce. I actually think its very possible that AI causes the rift between rich and poor to grow even further. Guess we’ll find out soon enough \[[Link](https://twitter.com/BloombergTV/status/1644388988071886848)\]
* Perplexity AI is getting an upgrade with login, threads, better search and more \[[Link](https://twitter.com/perplexity_ai/status/1646549544094531588)\]
* A thread explaining the updated US copyright laws in AI art \[[Link](https://twitter.com/ElunaAI/status/1642332047543861249)\]
* Anthropic plans to build a model 10X more powerful than todays AI by spending over 1 billion over the next 18 months \[[Link](https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai/)\]
* Roblox is adding AI to 3D creation. A great thread breaking it down \[[Link](https://twitter.com/bilawalsidhu/status/1644817961952374784)\]
* So snapchat released their My AI and it had problems. Was saying very inappropriate things to young kids \[[Link](https://www.washingtonpost.com/technology/2023/03/14/snapchat-myai/)\]. Turns out they didn’t even implement OpenAI’s moderation tech which is free and has been there this whole time. Morons \[[Link](https://techcrunch.com/2023/04/05/snapchat-adds-new-safeguards-around-its-ai-chatbot/)\]
* A freelance writer talks about losing their biggest client to chatgpt \[[Link](https://www.reddit.com/r/freelanceWriters/comments/12ff5mw/it_happened_to_me_today/)\]
* Poe lets you create custom chatbots using prompts now \[[Link](https://techcrunch.com/2023/04/10/poes-ai-chatbot-app-now-lets-you-make-your-bots-using-prompts/)\]
* Stack Overflow traffic has reportedly dropped 13% on average since chatgpt got released \[[Link](https://twitter.com/mohadany/status/1642544573137158144)\]
* Sam Altman was at MIT and he said ""We are *not* currently training GPT-5. We're working on doing more things with GPT-4."" \[[Link](https://twitter.com/dharmesh/status/1646581646030786560)\]
* Amazon is getting in on AI, letting companies fine tune models on their own data \[[Link](https://aws.amazon.com/bedrock/)\]. They also released CodeWhisperer which is like Githubs Copilot \[[Link](https://aws.amazon.com/codewhisperer/)\]
* Google released Med-PaLM 2 to some healthcare customers \[[Link](https://cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model)\]
* Meta open sourced Animated Drawings, bringing sketches to life \[[Link](https://github.com/facebookresearch/AnimatedDrawings)\]
* Elon Musk has purchased 10k gpus after alrdy hiring 2 ex Deepmind engineers \[[Link](https://www.businessinsider.com/elon-musk-twitter-investment-generative-ai-project-2023-4)\]
* OpenAI released a bug bounty program \[[Link](https://openai.com/blog/bug-bounty-program)\]
* AI is already taking video game illustrators’ jobs in China. Two people could potentially do the work that used to be done by 10 \[[Link](https://restofworld.org/2023/ai-image-china-video-game-layoffs/)\]
* ChatGPT might be coming to windows 11 \[[Link](https://www.tomsguide.com/news/chatgpt-is-coming-directly-to-windows-but-theres-a-catch)\]
* Someone is using AI and selling nude photos online.. \[[Link](https://archive.is/XqogQ)\]
* Australian mayor is suing chatgpt for saying false info lol. aussie politicians smh \[[Link](https://thebuzz.news/article/first-defamation-suit-against-chatgpt/5344/)\]
* Donald Glover is hiring prompt engineers for his creative studios \[[Link](https://twitter.com/nonmayorpete/status/1647117008411197441?s=20)\]
* Cooling ChatGPT takes a lot of water \[[Link](https://futurism.com/the-byte/chatgpt-ai-water-consumption)\]

# Research Papers

&#x200B;

* OpenAI released a paper showcasing what gpt4 looked like before they released it and added guard rails. It would answer anything and had incredibly unhinged responses. Link to paper \[[Link](https://cdn.openai.com/papers/gpt-4-system-card.pdf)\]
* Create 3D worlds with only 2d images. Crazy stuff and you can test it on HuggingFace \[[Link](https://twitter.com/liuziwei7/status/1644701636902924290)\]
* NeRF’s are looking so real its absolutely insane. Just look at the video \[[Link](https://jonbarron.info/zipnerf/)\]
* Expressive Text-to-Image Generation. I dont even know how to describe this except like the holodeck from Star Trek? \[[Link](https://rich-text-to-image.github.io/)\]
* Deepmind released a paper on transformers. Good read if you want to understand LM’s \[[Link](https://twitter.com/AlphaSignalAI/status/1645091408951353348)\]
* Real time rendering of NeRF’s across devices. Render NeRF’s in real time which can run on AR, VR or mobile devices. Crazy \[[Link](https://arxiv.org/abs/2303.08717)\]
* What does ChatGPT return about human values? Exploring value bias in ChatGPT \[[Link](https://arxiv.org/abs/2304.03612)\]. Interestingly it suggests that text generated by chatgpt doesnt show clear signs of bias
* A new technique for recreating 3D scenes from images. The video looks crazy \[[Link](http://rgl.epfl.ch/publications/Vicini2022SDF)\]
* Big AI models will use small AI models as domain experts \[[Link](https://arxiv.org/abs/2304.04370)\]
* A great thread talking about 5 cool biomedical vision language models \[[Link](https://twitter.com/katieelink/status/1645542156533383168)\]
* Teaching LLMs to self debug \[[Link](https://arxiv.org/abs/2304.05128)\]
* Fashion image to video with SD \[[Link](https://grail.cs.washington.edu/projects/dreampose/)\]
* ChatGPT Can Convert Natural Language Instructions Into Executable Robot Actions \[[Link](https://arxiv.org/abs/2304.03893)\]
* Old but interesting paper I found on using LLMs to measure public opinion like during election times \[[Link](https://arxiv.org/abs/2303.16779)\]. Got me thinking how messed up the next US election is going to be with how easy it is going to be to spread misinformation. It’s going to be very interesting to see what happens

For one coffee a month, I'll send you 2 newsletters a week with all of the most important & interesting stories like these written in a digestible way. You can [sub here](https://nofil.beehiiv.com/upgrade)

I'm kinda sad I wrote about like 3-4 of these stories in detailed in my newsletter on thursday but most won't read it because it's part of the paid sub. I'm gona start making videos to cover all the content in a more digestible way. You can sub on youtube to see when I start posting \[[Link](https://www.youtube.com/channel/UCsLlhrCXQoGdUEzDdBPFrrQ)\]

You can read the free newsletter [here](https://nofil.beehiiv.com/?utm_source=reddit)

If you'd like to tip you can [buy me a coffee](https://www.buymeacoffee.com/nofil) or sub on [patreon](https://patreon.com/NoLongerANincompoopwithNofil?utm_medium=clipboard_copy&utm_source=copyLink&utm_campaign=creatorshare_creator&utm_content=join_link). No pressure to do so, appreciate all the comments and support 🙏

(I'm not associated with any tool or company. Written and collated entirely by me, no chatgpt used. I tried, it doesn't work with how I gather the info trust me. Also a great way for me to basically know everything thats going on)",/r/ChatGPT/comments/12o29gl/gpt4_week_4_the_rise_of_agents_and_the_beginning/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
GPT-4 Week 5. Open Source is coming + Music industry in shambles - Nofil's Weekly Breakdown,lostlifon,https://www.reddit.com/r/ChatGPT/comments/12v8oly/gpt4_week_5_open_source_is_coming_music_industry/,3355,320,1682175955.0,12v8oly,"So I thought I might as well do a lil intro since this has become a weekly thing. I'm Nofil. lifon is my name backwards, hence the username lostlifon.

Better formatting yay!

# Google + DeepMind

* Google Brain and Deepmind have combined to form Google Deepmind. This is a big deal. Expecting big things from Google. Yes we’ve all been shitting on Google recently but we have to remember, they have most of the worlds data. The amount of things they can do with it should be insane. Will be very interesting to see what they come up with \[[Link](https://www.deepmind.com/blog/announcing-google-deepmind?utm_source=twitter&utm_medium=social&utm_campaign=GDM)\] Funnily enough over the last 13 years they went from DeepMind → Google DeepMind → DeepMind → Google DeepMind
* Google announced Project Magi, an AI powered search engine with the purpose of creating a more personalised user experience. It will apparently offer options for purchases, research and will be more of a conversational bot. Other things Google is working on include AI powered Google Earth, music search chatbot, a language learning tutor and a few other things \[[Link](https://me.mashable.com/tech/27276/project-magi-googles-team-of-160-working-on-adding-new-features-to-search-engine)\]
* Google’s Bard can now write code for you, explain code, debug code and export it Colab \[[Link](https://blog.google/technology/ai/code-with-bard/)\]
* DeepMind developed an AI program that created a 3D mapping of all 200 million proteins known to science \[[Link](https://twitter.com/60Minutes/status/1647745216986710018)\]

# Bark + Whisper JAX

* Bark is an incredible text-to-audio model and can also generate in multiple languages \[[Link](https://github.com/suno-ai/bark)\]
* Whisper Jax makes transcribing audio unbelievably fast, the fastest model on the web. Transcribe 30 min of audio in \~30 secs. Link to Github \[[Link](https://github.com/sanchit-gandhi/whisper-jax)\] Link to try online on huggingface \[[Link](https://huggingface.co/spaces/sanchit-gandhi/whisper-jax)\]

&#x200B;

# Open Source

* Open Assistant - just wow - is an open source Chat AI. The entire dataset is free and open source, you can find the code and all here \[[Link](https://huggingface.co/OpenAssistant)\]. You can play around with the chat here \[[Link](https://t.co/5lcaGKfu3i)\]. For an open source model I think its brilliant. I got it to make website copy and compared it to gpt-4 and honestly there was hardly a difference in this case. Very exciting. We’re getting closer and closer to a point where we’ll have open source models as powerful as gpt3.5 & 4. Video discussing it \[[Link](https://www.youtube.com/watch?v=ddG2fM9i4Kk)\]
* Stability AI announced StableLM - their Language Models. They’ve released 3B and 7B models with 15-65B models to come. Don’t be confused - this isn’t a chat bot like ChatGPT - that will come as they release RLHF models and go from StableLM to StableChat \[[Link](https://github.com/Stability-AI/StableLM/)\]. Another great win for open source
* LlamaAcademy is an open source repo designed to teach models how to read API docs and then produce code specifically for certain API’s. This type of thing will be very important in the coming adoption of AI \[[Link](https://github.com/danielgross/LlamaAcademy)\]. Still very experimental atm
* Detailed instructions on how to run LLaMA on Macbook M1 \[[Link](https://til.simonwillison.net/llms/llama-7b-m2)\]
* LLaVA is an open source model that can also interpret images. It’s good \[[Link](https://twitter.com/ChunyuanLi/status/1648222285889953793)\]. Link to try it out \[[Link](https://llava-vl.github.io/)\]
* MiniGPT-4 - an open source model for visual tasks. It can even generate html given a picture of a design of a website, albeit basic. The fact that this is open source is awesome, can’t wait for these open source models to get even better. \[[Link](https://minigpt-4.github.io/)\] Also provide a pretrained MiniGPT-4 aligned with Vicuna-7B \[[Link](https://github.com/Vision-CAIR/MiniGPT-4)\]
* Red Pajama is a project to create open source LLMs. They’ve just released a 1.2 trillion token dataset. This is actually a very big deal but because there's no demo, just a dataset its flown under the radar. They’re alrdy training ontop of it right now. I hope this will also work for commercial use as well \[[Link](https://twitter.com/togethercompute/status/1647917989264519174)\]

&#x200B;

# Elon's TruthGPT

* Elon Musk went on Tucker Carlson and spoke about AI. He’s building his own AI called TruthGPT - a maximum truth-seeking AI that tries to understand the nature of the universe. Whatever that means. This comes only a few weeks after he called for a pause on AI advancements. Why’s he doing this? He was scared that Google/DeepMind were winning and would lead to unsafe AGI because Larry Page (co-founder of Google) called Elon a “species-ist” for being pro human because he wants AI to be safe for humanity. Page has openly stated that Google's goal is to create AGI \[[Link](https://www.youtube.com/watch?v=fm04Dvky3w8)\]

&#x200B;

# OpenAI TED Talk

* President and Co-Founder of OpenAI, Greg Brokman did a TED talk and its worth a watch. He showcases the potential for plugins in chatgpt and ends with “We all need to become literate…together I believe we can achieve the OpenAI mission of ensuring AGI benefits all of humanity”. Another interesting point is that chatgpt or plugins is essentially “a unified language interface on top of tools”. Genuinely wonder what they have access to behind the scenes \[[Link](https://www.youtube.com/watch?app=desktop&v=C_78DM8fG6E)\] \[[Link](https://twitter.com/mezaoptimizer/status/1648195392557727744)\]

# Games

* AI in Game dev - You can now connect any hugging face model in Unity. Open source API integration \[[Link](https://github.com/huggingface/unity-api)\]. This concept shows working AI in a game \[[Link](https://twitter.com/mayfer/status/1648277360599502850?s=20)\]. Video showing how to connect the api \[[Link](https://twitter.com/dylan_ebert_/status/1648759808630353921?s=20)\]
* A demo of using ChatGPT NPC’s in virtual reality \[[Link](https://www.youtube.com/watch?v=7xA5K7fRmig)\]
* Someone made a game where you guess if the image of a lady is real or AI. I got 13/17 lol \[[Link](https://caitfished.com/)\]. A good way to show someone the power of AI but also highlights just how used to were seeing fake looking pics on social media
* AI powered 3D editor, looks cool \[[Link](https://dup.ai/)\]

&#x200B;

# Music

* The music industry is about to undergo crazy change with AI songs of Drake, The Weekend and others popping up and they are getting very good \[[Link](https://twitter.com/lostlifon/status/1647887306874060800?s=20)\] \[[Link](https://twitter.com/WeirdAiGens/status/1648648898628526082)\]. Kanye, Drake singing Call Me Maybe & kpop is one of the funniest thing I’ve heard in a while lol \[[Link](https://twitter.com/brickroad7/status/1648492914383917058)\] \[[Link](https://www.youtube.com/watch?v=gLWa5xC7CIE)\] \[[Link](https://www.youtube.com/shorts/RVPh0KaC7U4)\]. Obviously music companies are fighting against this very hard. Will be very interesting how this plays out re artists essentially offering their voices as models to be bought or something like that \[[Link](https://www.musicbusinessworldwide.com/universal-music-group-responds-to-fake-drake-ai-track-streaming-platforms-have-a-fundamental-responsibility/)\]

&#x200B;

# Text-to-video

* NVIDIA released their text-to-video research and it is pretty good. Text-to-video is getting better so fast, its going to be a kind of scary when it becomes as good as photo generation now. Being able to create a realistic video of absolutely anything sounds crazy when you consider what some people will do with it \[[Link](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)\]
* Adobe released their text-to-video editing and it looks pretty cool actually. You can generate sound effects/music clips & auto generate storyboards + a lot more \[[Link](https://twitter.com/jnack/status/1648027068888920065)\]

&#x200B;

# AR + AI

* AR + AI for cooking, looks cool \[[Link](https://twitter.com/metaverseplane/status/1648911560268546048)\]
* AR + AI for 3D knowledge mapping, looks so cool. If you have a metaquestvr you can download and try it \[[Link](https://twitter.com/yiliu_shenburke/status/1645818274981072897)\]

&#x200B;

# Law

* Two comedians made an AI tom brady say funny stuff. He threatened to sue. This is going to be very common going forward \[[Link](https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/)\]
* A german magazine did an “interview” with an AI Michael Schumacher and his family is now gona sue them \[[Link](https://www.theverge.com/2023/4/20/23691415/michael-schumacher-fake-ai-generated-interview-racing-f1-lawsuit)\]
* An AI copilot for lawyers \[[Link](https://www.spellbook.legal/)\]
* A lawyer discusses how he uses ChatGPT daily, an interesting thread \[[Link](https://twitter.com/SMB_Attorney/status/1648302869517312001)\]

&#x200B;

# Finance

* Finchat is chatgpt for finance - ask questions about public companies. It provides reasoning, sources and data \[[Link](https://finchat.io/)\]

&#x200B;

# Wearable AI devices

* Humane, a company founded by some vet ex Apple folks just showed what they’re building - an AI powered projector that just sits with you and hears what you hear, sees what you see. It can translate anything you say in real time, give advice on what you can/cant eat and a whole lot more. Very interesting to see how AI wearables will look like and how they’ll change daily life in the years to come. Still a bit skeptical tbh but only time will tell \[[Link](https://www.inverse.com/tech/humane-ai-wearable-camera-sensor-projector-video-demo)\]

&#x200B;

# Other News + Tools

* A graph dialogue with LLMs will become the norm in the future. A great way to ideate and visualise thought processes \[[Link](https://creativity.ucsd.edu/ai)\]. Work is being done to make these open source and available to the public
* Replit have an interesting article on how they train LLMs. They also plan to open source some of their models \[[Link](https://blog.replit.com/llm-training)\]
* If you’re wondering how search might look with chatgpt, Multi-ON is a browser plugin that showcases what it will look like \[[Link](https://www.youtube.com/watch?v=2X1tIvrf68s)\]. It even manages its own twitter acc \[[Link](https://twitter.com/DivGarg9/status/1648724891884220416)\]
* A web ui of autogpt on huggingface \[[Link](https://huggingface.co/spaces/aliabid94/AutoGPT)\]
* Brex becomes one of the first companies to actually use AI as part of their brand work. They used image tools like ControlNet to create brand images for different countries \[[Link](https://twitter.com/skirano/status/1648834264396443654)\]
* An AI playground similar to [nat.dev](http://nat.dev/) by Vercel. Use this to compare different models and their outputs \[[Link](https://play.vercel.ai/r/mWjP5Dt)\]
* Someone connected ChatGPT to their personal health data and can have convos about their health. This will be massive in the future. Genuinely surprised I haven’t seen a company raise 50M+ VC money to transform digital health with AI yet. The code is also open source \[[Link](https://twitter.com/varunshenoy_/status/1648374949537775616)\]
* Mckay is releasing tutorials on how to get started coding with AI. For anyone wanting to learn, this is free and a good starting point - a simple Q&A bot in 21 lines of code. Link to youtube video \[[Link](https://www.youtube.com/watch?v=JI2rmCII4fg)\]. Link to Replit \[[Link](https://replit.com/@MckayWrigley/Takeoff-School-Your-1st-AI-App?v=1)\]. If you don’t know what replit is, become familiar with it, its good
* Reddit will begin charging companies for scraping their data to train LLMs \[[Link](https://www.marketwatch.com/story/reddit-founder-wants-to-charge-big-tech-for-scraped-data-used-to-train-ais-report-6f407265)\]. Same with Stack Overflow \[[Link](https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/)\]
* Microsoft has been working on an AI chip since 2019 code named Athena. It’s designed to train LLMs like chatgpt \[[Link](https://www.theverge.com/2023/4/18/23687912/microsoft-athena-ai-chips-nvidia)\]
* Seems like the ability to perform complex reasoning in LLMs is likely to be from training on code. Unfortunately open models like LLaMA are trained on very little code. Link to article \[[Link](https://www.notion.so/b9a57ac0fcf74f30a1ab9e3e36fa1dc1)\]
* Chegg is integrating AI to create CheggMate, a personalised study assistant for students that knows what you’re good at from conversations and provide instant help \[[Link](https://www.chegg.com/cheggmate)\]
* Scale AI released an AI readiness report. Some industries plan on increasing their AI budget by over 80%, most interested include Insurance, Logistics & supply chain, healthcare, finance, retail to work on things like claims processing, fraud detection, risk assesment, ops etc. \[[Link](https://scale.com/ai-readiness-report)\]
* An interesting thread on AI and Autism \[[Link](https://twitter.com/LeverhulmeCFI/status/1647879217495826434)\]
* ChatGPT talking about the NBA Playoffs \[[Link](https://twitter.com/NBAonTNT/status/1647710159236665344)\]
* Atlassian announces AI implementation with Atlassian Intelligence \[[Link](https://www.atlassian.com/blog/announcements/unleashing-power-of-ai)\]
* BerkeleyQuest - an AI powered search engine to help browse 6000+ courses at UC Berkeley \[[Link](https://berkeley.streamlit.app/)\]
* Grammarly is introducing AI writing tools \[[Link](https://www.grammarly.com/grammarlygo)\]
* NexusGPT - a marketplace for AI agents. Something I didn’t even consider before but seems like an interesting idea. Can see something like this becoming a big deal in the future \[[Link](https://twitter.com/achammah1/status/1649482899253501958)\]
* Forefront is a better way to use ChatGPT with image generation, custom personas, shareable chats and if you sign up now you get free access to GPT-4 \[[Link](https://twitter.com/ForefrontAI/status/1649429139907137540)\]
* Someone got Snapchat AI to show some of the instructions it has \[[Link](https://twitter.com/angelwingdel/status/1648910367332900866)\]
* Webflow is introducing AI \[[Link](https://webflow.com/blog/power-of-ai)\]

I haven't done anything the past week coz the flu had me in prison. Still have a terrible cough but whatever, newsletters back next week

For one coffee a month, I'll send you 2 newsletters a week with all of the most important & interesting stories like these written in a digestible way. You can [sub here](https://nofil.beehiiv.com/upgrade)

I'm gona start making videos explaining things like research papers and advancements on youtube, You can sub to see when I start posting \[[Link](https://www.youtube.com/channel/UCsLlhrCXQoGdUEzDdBPFrrQ)\]

You can read the free newsletter [here](https://nofil.beehiiv.com/?utm_source=reddit)

If you'd like to tip you can [buy me a coffee](https://www.buymeacoffee.com/nofil) or sub on [patreon](https://patreon.com/NoLongerANincompoopwithNofil?utm_medium=clipboard_copy&utm_source=copyLink&utm_campaign=creatorshare_creator&utm_content=join_link). No pressure to do so, appreciate all the comments and support 🙏

(I'm not associated with any tool or company. Written and collated entirely by me, no chatgpt used)",/r/ChatGPT/comments/12v8oly/gpt4_week_5_open_source_is_coming_music_industry/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
NVDA is Worth $1000+ This Year - AI Will Be The Largest Wealth Transfer In The History of The World - Sam Altman Wasn't Joking...,Xtianus21,https://www.reddit.com/r/wallstreetbets/comments/1aqf0zb/nvda_is_worth_1000_this_year_ai_will_be_the/,2340,955,1707887449.0,1aqf0zb,"**UPDATE2: Open AI Release Massive Update SORA Text/Speech to Video**  
[https://www.theverge.com/2024/2/15/24074151/openai-sora-text-to-video-ai](https://www.theverge.com/2024/2/15/24074151/openai-sora-text-to-video-ai)

[https://www.youtube.com/watch?v=nEuEMwU45Hs](https://www.youtube.com/watch?v=nEuEMwU45Hs)

**UPDATE: Sam Altman Tells the World (literally** [The World Governments Summit](https://youtu.be/JVatgo0TJIw?si=3-Jkw9F3ZlkilulX&t=61)**) that** [GPT-5 Is Going To Be a Big Deal](https://youtu.be/-Mca6eN81Is?si=Ym59zLD2VZ2yefZU) **- GPT-5 Will Be Smarter Across The Board - Serious AGI in 5 - 10 Years.**

**THIS IS WAR - And Nvidia is the United States Military Industrial Complex, The Mongol Empire, and Roma combined.**

AI will be as large as the internet and then it will surpass it. AI is the internet plus the ability to reason and analyze anything you give it in fractions of a second. A new unequivocal boomstick to whomever wants to use it.

The true winners will be those startups in fields such as robotics, healthcare, pharmaceuticals, space-aeronautics, aviation, protein synthesis, new materials and so, so much more who will use AI in new and exciting ways.

Boston dynamics, set to boom. Self-driving robotaxis, set to boom. Flying taxis, set to boom. Job replacement/automation for legacy industry jobs white collar, set to boom. Personal AI agents for your individual workloads, booming. Healthcare change as we know it (doctors won't like this but too bad), set to boom.

The amount of industry that is set to shift and mutate and emerge from AI in the next 3 - 5 years will be astonishing.

I can tell you, standing on principal, that OpenAI's next release will be so game changing that nobody will deny where AI is heading. There is not a rock you can hide under to be so oblivious as to not see where this is going.

The reason why I bring up the next iteration of ChatGPT, GPT5, is because they are initiators of this phenomenon. Other, such as Google (and others) are furiously trying to catch up but as of today the 'MOAT' may be upon us.

The reason to believe that one may catch up (or try like hell to) is from the amount of compute power from GPU's it takes to train an ungodly amount of data. Trillions of data points. Billions (soon to be Trillions) of parameters all simulating that of the synaptic neuron connections in which the human brain functions that in turn gives us the spark of life and consciousness. Make no mistake, these guys are living out a present day Manhattan project.

These people are trying to build consciousness agency with the all the world's information as a reference document at it's finger tips. Today.

And guess what. The only way these guys can build that thing - That AGI/ASI/GAI reality - Is through Nvidia.

These guys believe and have tested that if you throw MORE compute at the problem it actually GAINS function. More compute equals more consciousness. That's what these people believe and they're attempting it.

Here, let me show you what I mean. What the graph below shows is that over time the amount of data and parameters that are being used to train an AI model. I implore you to watch this video as it is a great easy to understand educational video into what the hell is going on with all of this AI stuff. It's a little technical but very informative and there are varying opinions. I pulled out the very best part in relation to Nvidia here. [AI: Grappling with a New Kind of Intelligence](https://youtu.be/EGDG3hgPNp8?si=L71CUbmqho3jhKEh&t=3768)

https://preview.redd.it/vk5twc041hic1.png?width=1318&format=png&auto=webp&s=a3d2bdf42cf25e1c04e940b2f61d0219871f2334

It's SO RIDICULOUS that you wouldn't be able to continue to see the beginning so they have to use a log plot chart. And as you see we are heading into Trillions of parameters. For reference GPT-4 was trained on roughly 200 billion parameters.

https://preview.redd.it/6cq17n5i1hic1.png?width=1327&format=png&auto=webp&s=830334aa7cdaddfa10d7f6512d10025f136076ed

It is estimated GPT-5 will be trained with 2-5 trillion parameters.

Sam Altman was dead ass serious when he is inquiring about obtaining $7 trillion for chip development. They believe that with enough compute they can create GOD.

So what's the response from Google, Meta and others. Well, they're forming ""AI """"Alliances"""""". Along with that they are going to and buying from the largest AI arms dealer on earth; Nvidia.

Nvidia is a common day AI Industrial Complex War machine.

**Sovereign AI with AI Foundries**

It's not just companies that are looking to compete it's also entire Nation States. Remember, when Italy banned GPT. Well, it turns out, countries don't want the United States building and implementing their AI into other country's culture and way of life.

So as of today, you have a battle of not just corporate America but entire countries looking to buy the bullets, tanks and missiles needed for this AI fight. Nvidia sells the absolute best bullets, the best guns, the best ammo one needs to attempt to create their own AI epicenters.

And it's so important that it is a national security risk to not just us the United States but to be a nation and not have the capability of AI.

Remember the leak about Q\* and a certain encryption being undone. You don't think heads of State where listening to that. Whether it was true or not it is now an imperative that you get with AI or get left behind. That goes just as much for a nation as it does for you as an individual.

When asked about the risk of losing out sales to China on Nvidia's last earnings call Jensen Huang clearly stated he was not worried about it because literally nations are coming online to build AI foundries.

**Nvidia's Numbers and The Power Of Compounding**

The power of compounding and why I think there share price is where it is today and has so much more room to grow. Let me ask you a question but first let me say that AWS's annual revenues are at \~$80/Y Billion. How long do you think with Nvidia's revenues of \~$18/Q Billion to reach or eclipse AWS at a 250% growth rate?

15 years? 10 Years? 5 years? Answer: 1.19 years. Ok let's not be ridiculous perhaps it's 200% instead.

5 years? Nope. 1.35 years.

Let's say they have a bad quarter and Italy doesn't pay up. 150%

5 years right? Nope. 1.62 years.

Come on they can't keep this up. 100%.

has to be 5 years this time. Nope. 2.15 years.

100% growth/2.15 years to 250% growth/1.19 years to reach 80 billion in annual revenues.

They're growth last year was 281%.

So wait, I wasn't being fair. I used $80 billion for AWS while their revenues last year where $88 Billion and Nvidia's last years 4 quarters where \~$33 Billion.

Here are those growth numbers it would take Nvidia to reach $88 billion.

At 279% =  0.73 years

At 250% = 0.78 years

At 200% = 0.89 years

at 100% = 1.41 years

Folks. That's JUST the data center. They are poised to surpass AWS, Azure and Google Cloud in about .73 to 1.5 years. Yes, you heard that right, your daddy's cloud company is about to be overtaken by your son's gaming GPU company.

When people say Nvidia is undervalued. This is what they are talking about. This is a P/S story not a P/E story.

[https:\/\/ycharts.com\/indicators\/nvidia\_corp\_nvda\_data\_center\_revenue\_quarterly](https://preview.redd.it/xa46amh0dhic1.png?width=1450&format=png&auto=webp&s=6e9116e55235991b1113713b5b7cd7d30c58d515)

This isn't a stonk price. This is just Nvidia executing ferociously.

|**Date**|**Value**|
|:-|:-|
|October 29, 2023|14.51B|
|July 31, 2023|10.32B|
|April 30, 2023|4.284B|
|January 29, 2023|3.616B|

This isn't Y2k and the AI ""dot-com"" bubble. **This is a reckoning.** This is the largest transfer of wealth the world has ever seen.

Look at the graph. Look at the growth. That's all before the next iteration of GPT-5 has even been announced.

I will tell you personally. The things that will be built with GPT-5 will truly be mind blowing. That Jetson cartoon some of you may have watched as a kid will finally be a reality coming to you soon in 2024/2025/2026.

The foundation of work being laid now is only the beginning. There will be winners and there will be loser but as of today:

**$NVDA** is fucking **KING**

For those of you who still just don't believe or are thinking this has to end sometimes. Or fucking Cramer who keeps saying be careful and take some money out and on and on. Think about this.

It costs you to just open an enterprise Nvidia data center account \~$50k via a ""limited time offer""

>DATA CENTER NEWS. Subscribe. Get the Latest from NVIDIA on Data Center. LIMITED *TIME OFFER: $49,900 ON NVIDIA DGX STATION*. For a limited time only, purchase a ...

To train a model a major LLM could cost millions who knows maybe for the largest model runs BILLIONS.

Everyone is using them from Nation States to AWS, Microsoft, Meta, Google, X. Everybody is using them.

I get it. The price of the stock being so high and the valuation makes you pause. The price is purely psychological especially when they are hitting so many data points regarding revenues. The stock will split and rightly so (perhaps next year) but make not mistake this company is firing on ALL cylinders. The are executing S Tier. Fucking Max 9000 MX9+ Tier. Some god level tier ok.

There will be shit money that hits this quarter with all the puts and calls. The stock may rescind this quarter who knows. All i'm saying is you have the opportunity to buy into one of the most prolific tech companies the world has ever known. You may not think of them as the Apples or the Amazons or the Microsoft's or the Google's and that's ok. Just know that they are 1000% percent legit and AI has just gotten started.

Position: 33% of my portfolio. Another 33% in$Arm. Why? Because What trains on Nvidia will ultimately run/inference on ARM. And 33% Microsoft (OAI light).

If OpenAI came out today public I would have %50 of my portfolio in OAI i'll tell you that.

This is something you should have and should own in your portfolio. It's up to you to decide how much. When you can pay your children's college. When you can finally get that downpayment on that dream house. When you can buy that dream car you've always wanted. Feel free to drop a thank you.

TLDR; BUY NVIDIA, SMCI and ARM. This is not financial advice. The contents of this advertisement where paid by the following... ARM (;)",/r/wallstreetbets/comments/1aqf0zb/nvda_is_worth_1000_this_year_ai_will_be_the/,wallstreetbets,r/wallstreetbets,False,relevant,highly relevant
GPT-4 Week 6. The first AI Political Ad + Palantir's Military AI could be a new frontier for warfare - Nofil's Weekly Breakdown,lostlifon,https://www.reddit.com/r/ChatGPT/comments/1323qlg/gpt4_week_6_the_first_ai_political_ad_palantirs/,2261,325,1682702865.0,1323qlg,"Honestly I don't understand how things aren't slowing down. 6 weeks straight and there's about 100 more things I could add to this. These are unprecedented times

Link to newsletter at the bottom + call for help in the comments (want to partner with someone to make video content about AI)

Enjoy

# AI x Military

* Palantir announced their AI platform for military use. There’s too much to even put here but it’s legit terrifying. The AI can assess a situation and come up with action plans by accessing info from satellites, drones and other data sources within the org. Getting serious MGS vibes reading this \[[Link](https://www.palantir.com/platforms/aip/)\]. I’ll be talking about this in depth in my newsletter. Link to youtube video \[[Link](https://www.youtube.com/watch?v=XEM5qz__HOU&t=1s)\]

&#x200B;

# AI Safety

* Dr Paul Christiano was a researcher at OpenAI on their safety team from 2017-2021 and helped create the RLHF technique (RLHF is the reason ChatGPT sounds so human). He did an interview on AI alignment and its fascinating, well worth a watch. Quote from the very beginning of the video “The most likely way we die involves not AI comes out of the blue and kills everyone, but involves we have deployed a lot of AI everywhere.. and if for some reason all the AI systems would try and kill us, they would definitely kill us”. Another quote for the heck of it “10-20% chance AI takeover and most humans die… 50% chance of doom once AI systems are human-level intelligent”. Will explore this more in upcoming newsletters coz its crazy. Link to interview \[[Link](https://www.youtube.com/watch?v=GyFkWb903aU)\]

&#x200B;

# Boston Dynamics + ChatGPT

* Boston Dynamics put ChatGPT in a robot. Atm it can do things like identify changes in the environment, read gauges, detect thermal anomalies \[[Link](https://twitter.com/svpino/status/1650832349008125952)\]. Not sure I’m looking forward to this one lol

# Music

* Grimes is the first artist to offer splitting royalties with AI generated songs \[[Link](https://twitter.com/Grimezsz/status/1650304051718791170)\]. Shes working on a program that can simulate her voice \[[Link](https://twitter.com/Grimezsz/status/1650325296850018306)\]. Think its inevitable musicians do something similar to “own” their voices and have some control of how they’re used
* AI model analyses music and creates realistic dances that actual dancers can perform. Scroll down and take a look at the moves, very curious to hear what actual dancers think of this \[[Link](https://edge-dance.github.io/)\]
* People are already making sites to create AI music \[[Link](https://create.musicfy.lol/)\]. One site already got taken down by UMG lol \[[Link](https://twitter.com/gd3kr/status/1651590854312861698?s=20)\]
* A website to find AI hits \[[Link](https://aihits.co/)\]

&#x200B;

# Open Source

Hugging Face is a website that hosts models for AI & ML and allows for open source contributions. The website hosts a tonne of models.

* Hugging Face released an open source chat model called Hugging Chat \[[Link](https://huggingface.co/chat/)\]. It’s very possible we see HuggingChat Apps - apps that use a number of models across Hugging Face. Very well could become the Android App store of AI
* Gradio tools is an open source library that lets you combine an LLM agent with any gradio app, and it integrates with Langchain. Honestly I can see so many use cases with something like this, just not enough time to build 😢 \[[Link](https://github.com/freddyaboulton/gradio-tools)\]
* GPT4Tools lets you generate images, edit them, and do normal text stuff, make code - everything in one place. Its based on Meta’s LLaMA \[[Link](https://gpt4tools.github.io/)\]
* Databerry lets you build agents trained on your own data. Link to website \[[Link](https://www.databerry.ai/)\]. Link to Github \[[Link](https://github.com/gmpetrov/databerry)\]
* Chatbot Arena - Someone made a way to compare and vote on which open source LM is the best \[[Link](https://chat.lmsys.org/?arena)\]
* gpt4free is an open source repo that lets you use gpt3&4 ***without*** an API key \[[Link](https://github.com/xtekky/gpt4free)\]. Its blown up on github for a reason
* Someone fine tuned an LLM on all their iMessages and open sourced it \[[Link](https://github.com/1rgs/MeGPT)\]

&#x200B;

# OpenAI

* You can now disable chat history and training in ChatGPT, meaning you don’t have to worry about sensitive info being leaked. ChatGPT Business is also coming in a few months \[[Link](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)\]
* OpenAI released a new branding guideline. Lots of new products are going to have to change their names \[[Link](https://openai.com/brand)\]
* You can make asynchronous calls to openai api now. 100+ in a few seconds \[[Link](https://twitter.com/gneubig/status/1649413966995619845)\]

&#x200B;

# Politics & Media

* The RNC (Republican National Committee) made a 100% AI generated Ad shitting Biden. The video basically shows a post apocaluytpic looking US. This is just the beginning. AI content is going to spread misinformation and fear mongering like crazy when the US elections come around \[[Link](https://www.youtube.com/watch?v=kLMMxgtxQ1Y)\]
* A news reporter interviewed his AI self live and was absolutely stunned. tbf the AI voice cloning was impeccable, done using forever voices \[[Link](https://twitter.com/martinmco/status/1649638460712468481)\]

&#x200B;

# Looming Regulation

* Four federal agencies released a joint statement on AI and regulating the industry. Some things in the statement suggest they have absolutely no idea what they’re talking about so it’s looking great so far. Link to pdf statement \[[Link](https://www.ftc.gov/system/files/ftc_gov/pdf/EEOC-CRT-FTC-CFPB-AI-Joint-Statement%28final%29.pdf)\]

&#x200B;

# Replit

Replit is a software company that lets you code in your browser. They do a tonne of stuff and have been at the forfront of coding x AI. What they’ve been doing is crazy and they have a team of just 85

* They announced their own code LLM. I won’t bore you with the details but its looking good + it will be open source and freely licensed meaning it can be used commercially \[[Link](https://twitter.com/Replit/status/1651344182425051136?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet)\]
* They also announced they’ve turned their IDE into a set of tools for an autonomous agent. What does this mean? Tell it to build something and watch it try and build an entire app in Replit. The future of coding folks

&#x200B;

# Research Papers

* Probably the craziest paper from the last few weeks. Researchers may have found a way to allow models to retain info up to 2 million tokens. To put into perspective just how much info that is, currently GPT-4’s biggest option is 32k tokens which is \~50 pages of documents. The entire Harry Potter series is \~1.5M tokens. Models could retain years of info, analyse gigantic amounts of data. I can imagine this will be very big for things like AI customer service, therapists etc. Will explore this further in upcoming newsletters. Link to paper \[[Link](https://arxiv.org/abs/2304.11062)}
* Record a video and then see the video from any different perspective. In the example they record a video of a person playing with their dog and then construct video from the dogs point of view \[[Link](https://andrewsonga.github.io/totalrecon/)\]
* A way for robots to create a full map and 3d scene of your home without a lot of training data \[[Link](https://pierremarza.github.io/projects/autonerf/)\]
* Researchers were able to generate images with stable diffusion on a mobile device in under 12 seconds \[[Link](https://arxiv.org/abs/2304.11267)\]
* Research shows the intro of LLMs with customer support workers led to a large increase in productivity, improved customer retention and even employee retention \[[Link](https://www.nber.org/papers/w31161)\]

&#x200B;

# Money

* PWC invests $1 billion to use AI like Azure OpenAI services \[[Link](https://www.pwc.com/us/en/about-us/newsroom/press-releases/pwc-us-makes-billion-investment-in-ai-capabilities.html)\]
* Replit raised a $97.4M Series B valuing the company at over a billion. A new unicorn emerges \[[Link](https://twitter.com/Replit/status/1650900629521596421)\]
* Harvey the AI law startup raised a 21M Series A. Honestly surprised its not bigger \[[Link](https://www.harvey.ai/blog)\]

&#x200B;

# Prompting

* Andrew Ng (co founder of Google Brain) released a course on ChatGPT prompt engineering for developers. It says it’s a beginner friendly course and only basic knowledge of python is needed \[[Link](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\]
* Microsoft released a prompt engineering technique guide \[[Link](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#specifying-the-output-structure)\]

&#x200B;

# Games

* Someone modded ChatGPT in Skyrim VR and man, AI in games is gona be so cool. NPC’s know the time of day and they can see what items you have \[[Link](https://www.youtube.com/watch?v=Gz6mAX41fs0)\]
* Someone is launching a Rust server where AI will control all the NPCs. It will remember who does what and it will have plans to achieve. Very interesting to see how this goes. Link to video announcement \[[Link](https://twitter.com/the_carlosdp/status/1649937143253352449)\]. Link to website \[[Link](https://www.whisperingfable.com/)\]

&#x200B;

# Text-to-Video

If you’re sceptical of the speed of progress, check out this tweet showing the difference between Midjourney v1 and v5. Not even a year apart and its like comparing a toddlers drawing to an artist \[[Link](https://twitter.com/nickfloats/status/1650822516972060675)\] Will the same happen with video? We might be seeing it happen right now

* If you haven’t seen it already, someone made a pizza commercial with AI and its good \[[Link](https://twitter.com/Pizza_Later/status/1650605646620794916)\]
* Someone made a whole trailer for an anime movie using text and it looks crazy. The speed at which this is progressing is genuinely staggering \[[Link](https://twitter.com/IXITimmyIXI/status/1650936620722298896)\]
* A thread showcasing some of the crazy things people are building with Gen2 \[[Link](https://twitter.com/danberridge/status/1651746835357396992)\]
* Marvel Masterchef is one of the funniest things I’ve seen recently. Hearing Thanos talk about how he prepared his dish is absolute comedy. The shit people are going to make with this stuff is gona be wild \[[Link](https://www.youtube.com/watch?v=fJVivRn35RI)\]
* Gen-1 can now be used from your iPhone \[[Link](https://apps.apple.com/app/apple-store/id1665024375?pt=119558478&ct=RunwayTwitter&mt=8)\]

&#x200B;

# Other Stuff

* The 3 founders of Siri talk about AI and their predictions for what it could look like in 10 years. A great watch, highly recommend \[[Link](https://www.youtube.com/watch?v=oY7hLWgMI28)\]
* John Schulman talks about how to build something like TruthGPT. A great watch if you want to learn in depth about Reinforcement Learning and hallucinations \[[Link](https://www.youtube.com/watch?v=hhiLw5Q_UFg)\]
* Dropbox is laying off 500 people (16% of staff) and pivoting to AI \[[Link](https://www.computerworld.com/article/3694929/dropbox-lays-off-16-of-staff-to-refocus-on-ai-as-sales-growth-slows.html)\]
* Video call your favourite celebrities with FakeTime. Actually looks so good its kinda creepy \[[Link](https://twitter.com/FakeTimeAI)\]
* TikTok launches an AI avatar creator \[[Link](https://www.theverge.com/2023/4/25/23698394/tiktok-ai-profile-picture-avatar-lensa)\]. I think its quite possible TikTok becomes a massive player in the AI space
* DevGPT - AI assistant for developers \[[Link](https://www.getdevkit.com/)\]
* Studio AI - Web design meets AI \[[Link](https://studio.design/)\]
* You can facetime an AI with near realtime ChatGPT responses. It’s pretty crazy \[[Link](https://twitter.com/frantzfries/status/1651316031762071553)\]
* Robots learned to play soccer \[[Link](https://sites.google.com/view/op3-soccer)\]
* Telling GPT-4 it was competent increased its success rate for a task from 35% to 92% lol \[[Link](https://twitter.com/kareem_carr/status/1650637744022908931)\]
* Deepfakes are getting unbelievably good \[[Link](https://twitter.com/AiBreakfast/status/1650956385260281856)\]
* David Bowie on the future of the internet. He was thinking far ahead than most at the time thats for sure \[[Link](https://twitter.com/BrianRoemmele/status/1650594068643352576)\]
* Notion slowly unveiling the next evolution of AI features \[[Link](https://twitter.com/swyx/status/1651778880645271552)\]. Seems like theyre in a great position to leverage AI since they have so much data
* Search videos using Twelve Labs \[[Link](https://twelvelabs.io/)\]
* Someone is building an open source project for building pro-social AGIs. The first one is Samantha. You can talk to samantha here \[[Link](https://www.meetsamantha.ai/)\]. Link to code \[[Link](https://github.com/Methexis-Inc/SocialAGI)\]
* Make charts instantly with AI \[[Link](https://www.chartgpt.dev/)\]
* chatgpt in every app on your phone \[[Link](https://nickdobos.gumroad.com/l/gptAndMe)\]
* Apple is working on an AI powered health coach \[[Link](https://techcrunch.com/2023/04/25/apple-is-reportedly-developing-an-ai-powered-health-coaching-service/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAApbcPzz00OFGWD-B8SyRygZgtbb1OXmfK_5rdizW_roGJpT5p4zwNkI2Mk875oGABSZ7xR3CJJEMa9i1DwZ2YkIev6KgwpP0wV3lpDbMBBZvFa0Zi5A_-M6M0J-j1o_lIr3reLFOzhMp-YkdS1apq24f0SBvV-DRXZNiGO0-K_A)\]

For one coffee a month, I'll send you 2 newsletters a week with all of the most important & interesting stories like these written in a digestible way. You can [sub here](https://nofil.beehiiv.com/upgrade)

I'm gona start making videos explaining things like research papers and advancements on youtube. Once I can put more than 5 sentences together without coughing the videos will be coming out. You can sub to see when I start posting \[[Link](https://www.youtube.com/channel/UCsLlhrCXQoGdUEzDdBPFrrQ)\]

You can read the free newsletter [here](https://nofil.beehiiv.com/?utm_source=reddit)

If you'd like to tip you can [buy me a coffee](https://www.buymeacoffee.com/nofil) or sub on [patreon](https://patreon.com/NoLongerANincompoopwithNofil?utm_medium=clipboard_copy&utm_source=copyLink&utm_campaign=creatorshare_creator&utm_content=join_link). No pressure to do so, appreciate all the comments and support 🙏

(I'm not associated with any tool or company. Written and collated entirely by me, Nofil)",/r/ChatGPT/comments/1323qlg/gpt4_week_6_the_first_ai_political_ad_palantirs/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
"This Week in AI (4/30/23): AI job losses, AI music drama continues, and the EU's AI Act, plus more.",ShotgunProxy,https://www.reddit.com/r/ChatGPT/comments/133rmw5/this_week_in_ai_43023_ai_job_losses_ai_music/,1547,152,1682862271.0,133rmw5,"Wow. The developments on the AI front keep rolling in. AI music, EU regulations, Elon Musk, and more all made major headlines this week. From about 425 saved links this week, I’ve curated and grouped the weeks’ AI developments into the biggest themes designed to help make sense of it all.

### News to Know

**Music is the next legal frontier AI will confront**

The music industry is confronting a litany of AI-related issues at light speed.

* Since the release and takedown of “Heart on My Sleeve,” which featured AI voices mimicking Drake and The Weeknd, the internet has been flooded with additional AI-made Drake songs.
* Expect each of these to test legal waters around what is fair use, and what is copyrighted
* Josh Constine, a VC at SignalFire, [puts it succinctly](https://twitter.com/JoshConstine/status/1650179054723805184): “Google is caught between an AI rock and a copyright hard place. Either the AI Drake song trained on copyrighted data is fair use, YouTube floods with this content, and labels panic Or it’s infringement, which means Google’s Bard AI is illegal.”
* Adding to the conversation: musician Grimes has proclaimed [anyone can use her voice for AI-generated songs](https://www.theverge.com/2023/4/24/23695746/grimes-ai-music-profit-sharing-copyright-ip), and she’d split 50% of royalties.
* [An ongoing Andy Warhol copyright case](https://www.wired.com/story/andy-warhol-fair-use-prince-generative-ai) could also have implications for generative AI, including AI music.

**Regulatory developments in the EU speed up**

The legal landscape impacting AI models is rapidly changing, and this week saw two major developments:

* ChatGPT complied with Italy’s initial demands, resulting in [the lifting of the ban](https://www.bbc.com/news/technology-65431914). OpenAI added additional information on how it trains ChatGPT, provided EU users with a new form objecting to have their data used for training, and now verifies users’ age when signing up. Investigations into ChatGPT, however, remain ongoing in France and Germany.
* The EU has passed [a draft of its AI Act](https://venturebeat.com/ai/eu-lawmakers-pass-draft-of-ai-act-includes-last-minute-change-on-generative-ai-models/), setting the stage for a finalization phase. The most important provision? A new clause that specifies AI models “would have to be designed in accordance with EU law and fundamental rights,” as well as a requirement that AI tools disclose the use of copyrighted materials. We’ll be watching this closely.

**Google’s challenges continue**

Poor Google. Since Bard’s tepid launch they can’t seem to catch a break, and new reports highlight exactly how daunting the AI race will be for them:

* Mindshare about Bard remains low relative to OpenAI and Bing. According to Google Trends, ChatGPT is 8.3x more popular than Bing and [33x more popular than Bard](https://www.artisana.ai/articles/chatgpt-grows-in-popularity-as-bing-and-bard-flatline). 
* Google’s recent merging of Google Brain and Deepmind into a single AI-focused Google Deepmind team [will face steep challenges](https://www.ft.com/content/f4f73815-6fc2-4016-bd97-4bace459e95e). According to Google insiders, Deepmind has historically functioned very independently, thinking about Nobel prize-worthy problems, while Google Brain has operated with indecisive leadership. For the two teams to merge and move quickly to match OpenAI’s focus and speed will be a daunting task.

**Elon Musk’s complicated views on AI**

What exactly are the billionaire’s plans for AI? While no one knows for sure, new details surfaced this week that adds color to the mixed messages he’s been sending:

* The New York Times reported that Elon Musk had ordered Twitter to [turn off OpenAI’s access](https://www.nytimes.com/2023/04/27/technology/elon-musk-ai-openai.html) to its historical tweets after ChatGPT surged in popularity
* Despite founding OpenAI in 2015, Musk has had a falling out with OpenAI on its mission and direction. He reportedly grew disillusioned when OpenAI stopped operating as a non-profit and built “woke” AI models.
* At the same time, the billionaire CEO is building his own Large Language Model as part of his new X.AI initiative. TruthGPT, Musk claims, is a “a maximum-truth-seeking A.I. that tries to understand the nature of the universe.”

**AI roils the job landscape**

Transformative technology has historically been a net benefit for society and GDP, but not without its intermediate pain. This is playing out at warp speed across multiple professions as AI’s power rapidly forces transformation.

* Dropbox announced a 16% headcount cut, [citing AI as one of the reasons](https://www.fastcompany.com/90888639/dropbox-tech-industry-layoffs-ai-job-cuts) behind the significant layoff. What’s notable: this is a profitable, public tech company whose financial metrics have only improved in recent years. For AI initiatives, Dropbox is doubling down — but for mature teams, they’re making cuts. Expect this to be the broad theme of tech as AI surges to the forefront.
* Kenyan ghostwriters, who normally help US college students write essays, [are losing jobs to ChatGPT](https://restofworld.org/2023/chatgpt-taking-kenya-ghostwriters-jobs/). Rest of World reports that many ghostwriters have seen up to 50% decrease in work as AI has reduced demand for human writers.
* A Stanford/MIT study showed that GPT-3 software [helped customer service agents perform as much as 35% better](https://www.artisana.ai/articles/stanford-mit-study-gpt-boosts-support-agent-productivity-by-up-to-35), portending big shifts in knowledge worker jobs as AI makes its way into numerous industries 

**Corporations are unprepared for generative AI, study finds**

[A KPMG study of 225 US executives](https://info.kpmg.us/news-perspectives/technology-innovation/kpmg-generative-ai-2023.html) found that 65% believe generative AI will have a high or extremely high impact on their companies, but nearly the same percentage say generative AI is still a year or two away from having an impact.

* While executives are optimistic, they are also worried it could have a negative impact, especially if risk is not managed
* Almost 4 in 10 executives believe generative AI could decrease social interactions and human connections among employees
* PwC announced a $1B investment in AI over the next 3 years; this is likely to become the norm as corporations pull the trigger on AI investments 

### Science Experiments

Amongst the dozens of impactful research papers coming out each week, we feature the most mindblowing examples below. As always, we try to explain anything technical to a non-technical reader.

**Text to Video**

RunwayML launched its Gen2 text-to-video model and the results are gorgeous. [Here are several examples of what users have created in concert with Midjourney](https://twitter.com/heyBarsee/status/1651961767810179072). The pace of development in the video space is simply on fire; imagine what could be possible by the end of the year.  

* We previously reported on Nvidia’s text-to-video experiments, [click here to see their examples](https://research.nvidia.com/labs/toronto-ai/VideoLDM/).

&#x200B;

[An example of Midjourney combined with RunwayML's Gen 2](https://i.redd.it/4e9rohojz0xa1.gif)

**Segment Anything, but for video**

In another sign of how fast open-source tech is quickly improved, Facebook’s Segment Anything AI library was rapidly adapted into a video-tracking tech that beats Adobe’s own rotoscoping features in its professional software. [The open source repository can be accessed here](https://github.com/gaomingqi/Track-Anything).

&#x200B;

[Better than Adobe's own rotoscoping tools. And free.](https://i.redd.it/fm50gr1mz0xa1.gif)

**Robots playing soccer**

Straight out of the geniuses at Google’s Deepmind team, they’ve applied a technique called Deep Reinforcement learning to help robots move in a dynamic environment. This is a good reminder that the latest breakthroughs aren’t just limited to generative AI. [Check out the full videos here!](https://sites.google.com/view/op3-soccer?pli=1)

&#x200B;

https://preview.redd.it/vdl3d2joz0xa1.png?width=1034&format=png&auto=webp&s=ac93c47f8d87c6088b9531d1cb2bc1e91daeb456

**Text to 3D Models**

A team of researchers generates surprisingly great 3D models out of text prompts. Expect this area of technology to rapidly improve in the next few months. What could that mean for 3D art and the artists who create models? [Full research paper here.](https://arxiv.org/abs/2304.12439)

### Other News

News that didn’t make it into the key themes but is still worth keeping in mind.

WSJ reporter clones her own voice using AI, fooling her family and bank \[[Link](https://www.wsj.com/articles/i-cloned-myself-with-ai-she-fooled-my-bank-and-my-family-356bd1a3)\] 

Free course for developers on ChatGPT prompt engineering released, taught by OpenAI staff \[[Link](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/?ref=emergentmind)\]

Bark, an open source voice cloning tool is released \[[Link](https://github.com/serp-ai/bark-with-voice-clone)\]

ChatGPT finally allows you to turn off conversation history and choose which conversations train their models \[[Link](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt?ref=emergentmind)\]

Will AI lead to mass employment? This author argues it won’t and examines how past technology disruptions have played out \[[Link](https://www.understandingai.org/p/software-didnt-eat-the-world)\]

\-------

P.S. (small self plug) -- If you like this kind of analysis, I offer [a free newsletter](https://artisana.beehiiv.com/subscribe?utm_source=reddit&utm_campaign=chatgpt) that tracks the biggest issues and implications of generative AI tech. Readers from a16z, Sequoia, Meta, McKinsey, Apple and more are all fans. It's been great hearing from so many of you how helpful it is!",/r/ChatGPT/comments/133rmw5/this_week_in_ai_43023_ai_job_losses_ai_music/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
"Apparently if you change mediums, let's say from am article to a video, you can infringe copyright at will",angrybak3r,https://i.redd.it/2eap6gi0ji4c1.jpeg,944,117,1701797942.0,18bhhps,"The subject here is that a youtube content creator copied an article, almost verbatim,  and his take is that as long as it was in a different medium (in this case text to video) everything is fine.",/r/confidentlyincorrect/comments/18bhhps/apparently_if_you_change_mediums_lets_say_from_am/,confidentlyincorrect,r/confidentlyincorrect,False,relevant,highly relevant
Roundup of some of the latest advancements in the field (with links),North-Ad6756,https://www.reddit.com/r/ChatGPT/comments/12hgtcz/roundup_of_some_of_the_latest_advancements_in_the/,815,176,1681128367.0,12hgtcz," 

* **SceneDreamer learns to generate unbounded 3D scenes from in-the-wild 2D image** collections. \[[paper](https://arxiv.org/abs/2302.01330)\] \[[project page](https://scene-dreamer.github.io/)\] \[[video](https://youtu.be/nEfSKL2_FoA)\] \[[demo](https://huggingface.co/spaces/FrozenBurning/SceneDreamer)\]
* OpenAI cofounder **Andrej Karpathy releases baby GPT** \[[demo](https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing)\] \[[link](https://twitter.com/karpathy/status/1645115622517542913)\]
* Last week **NASA released an AI system called DAGGER** predicts solar storms 30 mins before they occur \[[link](https://twitter.com/thealexbanks/status/1644675215891513344)\]
* New model **“InstantBooth” can instantly generate personalized images** with only a single forward pass. \[[abstract](https://arxiv.org/abs/2304.03411)\] \[[project page](https://jshi31.github.io/InstantBooth/)\]
* **ChatGPT now has access to every episode of the Lex Fridman Podcast** thanks to plugins \[[link](https://twitter.com/transitive_bs/status/1643990888417464332)\]
* New ChatGPT plugin can **summarize any YouTube video, answer questions about it, and give specific timestamps** when asked \[[link](https://twitter.com/ykdojo/status/1645300576043794432)\]
* WallStreet legend **Martin Shkreli releases H**[**umE**](http://humeai.herokuapp.com/), an agentic AutoAI with the ability to interact in an abstracted MUD universe \[[link](https://twitter.com/marty_catboy/status/1645135955085471747)\]
* Glass Health releases Glass AI 2.0, which combines a base LLM with a clinical knowledge database, created and maintained by clinicians, to **create DDx and Clinical Plan outputs** \[[link](https://glass.health/ai/)\]
* **Fast.ai releases their new course** “From Deep Learning Foundations to Stable Diffusion”, which is part 2 of Practical Deep Learning for Coders \[[link](https://www.fast.ai/posts/part2-2023.html)\]
* Someone ported yoheinakajima’s **BabyAGI library to Streamlit** \[[github](https://github.com/dory111111/babyagi-streamlit)\] \[[link](https://twitter.com/DataChaz/status/1645152577258962944)\]
* **Cerebras released Cerebras-GPT**, their own LLMs trained following Chinchilla strategy on Cerebras wafers \[[link](https://twitter.com/madiator/status/1644900029830950912)\]
* **LangChain releases a ChatGPT plugin** \[[github](https://github.com/langchain-ai/langchain-aiplugin)\]
* **AI Steve Jobs converses with AI Elon Musk** \[[link](https://twitter.com/heyBarsee/status/1644617954363834368)\]
* Chatbase allows you to **create a custom ChatGPT from your website content** and add it to your site as a chat widget \[[link](https://twitter.com/yasser_elsaid_/status/1645328188086833152)\]
* New paper **“Generative Agents: Interactive Simulacra of Human Behavior” introduces generative agents--computational software agents that simulate believable human behavior.** Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. \[[paper](https://arxiv.org/abs/2304.03442)\] \[[project page](https://t.co/khS5i3jsHN)\]
* Huge **ChatGPT plugins hackathon** with Chroma , Replit and OpenAI at Retool \[[demo videos](https://twitter.com/swyx/status/1644765314176151552)\]
* MemoryGPT (plugin) - **ChatGPT but with long term memory**. It will remember the things you say and will be able to personalize your conversation based on that \[[demo video](https://twitter.com/rikvk01/status/1644787327057776645)\]
* **Incredible short films (action movies) being made with GPT-4 api and WonderDynamics** \[[link](https://twitter.com/heyBarsee/status/1645079642137567232)\] \[[link](https://twitter.com/ZappyZappy7/status/1644830155595194369)\]
* Marrying Grounding DINO with Segment Anything & Stable Diffusion & BLIP - **Automatically Detect, Segment and Generate Anything with Image and Text Inputs** \[[github](https://github.com/IDEA-Research/Grounded-Segment-Anything)\]
* Meta AI releases “**Segment Anything Model (SAM)**” a new AI model from Meta AI that can ""cut out"" any object, in any image, with a single click \[[Paper](https://ai.facebook.com/research/publications/segment-anything/)\] \[[Project](https://segment-anything.com/)\] \[[Demo](https://segment-anything.com/demo)\] \[[Dataset](https://segment-anything.com/dataset/index.html)\] \[[Blog](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)\] \[[BibTeX](https://github.com/facebookresearch/segment-anything#citing-segment-anything)\]
* Nomic-AI releases a Flask web application that provides a **chat UI for interacting with the GPT4All chatbot** \[[github](https://github.com/nomic-ai/gpt4all-ui)\]
* Microsoft researchers present **first attempt to use GPT-4 to generate instruction-following data for LLM fine tuning** \[[Project Page](https://instruction-tuning-with-gpt-4.github.io/)\] \[[Paper](https://arxiv.org/abs/2304.03277)\]
* New open source vector database **Chroma** trending on Github \[[github](https://github.com/chroma-core/chroma)\]
* SadTalker - Learning Realistic 3D Motion Coefficients for **Stylized Audio-Driven Single Image Talking Face Animation** \[[project page](http://sadtalker.github.io/)\]
* VideoCrafter - A Toolkit for **Text-to-Video Generation and Editing** \[[github](https://github.com/VideoCrafter/VideoCrafter-gallery-showcase)\]
* AlpacaTurbo - **Web UI to run alpaca model locally** \[[github](https://github.com/ViperX7/Alpaca-Turbo)\]
* Tabby - **Self-hosted AI coding assistant**. An opensource / on-prem alternative to GitHub Copilot \[[github](https://github.com/TabbyML/tabby)\]
* OpenAI CEO (Sam Altman) considers opening office as Japan government eyes adoption \[[link](https://www.reuters.com/technology/japan-eyes-government-ai-adoption-openai-ceo-mulls-opening-office-2023-04-10/)\]
* Apparently, high paying jobs are more vulnerable to AI \[[link](https://www.ft.com/content/82a52547-57e0-422d-833b-9c4465d95699)\]

I hope you find these AI breakthroughs and projects as exciting as I do! I'd love to hear your thoughts, opinions, and predictions about these advancements in the comments below. Let's have a lively discussion! 🗣️

I'm also excited to announce that I've started a free daily newsletter called ""The AI Revolution"" to help you stay updated on the latest AI advancements, all in one place. Today's post is just the first issue, and I'm completely open to suggestions for improving tomorrow's newsletter. Your feedback will be invaluable in shaping this resource.

Subscribe to ""The AI Revolution"" and never miss an update: [https://theairevolution.beehiiv.com/subscribe](https://theairevolution.beehiiv.com/subscribe) 📧

And feel free to follow us on Twitter for more recent updates: [https://twitter.com/TheAIRevolu](https://twitter.com/TheAIRevolu)

Looking forward to your thoughts and ideas!",/r/ChatGPT/comments/12hgtcz/roundup_of_some_of_the_latest_advancements_in_the/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
The most remarkable AI releases of 2023,alina_valyaeva,https://i.redd.it/1ues5xc8g18c1.png,675,95,1703334717.0,18p4qwb,,/r/artificial/comments/18p4qwb/the_most_remarkable_ai_releases_of_2023/,artificial,r/artificial,False,not relevant,highly relevant
PIKA LABS Future of AI video / Animation,AI-For-Success,https://v.redd.it/fsr9m5n0rfcb1,542,69,1689560570.0,151p0s0,"Absolutely loving PIKA LABS, my top AI tool right now! 🚀 The feature to animate still images into dynamic videos is a game changer and it supports text to video which is 100 times better than Runway ML. 

https://youtu.be/YbwHzT9TZ4U
https://youtu.be/7ugRPWoY1xA",/r/singularity/comments/151p0s0/pika_labs_future_of_ai_video_animation/,singularity,r/singularity,True,relevant,highly relevant
Will history come to see today's announcement of Sora by OpenAI as a landmark moment in the debate about the economic effects of automation?,lughnasadh,https://www.reddit.com/r/Futurology/comments/1arry7l/will_history_come_to_see_todays_announcement_of/,490,416,1708035388.0,1arry7l,"I'd always expected there would be landmark moments along the journey to the day when robots & AI will be capable of doing all the work that humans can do. Five years ago I thought the day self-driving technology was perfected and began the elimination of human driving jobs would be such a moment. It turns out that day hasn't arrived just yet, but something as significant has.

I think [today's demonstration by OpenAI of their text-to-video AI called Sora](https://openai.com/sora) could be such a moment. It won't end Hollywood overnight, but suddenly millions of production jobs in movies, TV, video games, VFX, and 3D look very threatened.

So far the debate about AI and its destructive economic effects has been in the abstract, but I wonder if that is about to change for good. Millions of people who were doing their best to not worry about this debate now have reasons for real anxiety. 

I won't be surprised if it moves to the top of the list of concerns in political debate. Furthermore, I suspect many in the economic establishment are going to get more worried. The loss of employment in huge sectors of the creative economy has knock-on effects - risk of recession, mortgage defaults, deflation, stock market valuations. Suddenly this starts to become everyone's problem and can't be ignored.",/r/Futurology/comments/1arry7l/will_history_come_to_see_todays_announcement_of/,Futurology,r/Futurology,False,relevant,less relevant
"Google’s AI Research is Unreal, But Where are the Products?",nerdynavblogs,https://www.reddit.com/r/ChatGPT/comments/1ad418n/googles_ai_research_is_unreal_but_where_are_the/,478,95,1706452717.0,1ad418n,"This is not an AI news post. But an opinion post.
==========================

Did you hear about Google's new AI text to video which beats all other models like Gen 2? ([Lumiere](https://lumiere-video.github.io/))
  

  
Or about Google's new revolutionary AI text to speech which produces lifelike AI voices that leave eleven labs in dust? ([Soundstorm](https://google-research.github.io/seanet/soundstorm/examples/))
  

  
Or about Google's new AI image generator that will supposedly kill Dall E and Midjourney? ([Imagen](https://google-research.github.io/seanet/soundstorm/examples/))
  

  
Or did you hear about Google's new multimodal AI that beat GPT-4 at most benchmarks? (Gemini Ultra, unreleased)

  
I think you get the point I am trying to make. When it comes to Google, we hear a lot. But we rarely get to do anything.
  

  
None of the groundbreaking tools I just mentioned have a web UI for the normal public. They remain confined to research papers and github pages.
  
 ([Video version of this post with screenshots](https://www.youtube.com/watch?v=YW3ra6fwSlM))
  
To be honest, the biggest clickbaiter on the internet is none other than Google itself.
  

  
Google is literally behind the Transformers architecture that powers ChatGPT. Yet, it took Open AI releasing ChatGPT and then GPT-4 for Google to actually come up with a decent public facing LLM.
  

  
Lethargy and confining new tech to research arenas is not the only problem with Google.
  

  
Google has Music Fx to generate amazing music. But can you use it? Probably not. Unless you live in America.
  

  
Same with Text Fx which is an amazing tool for writers.
  

  
Same with Notebook LM which would be a game changer for students. It allows you to chat with pdfs, drive files (multiple sources) in a notebook env. [Notebook LM walkthrough](https://www.youtube.com/watch?v=4fyIR_3YA1I) - again US only.
  

  
All this amazing tech. Yet none of it available to the public at large. 
  

  
Starting projects and killing them is nothing new for Google. But when it comes to AI, I expected different. 
  

  
Google Deepmind, the AI research branch of Google, released so many models this year like Alpha Geometry which is an Olympiad level AI for geometry. Let that sink in. [Nature journal link](https://www.nature.com/articles/s41586-023-06747-5)
  

  
Google has created an AI which can do geometry problems better than the brightest humans can. They also have Google AMIE, a medical AI. During research surveys, they found that patients actually prefer Google's medical AI to human doctors because the AI is patient, helpful, and accurate. [Nature journal link](https://www.nature.com/articles/d41586-024-00099-4)
  

  
Google is sitting on valuable research that can get us closer to AGI, closer to Olympiad level teachers for every kid, closer to emphathetic medical care for everyone - poor, veterans, distressed.
  
  
But all that exists in research papers, github pages, and again archives of a million webpages. Not in hands of the normal user. And I think that is a problem and a tragedy. Not just a tragedy of business, but also of science.

Note: Github pages are not same as github code repos. These are just static webpages hosted on github with examples and pdfs.

Sources:
https://lumiere-video.github.io/
https://google-research.github.io/seanet/soundstorm/examples/
https://deepmind.google/technologies/imagen-2/
https://aitestkitchen.withgoogle.com/tools/music-fx
https://textfx.withgoogle.com/
https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/
https://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html",/r/ChatGPT/comments/1ad418n/googles_ai_research_is_unreal_but_where_are_the/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/1abkvm4/this_week_in_ai_all_the_major_ai_developments_in/,448,33,1706282796.0,1abkvm4,"1. **Amazon** presents ***Diffuse to Choose***, a diffusion-based image-conditioned inpainting model that allows users to virtually place any e-commerce item in any setting, ensuring detailed, semantically coherent blending with realistic lighting and shadows. Code and demo will be released soon \[[*Details*](https://diffuse2choose.github.io/)\].
2. **OpenAI** announced two new embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and lower pricing on GPT-3.5 Turbo. The updated GPT-4 Turbo preview model reduces cases of “laziness” where the model doesn’t complete a task. The new embedding models include a smaller and highly efficient ***text-embedding-3-small*** model, and a larger and more powerful ***text-embedding-3-large*** model. \[[*Details*](https://openai.com/blog/new-embedding-models-and-api-updates)\].
3. **Hugging Face** and **Google** partner to support developers building AI applications \[[*Details*](https://huggingface.co/blog/gcp-partnership)\].
4. **Adept** introduced ***Adept Fuyu-Heavy***, a new multimodal model designed specifically for digital agents. Fuyu-Heavy scores higher on the MMMU benchmark than Gemini Pro \[[*Details*](https://www.adept.ai/blog/adept-fuyu-heavy)\].
5. **Fireworks.ai** has open-sourced ***FireLLaVA***, a LLaVA multi-modality model trained on OSS LLM generated instruction following data, with a commercially permissive license. Firewroks.ai is also providing both the completions API and chat completions API to devlopers \[[*Details*](https://fireworks.ai/blog/firellava-the-first-commercially-permissive-oss-llava-model)\].
6. **01.AI** released ***Yi Vision Language (Yi-VL)*** model, an open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images. Yi-VL adopts the **LLaVA** architecture and is free for commercial use. Yi-VL-34B is the first open-source 34B vision language model worldwide \[[Details](https://huggingface.co/01-ai/Yi-VL-34B)\].
7. **Tencent AI Lab** introduced ***WebVoyager***, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites \[[*Paper*](https://arxiv.org/pdf/2401.13919.pdf)\].
8. **Prophetic** introduced ***MORPHEUS-1***, a multi-modal generative ultrasonic transformer model designed to induce and stabilize lucid dreams from brain states. Instead of generating words, Morpheus-1 generates ultrasonic holograms for neurostimulation to bring one to a lucid state \[[*Details*](https://x.com/PropheticAI/status/1750534355242418300)\].
9. **Google Research** presented ***Lumiere*** – a space-time video diffusion model for text-to-video, image-to-video, stylized generation, inpainting and cinemagraphs \[[*Details*](https://lumiere-video.github.io/)\].
10. **TikTok** released ***Depth Anything***, an image-based depth estimation method trained on 1.5M labeled images and 62M+ unlabeled images jointly \[[*Details*](https://depth-anything.github.io/)\].
11. **Nightshade**, the free tool that ***‘poisons’ AI models***, is now available for artists to use \[[*Details*](https://venturebeat.com/ai/nightshade-the-free-tool-that-poisons-ai-models-is-now-available-for-artists-to-use/)\].
12. **Stability AI** released ***Stable LM 2 1.6B***, 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. Stable LM 2 1.6B can be used now both commercially and non-commercially with a Stability AI Membership \[[*Details*](https://stability.ai/news/introducing-stable-lm-2)\].
13. **Etsy** launched ‘***Gift Mode***,’ an AI-powered feature designed to match users with tailored gift ideas based on specific preferences \[[*Details*](https://techcrunch.com/2024/01/24/etsy-launches-gift-mode-a-new-ai-powered-feature-that-generates-200-gift-guides/)\].
14. **Google DeepMind** presented ***AutoRT***, a framework that uses foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. In AutoRT, a VLM describes the scene, an LLM generates robot goals and filters for affordance and safety, then routes execution to policies \[[*Details*](https://auto-rt.github.io/)\].
15. **Google Chrome** gains AI features, including a writing helper, theme creator, and tab organizer \[[*Details*](https://techcrunch.com/2024/01/23/google-chrome-gains-ai-features-including-a-writing-helper-theme-creator-and-tab-organizer)\].
16. **Tencent AI Lab** released ***VideoCrafter2*** for high quality text-to-video generation, featuring major improvements in visual quality, motion and concept Composition compared to VideoCrafter1 \[[*Details*](https://ailab-cvc.github.io/videocrafter2/) | [*Demo*](https://huggingface.co/spaces/VideoCrafter/VideoCrafter2)\]
17. **Google** opens beta access to the *conversational experience,* a new chat-based feature in Google Ads, for English language advertisers in the U.S. & U.K. It will let advertisers create optimized Search campaigns from their website URL by generating relevant ad content, including creatives and keywords \[[*Details*](https://blog.google/products/ads-commerce/put-google-ai-to-work-with-search-ads)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.substack.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Links removed in this post due to Automod, but they are incuded in the newsletter. Thanks.",/r/StableDiffusion/comments/1abkvm4/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/145dt8g/this_week_in_ai_all_the_major_ai_developments_in/,443,39,1686337540.0,145dt8g,"1. Researchers from **Snap** present **SnapFusion**, a new approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than 2 seconds .
2. **StabilityAI** adds a new feature **Uncrop** to their generative AI tool, **Clipdrop**. It creates AI-generated backgrounds to automatically expand any image using Stable Diffusion XL as a foundation model. It’s free to try in the Clipdrop web app, with no need to log in.
3. **Google** has updated **Bard** with a new technique, implicit code execution. This lets Bard run code in the background when it sees math-related prompts, making word problems and math calculations about 30% more accurate. Bard can now also directly export any table it creates to Google Sheets\*.\*
4. **Microsoft** develops **Orca** \- a 13-billion parameter model outperforming smaller open-source models and at times equaling or outperforming ChatGPT, though it lags behind GPT-4.
5. **Google** presents and *open-sources* **Visual Captions**, a system that uses spoken words to add real-time images to video chats.
6. **AlphaDev**, Google DeepMind’s AI, discovers small sorting algorithms from scratch that outperformed human benchmarks. These algorithms have been added to the LLVM standard C++ sort library. This is the first time an algorithm designed by AI has been added to this library. AlphaDev also discovered a new hashing algorithm, now released in the open-source.
7. **Adobe** opens its Firefly generative AI model to enterprise customers, allowing them to customize the model with their own branded assets.
8. **HuggingChat**, the open-source alternative to ChatGPT by HuggingFace added a web search feature.
9. **Tafi**, the owner of Daz 3D announces the launch of a text-to-3D character engine, that will allow users to create high-quality custom 3D characters using simple text prompts. Tafi is using a massive 3D dataset derived from its proprietary Genesis character platform.
10. **Runway’s** much-awaited **Gen-2** for text-to-video is available now with free triaL.
11. **Europe** wants platforms to label AI-generated content to fight disinformation..
12. **Google** presents **SQuId**, a 600M parameter regression model that uses the SQuId dataset and cross-locale learning to evaluate speech synthesis quality in multiple languages and describe how natural it sounds.
13. **Together** released the v1 versions of the RedPajama-INCITE family of models, allowing commercial use. RedPajama-INCITE-7B-Instruct is the highest scoring open model on HELM benchmarks, outperforming Falcon-7B. RedPajama, is a project to create leading open-source models, and it reproduced LLaMA training dataset of over 1.2 trillion tokens in April.
14. **Wordpress** launches Jetpack AI Assistant for generating blog posts, detailed pages, structured lists and comprehensive tables from within the Wordpress editor.
15. **Google Research** presents **StyleDrop**: a method for generation of images from text prompts in any style described by a *single reference image.* StyleDrop is powered by Muse, a text-to-image generative vision transformer.

If you like this news format, you might find my [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks",/r/StableDiffusion/comments/145dt8g/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
[D] OpenAI Sora Video Gen -- How??,htrp,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,387,202,1708022346.0,1armmng,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?",/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,MachineLearning,r/MachineLearning,False,relevant,highly relevant
"GPT 4 Can Improve Itself - (ft. Reflexion, HuggingGPT, Bard Upgrade and much more)",emeka64,https://www.reddit.com/r/singularity/comments/129reay/gpt_4_can_improve_itself_ft_reflexion_hugginggpt/,379,269,1680455478.0,129reay,"""GPT 4 can self-correct and improve itself. With exclusive discussions with the lead author of the Reflexions paper, I show how significant this will be across a variety of tasks, and how you can benefit.  I go on to lay out an accelerating trend of self-improvement and tool use, laid out by Karpathy, and cover papers such as Dera, Language Models Can Solve Computer Tasks and TaskMatrix, all released in the last few days.  I also showcase HuggingGPT, a model that harnesses Hugging Face and which I argue could be as significant a breakthrough as Reflexions. I show examples of multi-model use, and even how it might soon be applied to text-to-video and CGI editing (guest-starring Wonder Studio). I discuss how language models are now generating their own data and feedback, needing far fewer human expert demonstrations. Ilya Sutskever weighs in, and I end by discussing how AI is even improving its own hardware and facilitating commercial pressure that has driven Google to upgrade Bard using PaLM. "" [https://www.youtube.com/watch?v=5SgJKZLBrmg](https://www.youtube.com/watch?v=5SgJKZLBrmg)",/r/singularity/comments/129reay/gpt_4_can_improve_itself_ft_reflexion_hugginggpt/,singularity,r/singularity,False,relevant,highly relevant
"WELCOME TO AI VIDEO!! Tools for Text to Video, Image to Video and Video to Video have all been released! FULL LIST OF LINKS IN COMMENTS",ZashManson,https://v.redd.it/n29p84k5ig4b1,324,16,1686082921.0,142s3mt,,/r/aivideo/comments/142s3mt/welcome_to_ai_video_tools_for_text_to_video_image/,aivideo,r/aivideo,True,relevant,highly relevant
"OpenAI's ""big"" announcement: do you think it is GPT-4.5 or GPT-Video?",Yuli-Ban,https://www.reddit.com/r/singularity/comments/18eul8k/openais_big_announcement_do_you_think_it_is_gpt45/,332,173,1702179350.0,18eul8k,"GPT-4.5 is what I'm leaning towards. A pretty substantial boost to GPT-4, with extra reasoning, creativity, maybe a doubled or tripled output window (so the ability to put out upwards of 8k to 10k words in one go) to better utilize the overall content window length being 128k, and whatnot. For programming and written content, an output length of 10k to 16k tokens would be a massive boost all its own so long as the coherence remained very high.

However, there is a decent possibility it may be a text-to-video genetator, as it has been leaked that OpenAI has had one since the spring but sat on it— the recent spate of video synthesis model releases may have convinced them to put it out now, adding it to the suite of ChatGPT abilities, probably with a Bing Video Creator going along with it.

It could go either way. There could be a third option, like the GPT store being opened early, but I'm leaning towards these two. Which would you say it is?

Edit: Holy shit, I step away from my computer hoping that someone would bother to post the rumors of a big announcement to those asking for it, and not a single person did, making me look like a /r/Singularity hypist jagoff.

https://twitter.com/futuristflower/status/1717634825023008797

https://twitter.com/apples_jimmy/status/1732553640215495109",/r/singularity/comments/18eul8k/openais_big_announcement_do_you_think_it_is_gpt45/,singularity,r/singularity,False,relevant,highly relevant
"Meta: Today we’re sharing two new advances in our generative AI research: Emu Video & Emu Edit. Emu Video This new text-to-video model leverages our Emu image generation model and can respond to text-only, image-only or combined text & image inputs to generate high quality video.",SharpCartographer831,https://twitter.com/aiatmeta/status/1725182736904712520?s=46,274,42,1700154024.0,17wru59,,/r/singularity/comments/17wru59/meta_today_were_sharing_two_new_advances_in_our/,singularity,r/singularity,False,relevant,highly relevant
Terrified.,Titanica25,https://www.reddit.com/r/animationcareer/comments/1arwhs5/terrified/,270,150,1708047517.0,1arwhs5,"The announcement of OpenAI's Sora text-to-video model has me genuinely mortified as a rising 3D animator, man. I'm heading off to college in a few months to major in digital arts in the hopes of working in animation. I've read through tons of posts on this sub and have mainly just lurked, as I'm just trying to keep a rational outlook towards what I can expect for my career. While the industry is definitely struggling right now, I still feel so strongly about working in it.

But the announcement of OpenAI's new video model has me so terrified, particularly the prompt that created a Pixar-style 3D animation. They've reached a point where their models can create videos that are genuinely hard to tell apart from the real things, and it is tearing me apart, man. What's worse is seeing all the damn comments about it here on Reddit and Twitter. People celebrating this, mocking those who will lose their opportunity to work not just in the animation industry, but film, stock work, etc.

It kills me how the human touch in art and art as a whole is being so damn misunderstood and undervalued, and it frightens me to think of the future. I just really need some help breaking it down from people who are more experienced in the industry and educated on AI.",/r/animationcareer/comments/1arwhs5/terrified/,animationcareer,r/animationcareer,False,relevant,less relevant
Family Guy: The Movie - AI Text-To-Video Movie Trailer,XXmynameisNeganXX,https://v.redd.it/nf2jeqlfyt4b1,254,5,1686245824.0,144fnsa,,/r/AIVoiceMemes/comments/144fnsa/family_guy_the_movie_ai_texttovideo_movie_trailer/,AIVoiceMemes,r/AIVoiceMemes,True,not relevant,neutral
Old Man Joe - AI Text-To-Video Movie Trailer ,XXmynameisNeganXX,https://v.redd.it/v11slzio7g4b1,227,6,1686079414.0,142ql8r,,/r/AIVoiceMemes/comments/142ql8r/old_man_joe_ai_texttovideo_movie_trailer/,AIVoiceMemes,r/AIVoiceMemes,True,not relevant,neutral
Better Text to Video. Einstein giving thumbs up while the camera orbits. This is the text to video method from today merged with my own temporal consistency method for better quality and no watermark. Isn't the future fun.,Tokyo_Jab,https://v.redd.it/1sdln11yznpa1,220,26,1679654034.0,120gb0a,,/r/StableDiffusion/comments/120gb0a/better_text_to_video_einstein_giving_thumbs_up/,StableDiffusion,r/StableDiffusion,True,relevant,highly relevant
John Wick as an Anime,Theblasian35,https://v.redd.it/sn2f70wshh6c1,221,8,1702657293.0,18j3tx4,Created with Pika 1.0 text to video ,/r/midjourney/comments/18j3tx4/john_wick_as_an_anime/,midjourney,r/midjourney,True,relevant,highly relevant
Thank you to the people who convinced me to try ComfyUI,jonhartattack,https://www.reddit.com/r/StableDiffusion/comments/1899177/thank_you_to_the_people_who_convinced_me_to_try/,215,99,1701539454.0,1899177,"I've been messing around with it for about 6 hours now and I am so freaking excited.

Already producing basic animations at 640x360 using Text to Video and Image to Video. 

If you're using Automatic 1111 and intimidated by Comfy UI, don't be. Try it out and you'll be producing some cool stuff in no time.",/r/StableDiffusion/comments/1899177/thank_you_to_the_people_who_convinced_me_to_try/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Yesterday there was a round of talk on SD Discord with Emad and the finetuners responsible for SD XL. It was quite interesting. Stability posted the video on YouTube. I made a transcription (Using Whisper-largev2) and also a summary of the main keypoints. Here's the information you need to know:,Seromelhor,https://www.reddit.com/r/StableDiffusion/comments/14kqwdy/yesterday_there_was_a_round_of_talk_on_sd_discord/,211,87,1687903110.0,14kqwdy,"So let's start from the beginning... You can watch/listen to the entire conversation here as Stability posted it on their YouTube channel. Link in comments.

I've put the transcriptions here (txt. srt, tsv, vtt,json) in comments.

And here are the main keypoints:

1. **SDXL 0.9 is a stepping stone to SDXL 1.0**: The team is excited about the progress they've made with SDXL 0.9 and see it as a stepping stone to the full release of SDXL 1.0.
2. **Community involvement**: The community has been actively involved in testing and providing feedback on the new AI versions, particularly through the Discord bot. This feedback has been crucial for fine-tuning the AI to better align with human preferences.
3. **Availability**: SDXL 0.9 is available on ClipDrop and will soon be live on the API and Dream Studio. The research weights will be sent out to researchers over the course of the week.
4. **Release timeline**: The full 1.0 launch is targeted for mid-July. The main release will be a creative ML license release, similar to the original Stable Diffusion.
5. **Continuous improvement**: The team plans to continuously improve the model, with periodic releases. They are also working on other models like SD3.
6. **Community-based entities**: The team is working to make fine-tuning easier and plans to provide guides and grants for extensions. They also plan to make the AI usable in community-based entities like Comfy UI.  The weights and fine-tunes from leading fine-tuners will be available around the time of release, along with guides and other resources.   The team is striving to improve their interaction with the community and increase transparency over the roadmaps. They are also working on making the AI more accessible, with the goal of running it on devices like MacBooks.
7. **Research and applied teams collaboration**: The research and applied teams have been working closely together to improve the model. The applied team brings a different perspective to the development process.
8. **Hardware requirements:** The team confirmed that 8 gigabytes of VRAM is sufficient to generate images and train a LORA at full 1024 by 1024 at rank eight on an RTX 2070.  A full fine-tuning can be done on a 3090.
9. **Future improvements**: The team is working on making the AI more accessible by optimizing data sets and refining architectures. They are also working on better tools and interfaces for users.
10. **SDXL 1.0 is still in development:** The architecture of SDXL 1.0 is expected to change before its release. The current benchmarks are based on the current version of SDXL 0.9.
11. **SDXL 1.0 is a state-of-the-art model**: SDXL 1.0 is likely one of the state-of-the-art models in the world. However, users can still use older versions like SD 1.5 for certain tasks.
12. **Integration of Stable LLM and Stable Diffusion:** The team is considering integrating these two systems in a similar way to Bing chat with Dall-e. They believe there's a lot of potential in this integration, but they're currently occupied with other projects.
13. **Release of StableLM:** The team has a language model UI ready to go, and they're preparing to release the beta version of StableLM. They've also released their Lama replications up to 13 billion parameters and have just finished 20 billion.
14. **Improvements in SDXL**: The team has noticed significant improvements in prompt comprehension with SDXL. They believe it performs better than other models on the market and is a big improvement on what can be created. The model's ability to understand and respond to natural language prompts has been particularly impressive.
15. **SDXL's Capabilities**: One of the key strengths of SDXL is its minimum performance bar. Even with minimal effort, the outputs it produces are at least equal to, if not better than, those produced by SD version 1. The team believes this will have significant implications for artists and others who work with the model.
16. **Environmental Considerations**: The team is aware of the environmental impact of the energy used by these models and is taking steps to reduce this. They're being cautious about the release of 1.0 to ensure it can be used efficiently and effectively.
17. **Release Plans**: The team is aiming for a mid-July release, but they're prepared to release a 0.95 version if necessary. They're also planning to release the code for the model and are considering a grant program for fine tuners.
18. **NSFW Filtering**: The team is aware of concerns about NSFW content. They believe the model is highly tunable in this regard.
19. **Future Plans:** After the release of SDXL 1.0, the team plans to work on a range of projects, including new SD versions, text-to-video models, specialized models, and more.
20. **Workflow in Building a Big Model**: The process of building a model like SDXL involves constant testing and iteration. The team runs many different tests, comparing different models and parameters, and gradually refines the model based on the results. The goal is to create a model that is useful and valuable, but also efficient and environmentally friendly.

&#x200B;

Thats all you need to know! I hope you enjoy it!

Thank you Redmond .AI for providing the machine for a quick transcription.

&#x200B;",/r/StableDiffusion/comments/14kqwdy/yesterday_there_was_a_round_of_talk_on_sd_discord/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"Text to video with any SD model is now possible! SD-CN-Animation got v0.5 update, It allows you to generate videos from text at any resolution, any length, using any SD model.",Another__one,https://v.redd.it/k8ucgldwqrua1,212,42,1681879010.0,12rgy1f,,/r/StableDiffusion/comments/12rgy1f/text_to_video_with_any_sd_model_is_now_possible/,StableDiffusion,r/StableDiffusion,True,relevant,neutral
Will Smith Eating Spaghetti - Text-to-Video AI Generated,Ramenko1,https://v.redd.it/s9j7ss07dqra1,199,13,1680554287.0,12ax0ji,,/r/aigeneratedmemes/comments/12ax0ji/will_smith_eating_spaghetti_texttovideo_ai/,aigeneratedmemes,r/aigeneratedmemes,True,not relevant,neutral
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/1asatxy/this_week_in_ai_all_the_major_ai_developments_in/,196,7,1708096067.0,1asatxy,"1. **Meta AI** introduces ***V-JEPA*** (Video Joint Embedding Predictive Architecture), a method for teaching machines to understand and model the physical world by watching videos. Meta AI releases a collection of V-JEPA vision models trained with a feature prediction objective using self-supervised learning. The models are able to understand and predict what is going on in a video, even with limited information \[[*Details*](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) | [*GitHub*](https://github.com/facebookresearch/jepa)\].
2. **Open AI** introduces ***Sora***, a text-to-video model that can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions \[[*Details + sample videos*](https://openai.com/sora)[ ](https://openai.com/sora)| [*Report*](https://openai.com/research/video-generation-models-as-world-simulators)\].
3. **Google** announces their next-generation model, **Gemini 1.5,** that uses a new [Mixture-of-Experts](https://arxiv.org/abs/1701.06538) (MoE) architecture. The first Gemini 1.5 model being released for early testing is ***Gemini 1.5 Pro*** with a context window of up to 1 million tokens, which is the longest context window of any large-scale foundation model yet. 1.5 Pro can perform sophisticated understanding and reasoning tasks for different modalities, including video and it performs at a similar level to 1.0 Ultra \[[*Details*](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15) *|*[*Tech Report*](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\].
4. Reka introduced **Reka Flash,** a new 21B multimodal and multilingual model trained entirely from scratch that is competitive with Gemini Pro & GPT 3.5 on key language & vision benchmarks. Reka also present a compact variant Reka Edge , a smaller and more efficient model (7B) suitable for local and on-device deployment. Both models are in public beta and available in [**Reka Playground** ](https://chat.reka.ai/chat)\[[*Details*](https://reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model)\].
5. **Cohere** For AI released ***Aya***, a new open-source, massively multilingual LLM & dataset to help support under-represented languages. Aya outperforms existing open-source models and covers 101 different languages – more than double covered by previous models \[[*Details*](https://cohere.com/research/aya)\].
6. **BAAI** released ***Bunny***, a family of lightweight but powerful multimodal models. Bunny-3B model built upon SigLIP and Phi-2 outperforms the state-of-the-art MLLMs, not only in comparison with models of similar size but also against larger MLLMs (7B), and even achieves performance on par with LLaVA-13B \[[*Details*](https://github.com/BAAI-DCAI/Bunny)\].
7. **Amazon** introduced a text-to-speech (TTS) model called ***BASE TTS*** (Big Adaptive Streamable TTS with Emergent abilities). BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data and exhibits “emergent” qualities improving its ability to speak even complex sentences naturally \[[*Details*](https://techcrunch.com/2024/02/14/largest-text-to-speech-ai-model-yet-shows-emergent-abilities/) | [*Paper*](https://assets.amazon.science/6e/82/1d037a4243c9a6cf4169895482d5/base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data.pdf)\].
8. **Stability AI** released ***Stable Cascade*** in research preview, a new text to image model that is exceptionally easy to train and finetune on consumer hardware due to its three-stage architecture. Stable Cascade can also generate image variations and image-to-image generations. In addition to providing checkpoints and inference scripts, Stability AI has also released scripts for finetuning, ControlNet, and LoRA training \[[*Details*](https://stability.ai/news/introducing-stable-cascade)\].
9. **Researchers** from UC berkeley released ***Large World Model (LWM)***, an open-source general-purpose large-context multimodal autoregressive model, trained from LLaMA-2, that can perform language, image, and video understanding and generation. LWM answers questions about 1 hour long YouTube video even if GPT-4V and Gemini Pro both fail and can retriev facts across 1M context with high accuracy \[[*Details*](https://largeworldmodel.github.io/)\].
10. **GitHub** opens applications for the next cohort of ***GitHub Accelerator program*** with a focus on funding the people and projects that are building ***AI-based solutions*** under an open source license \[[*Details*](https://github.blog/2024-02-13-powering-advancements-of-ai-in-the-open-apply-now-to-github-accelerator)\].
11. **NVIDIA** released ***Chat with RTX***, a locally running (Windows PCs with specific NVIDIA GPUs) AI assistant that integrates with your file system and lets you chat with your notes, documents, and videos using open source models \[[*Details*](https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai)\].
12. **Open AI** is testing ***memory with ChatGPT***, enabling it to remember things you discuss across all chats. ChatGPT's memories evolve with your interactions and aren't linked to specific conversations. It is being rolled out to a small portion of ChatGPT free and Plus users this week \[[*Details*](https://openai.com/blog/memory-and-new-controls-for-chatgpt)\].
13. **BCG X** released of ***AgentKit***, a LangChain-based starter kit (NextJS, FastAPI) to build constrained agent applications \[[*Details*](https://blog.langchain.dev/bcg-x-releases-agentkit-a-full-stack-starter-kit-for-building-constrained-agents/) | [*GitHub*](https://github.com/BCG-X-Official/agentkit)\].
14. **Elevenalabs**' Speech to Speech feature, launched in November, for voice transformation with control over emotions and delivery, is now ***multilingual*** and available in 29 languages \[[*Link*](https://elevenlabs.io/voice-changer)\]
15. **Apple** introduced ***Keyframer***, an LLM-powered animation prototyping tool that can generate animations from static images (SVGs). Users can iterate on their design by adding prompts and editing LLM-generated CSS animation code or properties \[[*Paper*](https://arxiv.org/pdf/2402.06071.pdf)\].
16. **Eleven Labs** launched a ***payout program*** for voice actors to earn rewards every time their voice clone is used \[[*Details*](https://elevenlabs.io/voice-actors)\].
17. **Azure OpenAI Service** announced Assistants API, new models for finetuning, new text-to-speech model and new generation of embeddings models with lower pricing \[[*Details*](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-openai-service-announces-assistants-api-new-models-for/ba-p/4049940)\].
18. **Brilliant Labs**, the developer of AI glasses, launched ***Frame***, the world’s first glasses featuring an integrated AI assistant, ***Noa***. Powered by an integrated multimodal generative AI system capable of running GPT4, Stability AI, and the Whisper AI model simultaneously, Noa performs real-world visual processing, novel image generation, and real-time speech recognition and translation. \[[*Details*](https://venturebeat.com/games/brilliant-labss-frame-glasses-serve-as-multimodal-ai-assistant/)\].
19. **Nous Research** released ***Nous Hermes 2 Llama-2 70B*** model trained on the Nous Hermes 2 dataset, with over 1,000,000 entries of primarily synthetic data \[[*Details*](https://huggingface.co/NousResearch/Nous-Hermes-2-Llama-2-70B)\].
20. **Open AI** in partnership with Microsoft Threat Intelligence, have disrupted five state-affiliated actors that sought to use AI services in support of malicious cyber activities \[[*Details*](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors)\]
21. **Perplexity** partners with **Vercel**, opening AI search to developer apps \[[*Details*](https://venturebeat.com/ai/perplexity-partners-with-vercel-opening-ai-search-to-developer-apps/)\].
22. **Researchers** show that ***LLM agents can autonomously hack websites***, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. The agent does not need to know the vulnerability beforehand \[[*Paper*](https://arxiv.org/html/2402.06664v1)\].
23. **FCC** makes AI-generated voices in unsolicited robocalls illegal \[[*Link*](https://www.msn.com/en-us/money/companies/fcc-bans-ai-voices-in-unsolicited-robocalls/ar-BB1hZoZ0)\].
24. **Slack** adds AI-powered search and summarization to the platform for enterprise plans \[[*Details*](https://techcrunch.com/2024/02/14/slack-brings-ai-fueled-search-and-summarization-to-the-platform/)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks.",/r/StableDiffusion/comments/1asatxy/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,183,16,1695476542.0,16q4ve6,"1. **Genmo** releases a new text-to-video model: **Genmo Replay** v0.1, which generates high-quality videos from text without the need for advanced prompt engineering. *Genmo is available for free to create AI videos* \[[*Details*](https://blog.genmo.ai/log/replay-ai-video) | [Genmo *Replay*](https://www.genmo.ai/)\] .
2. **OpenAI** unveils **DALL·E 3** \- a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training.
3. **Toyota Research Institute** has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexterous behaviors like peeling vegetables, using hand mixers, preparing snacks, and flipping pancakes.
4. **Microsoft** announced:
   1. Availability of AI Copilot for Windows from September 26th. Copilot will incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance. It will be integrated in Windows 11, Microsoft 365, Edge and Bing.
   2. Bing will add support for DALL.E 3 and deliver more personalized answers based on search history.
   3. New AI powered experiences in Paint, Photos and Clipchamp.
   4. New AI-powered shopping experience
5. **ElevenLabs** released **Projects** \- a tool that lets you generate an entire audiobook at the click of a button. Projects now supports .epub, .pdf, and .txt file imports, as well as initializing a project from a URL.
6. **Deci** presents **DeciDiffusion 1.0** \- an open-source text-to-image latent diffusion model which is 3x faster than Stable Diffusion v1.5 with the same quality.
7. **Google researchers** present a new approach that produces photo-realistic animations from a single picture. The model is trained on automatically extracted motion trajectories from a large collection of real video sequences.
8. **Google** has updated Bard\*\]\*:
9. **Bard Extensions:** With extensions, Bard can now connect to your Google apps and services like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels.
10. Users can use Bard’s “Google it” button to more easily double-check its answers and evaluate whether there is content across the web to substantiate it.
11. Bard can now let you continue chat via **shared public links**
12. **YouTube** announces new AI tools for creators. **Dream Screen** will let users create an AI-generated video or image background from text. Automatic AI-dubbing tool called **Aloud**, which will be integrated into YouTube Studio. **AI-powered insights** to generate video ideas and draft outlines. **Assistive Search in Creator Music** where AI will suggest the right music based on your description of your content.
13. **Amazon** announced that its voice assistant Alexa is being upgraded with a new, custom-built large language model.
14. **IBM** open-sources **MoLM** \- a collection of ModuleFormer-based language models ranging in scale from 4 billion to 8 billion parameters. ModuleFormer is a new neural network architecture based on the Sparse Mixture of Experts (SMoE) by IBM researchers. .
15. **Neuralink**, Elon Musk's brain implant startup, set to begin human trials.
16. **Lexica** has released **Aperture v3.5** \- their latest next-gen image model that can create photorealistic images and follows your prompt with precision.
17. **OpenAI** has invited domain experts to collaborate in evaluating and improving the safety of OpenAI's models by joining the new **OpenAI Red Teaming Network**.
18. \*\*GitHub Copilot Chat (\*\*beta) is now available for all individuals.
19. **Replit** announced a virtual hackathon for projects built using **Replit ModelFarm**
20. **Oracle** brings voice-activated AI to healthcare with Clinical Digital Assistant.
21. **Google** and the Department of Defense are building an AI-powered microscope to help doctors spot cancer.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks",/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,learnmachinelearning,r/learnmachinelearning,False,relevant,highly relevant
[R] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace - Yongliang Shen et al Microsoft Research Asia 2023 - Able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results!,Singularian2501,https://www.reddit.com/r/MachineLearning/comments/129qi8p/r_hugginggpt_solving_ai_tasks_with_chatgpt_and/,172,30,1680453563.0,129qi8p,"Paper: [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580) 

Abstract:

>Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which **paves a new way towards AGI.** 

https://preview.redd.it/huc5so9f1ira1.jpg?width=1201&format=pjpg&auto=webp&s=cd714263f8a6ea443195316d95704fd550beee95

https://preview.redd.it/d2dfhs9f1ira1.jpg?width=655&format=pjpg&auto=webp&s=07fcb2b969cdaaf649aed259296f3dfa9157531e

https://preview.redd.it/v4gc9r9f1ira1.jpg?width=773&format=pjpg&auto=webp&s=b014fa679a7bdc2024a3d27690950be2248735aa",/r/MachineLearning/comments/129qi8p/r_hugginggpt_solving_ai_tasks_with_chatgpt_and/,MachineLearning,r/MachineLearning,False,relevant,highly relevant
John Wick as an Anime,Theblasian35,https://v.redd.it/2gg6h89vih6c1,172,26,1702657495.0,18j3wjx,Using text to video primarily,/r/OpenAI/comments/18j3wjx/john_wick_as_an_anime/,OpenAI,r/OpenAI,True,relevant,neutral
For those feeling like SD is a waste of time for making video after seeing news on SORA,Oswald_Hydrabot,https://www.reddit.com/r/StableDiffusion/comments/1ars389/for_those_feeling_like_sd_is_a_waste_of_time_for/,167,88,1708035730.0,1ars389,"Do not forget what makes this community valuable.  Do not forget the value of freedom of expression and the freedom of thought.

&#x200B;

In video demos released by OpenAI today, SORA looks as though it will likely be superior in quality to any existing text to video generative AI tooling available.  There are many factors however that define what ""superiority"" would even mean (or that it may not even be possible to achieve).  

&#x200B;

I say this because I cannot stress enough how important it is that we don't underestimate the value of our community  based on technical quality demos -- of which were released by a monolothic corporation that supports pro-censorship legal policies, and is actively lobbying governments around the world to enact legislation to strip us of the right to freely develop, distribut and use our own unrestricted versions of these echnologies.

&#x200B;

The point I wish to make is simple: the demand for Open Source generative AI is as strong as the demand for legal Encryption, and it is not limited to being in demand by us plebs.  SORA will stand as a testament to the failure of greed and vanity; it will fail to fullfil what it would otherwise be capable of if it were open source, as our community continues to prove and proliferate the benefits of a non-captive alternative to the people we work for and our communities at large.

&#x200B;

Let's consider a generative AI user in China who wishes to make high quality or a high volume of ethical political art as a peaceful demonstration, anonymously, on their own, in criticism or satire of the Chinese government. 

&#x200B;

Let's say they are one person, and they want to use AI to help them scale their message through visual art.  

&#x200B;

How do they pursue peaceful demonstration?

&#x200B;

Are they going to use SORA?  Do you think OpenAI is going to protect genuinely ethical (and important) political activism, and the privacy of communication required of it?  

&#x200B;

Freedom of expression and freedom of thought require freedom of information.  There will never come a time where the value of free thinking will ever be defeated by those seeking any form of Utopianism nor the Tyranny of a wealthy, ruling class.  Progress depends on it, and a failure of progess fails them as much as it fails us.

&#x200B;

One of my favorite observations on the topic of free thinking says it far better than I can myself:

&#x200B;

 .......

&#x200B;

”We have associated the struggle for human equality with the struggle for freedom of knowledge and we have associated them rightly: they belong together. Because the recognition of individual possibility, to allow each to be what she and he can be, rests inherently upon the availability of knowledge; the perpetuation of ignorance is the beginning of slavery. So we are part then of two struggles, whether we like it or not. A struggle for the freedom of thought and a struggle for justice to persons. That the ownership of culture, the commoditization of learning, poses a danger to a movement for equality and economic justice is obvious to all. This is, as Thomas Krueger just pointed out, I think, very eloquently, an inherent part of the problem of globalization, whose sunny side we are. For globalization otherwise means the impoverishment of workers through remorseless competition between the rich and poor parts of humanity. A struggle conducted for the benefit of the shareholders, that is the few, through limitations of knowledge available to the many. Accordingly, we meet the 21th century not as the inventors of something new, but as the latest generation struggling for ideals that are very old.

&#x200B;

What differentiates us from those who have struggeled in the past, as Volker just identified for you, is a change from utopianism to practicality. From the moment at which that movement for freedom of thought and economic--or at least political--equality began to gain momentum in the middle of the 18th century, those struggeling for freedom where condemned to utopianism. The ideals of the American and French revolutions which brought what there was of freedom of thought and equality of persons into being at the end of the 18th century necessarily rested upon hopes, upon dreams and beliefs about what might be possible under conditions of tumultuous and unprecedented transformation.

&#x200B;

The constitution of the United States, as one of its greatest interpreters has pointed out, is an experiment. To Justice Holmes and to all those lawyers and judges who over 200 years struggled to turn that experiment into practical reality much is due. But we must also recognize, as photographs from Iraq have shown this year, that it remains in substantial part a dream subject to political disruption by those operating under the control of power.

&#x200B;

Utopianism has also the heavy drawback that the struggle to perfect a world never before experienced often turns violent as dreams confront unexpected realities and the dreamer has little choice but to lash out against the tyranny of fact. And so the struggle for freedom of thought and the struggle for economic equality has been substantially limited in prior generations by the inevitable reliance upon a dream of a perfect future never experienced before. And it is not insignificant that in all the European languages the phrase--the word--used to designate that perfection, ""Utopia"", means ""nowhere"". For it is, after all, a struggle to achieve what has never been achieved. A struggle to bring about conditions that would allow human beings to be what they have never been: new socialist man; the perfect citizen of the perfect republic. These were noble dreams, and the struggle to achieve them, even at its worst, has a nobility to which we aspire. But we are fortunate because ours is a movement built not on dreams but on actualities. Ours is an ideology of change which depends not on what might be but on what already is”.

&#x200B;

&#x200B;

........

&#x200B;

&#x200B;

“...what is at stake is exactly that moment at which we make learning open. Like the recognition that science itself can be based only upon print that is within the reach of every scientist. In the very same way that western science depended in the 16th century on the movement for freedom of thought--what more noble proposition could we take for our movement than the simple words ""epursi muove"" with which Galileo pointed to the intrinsic relationship between freedom of thought and scientific progress--in the same way that the scientific revolution in the west first depended upon free information exchange, so now. In the next generation we will confront once again the recognition that without a movement for freedom of thought science is tied to ownership. Does anybody who inspects the current pharmaceutical industry or the forthcoming genetic revolution doubt me? Without the free exchange of ideas, science is the servant of inequality. And it is science, the ability to know, the ability to teach, the opportunity to learn everything that any human mind can reason out: it is science which is still at the root of the development of our societies.

&#x200B;

So the movement for free spectrum, like the movement for unlicensed printing, is a movement to put beneath science the power of all the available human minds. Like the war against censorship in western Europe, the war for free spectrum is a war for the freedom of ideas in its most valuable sense: the ideas that changed society extend life, make human existance better. We have grown so accustomed to the idea that the power to communicate with one another is something we have to buy from someone else that we are in danger of forgetting just how much rests over the long history of human beings on the inherent virtue of untrammeled communication.

&#x200B;

So out of those parts, free software, free hardware, free culture and free spectrum, we build a society of justice, of equality, of liberty. Not in the belief that if we somehow force the aristocrats out, later society will become perfect; not out of the belief that there is some class needing liquidation, and then we imagine human beings can change; not a dream about nowhere, but an attempt to move what we have within our appartments, within our work places, within our schools, out into the larger world where it can begin to fulfill its perfectly legitimate, necessary, inevitable work of liberation.

&#x200B;

We have turned the freedom of ideas into an instrument of social change. We have become what all our ancestors have dreamed of becoming. People who can take what is and make it the method of liberation.”

&#x200B;

\-Eben Moglen

&#x200B;

(excerpt from ""Die Gedanken Sind Frei"": Free Software and the Struggle for Free Though, Wizards of OS 3, Opening Keynote [http://moglen.law.columbia.edu/publications/berlin-keynote.html](http://moglen.law.columbia.edu/publications/berlin-keynote.html))

&#x200B;

video of the full speech here: [https://archive.org/details/EbenMoglen-DieGedankenSindFreifreeSoftwareAndTheStruggleForFree](https://archive.org/details/EbenMoglen-DieGedankenSindFreifreeSoftwareAndTheStruggleForFree)

&#x200B;

I see very little cause for concern for anything other than losing hope on what's important.  Stable Diffusion is important, it is valuable, and it is not any less valuable as other tools that are captive to corporate greed imply their superiority.

&#x200B;

They simply aren't.

&#x200B;

&#x200B;

&#x200B;

&#x200B;",/r/StableDiffusion/comments/1ars389/for_those_feeling_like_sd_is_a_waste_of_time_for/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Iron Man Meets His Fans - Text-to-Video AI Generated,Ramenko1,https://v.redd.it/ash21o9ltira1,161,6,1680462946.0,129uzn7,,/r/aigeneratedmemes/comments/129uzn7/iron_man_meets_his_fans_texttovideo_ai_generated/,aigeneratedmemes,r/aigeneratedmemes,True,not relevant,neutral
[GUIDE] ComfyUI AnimateDiff Guide/Workflows Including Prompt Scheduling - An Inner-Reflections Guide (Including a Beginner Guide),Inner-Reflections,https://www.reddit.com/r/StableDiffusion/comments/16w4zcc/guide_comfyui_animatediff_guideworkflows/,158,62,1696075158.0,16w4zcc,"AnimateDiff  in ComfyUI is an amazing way to generate AI Videos. In this Guide I  will try to help you with starting out using this and give you some  starting workflows to work with. My attempt here is to try give you a  setup that gives you a jumping off point to start making your own  videos.

**\*\*WORKFLOWS  ARE ON CIVIT** [**https://civitai.com/articles/2379**](https://civitai.com/articles/2379) **AS WELL AS THIS GUIDE WITH PICTURES\*\***

# System Requirements

A  Windows Computer with a NVIDIA Graphics card with at least 10GB of VRAM  (You can do smaller resolutions or the Txt2VID workflows with a minimum  of 8GB VRAM). Anything else I will try to point you in the right  direction but will not be able to help you troubleshoot. Please note at  the resolutions I am using I am hitting 9.9-10GB VRAM with 2 ControlNets  so that may become an issues if things are borderline.

# Installing the Dependencies

These are things that you need in order to install and use ComfyUI.

1. GIT - [https://git-scm.com/downloads](https://git-scm.com/downloads) \- this lets you download the extensions from GitHub and update your nodes as updates get pushed.
2. (Optional) - [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)  \- this is what combine nodes use to take the images and turn them in a  gif. Installing is a guide in and of itself. I would YouTube how to  install it to PATH. If you do not have this the node will give an error  BUT the workflows still run and you will get the frames
3. 7zip - [https://7-zip.org/](https://7-zip.org/) \- this is to extract the ComfyUI Standalone

# Installing ComfyUI and Animation Nodes

Now let's Install ComfyUI and the nodes we need for Animate Diff!

1. Download ComfyUI either using this direct link: [https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI\_windows\_portable\_nvidia\_cu118\_or\_cpu.7z](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_cu118_or_cpu.7z) or navigate on the webpage: [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI) (If you have a Mac or AMD GPU there is a more complex install guide there).
2. Extract with 7zip Installed above. Please note it does not need to be installed per se just extracted to a target folder.
3. Navigate to the custom nodes part of comfy
4. In  the explorer tab (ie. the box pictured above) click select and type CMD  and then hit enter, you are now should have a command prompt box open.
5. You  are going to type the following commands (you can copy/paste one at a  time) - What we are doing here is using Git (installed above) to  download the node repositories that we want (some can take a while):  

   1. git clone [https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)
   2. git clone [https://github.com/ltdrdata/ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager)
   3. git clone [https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
   4. git clone [https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
   5. For  the ControlNet preprocessors you cannot simply download them you have  to use the manager we installed above. You start by running  ""run\_nvidia\_gpu"" in the ComfyUI\_windows\_portable folder. It will  initialize some of the above nodes. Then you will hit the Manager button  then ""install custom nodes"" then search for ""Auxiliary Preprocessors""  and install ComfyUI's ControlNet Auxiliary Preprocessors.
   6. Similar  to ControlNet preprocesors you need to search for ""FizzNodes"" and  install them. This is what is used for prompt traveling in workflows  4/5. Then close the comfy UI window and command window and when you  restart it will load them.
6. Download  checkpoint(s) and put them in the checkpoints folder. You can choose any  model based on stable diffusion 1.5 to use. For my tutorial download: [https://civitai.com/models/24779?modelVersionId=56071](https://civitai.com/models/24779?modelVersionId=56071) also [https://civitai.com/models/4384/dreamshaper](https://civitai.com/models/4384/dreamshaper). As an aside realistic/midreal models often struggle with animatediff for some reason, except [Epic Realism Natural Sin](https://civitai.com/models/25694/epicrealism) seems to work particularly well and not be blurry. Put 
7. Download VAE to put in the VAE folder. For my tutorial download [https://civitai.com/models/76118?modelVersionId=80869](https://civitai.com/models/76118?modelVersionId=80869) . It is a good general VAE and VAE's do not make a huge difference overall.
8. Download motion modules (original ones are here: [https://huggingface.co/guoyww/animatediff/tree/main](https://huggingface.co/guoyww/animatediff/tree/main) the fine tuned ones can by great like [https://huggingface.co/CiaraRowles/TemporalDiff/tree/main](https://huggingface.co/CiaraRowles/TemporalDiff/tree/main), [https://huggingface.co/manshoety/AD\_Stabilized\_Motion/tree/main](https://huggingface.co/manshoety/AD_Stabilized_Motion/tree/main), or [https://civitai.com/models/139237/motion-model-experiments](https://civitai.com/models/139237/motion-model-experiments) ). For my tutorial download the [original version 2 model](https://huggingface.co/guoyww/animatediff/blob/main/mm_sd_v15_v2.ckpt) and [TemporalDiff](https://huggingface.co/CiaraRowles/TemporalDiff/blob/main/temporaldiff-v1-animatediff.ckpt)  (you could just use one however your final results will be a bit  different than mine). As a note Motion models make a fairly big  difference to things especially with any new motion that AnimateDiff  Makes. So try different ones. Put them in the animate diff node:
9. Download Controlnets and put them in your controlnets folder. [https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main) . For my tutorials you need Lineart, Depth and OpenPose (download bot the pth and yaml files).

You should be all ready to start making your animations!

# Making Videos with AnimateDiff

The  basic workflows that I have are available for download in the top right  of this article. The zip File contains frames from a pre-split video to  get you started if you want to recreate my workflows exactly. There are  basically two ways of doing it. One which is just text2Vid - it is  great but motion is not always what you want. and Vid2Vid which uses  controlnet to extract some of the motion in the video to guide the  transformation.

1. If you are doing Vid2Vid you want to split frames from video (using and editing program or a site like [ezgif.com](http://ezgif.com/))  and reduce to the FPS desired (I usually delete/remove half the frames  in a video and go for 12-15fps). You can use the skip option in the load  images node noted below instead of having to delete them. If you want  to copy my workflows you can use the Input frames I have provided  (please note there are about 115 but I had to reduce to 90 due to file  size restrictions).
2. In the ComfyUI folder run  ""run\_nvidia\_gpu"" if this is the first time then it may take a while to  download an install a few things.
3. To load a workflow  either click load or drag the workflow onto comfy (as an aside any  picture will have the comfy workflow attached so you can drag any  generated image into comfy and it will load the workflow that created  it)
4. I will explain the workflows below, if you want to  start with something I would start with the workflow labeled ""1-Basic  Vid2Vid 1 ControlNet"". I will go through the nodes and what they mean.
5. Run! (this step takes a while because it is making all the frames of the animation at once)

# Node Explanations

Some should be self explanatory, however I will make a note on most.

**Load Image Node**

You  need to select the directory your frames are located in (ie. where did  you extract the frames zip file if you are following along with the  tutorial)

image\_load\_cap will load every frame if it is set to 0,  otherwise it will load however many frames you choose which will  determine the length of the animation

skip\_first\_images will allow you to skip so many frames at the beginning of a batch if you needed to

select\_every\_nth will take every frame at 1, ever other frame at 2, every 3rd frame at 3 and so on if you need it to skip some.

**Load Checkpoint/VAE/AnimateDiff/ControlNet Model**

Each  of the above nodes have a model associated with them. The names of the  models you have and mine are likely not to be exactly the same in each  example. You will need to click on each of the model names and select  what you have instead. If there is nothing there then you have put the  models in the wrong folder (see Installing ComfyUI above).

**Green and Red Text Encode**

Green is your positive Prompt

Red is your negative Prompt

They are this color not because they are special but because they are set to be this color by right clicking them FYI.

**Uniform Context Options**

The  uniform context options is new and basically what sets up unlimited  context length. Without it animate diff is only able to do up to 24 (v1)  or 36 (v2) frames at once. What it is doing is basically chaining and  overlapping runs of AD together to smooth things out. **The total length of the animation are determined by the number of frames the loader is fed in NOT context length.** The loader figures out what to do based on the options which mean as follows. The defaults are what I used and are pretty good.

context  length - this is the length of each run of animate diff. If you deviate  too far from 16 your animation won't look good (is a limitation of  animatediff can do). Default is good here for now

context overlap -  is how much overlap each run of animate diff is overlapped with the  next (ie. it is running frames 1-16 and then 12-28 with 4 frames  overlapping to make things consistent)

closed loop - selecting this will try to make animate diff a looping video, it does not work on vid2vid

context  stride - this is harder to explain. At 1 it is off. More than this what  it trys to do is make a single run of AD through the entire animation  and then fill in the frames. The idea is to make the whole animation  more consistent by making a framework and then filling in the  intermediate frames. However in practice I do not find it helps a whole  lot right now. Using it will significantly increase the length of time  it takes to run as it using it means more runs of AnimateDiff.

**Batch Prompt Schedule**

This is the new kid on the block. The prompt Scheduler from FizzNodes.

pre\_text - text to be put before the prompt (so you don't have to copy and paste a large prompt for each change)

app\_text - text to be put after the prompt

The  main text box works in the context ""frame number"": ""prompt"", (note the  last prompt does not have a comma and will give you an error if you put  one at the end of your list). It will blend between prompts so if you  want to have it held I suggest you put it in twice, once where you want  it to start and once where you want it to end.

There is  much more fancy stuff to do with this node (you can make an individual  term change with time). Documentation of this is at [https://github.com/FizzleDorf/ComfyUI\_FizzNodes](https://github.com/FizzleDorf/ComfyUI_FizzNodes). This is what the pw... stuff is for.

**KSampler**

This is the KSampler - essentially this is stable diffusion now that we have loaded everything needed to make the animation.

Steps - These matter and you need more than 20. 25 is the minimum but people do see better results with going higher.

CFG - Feels free to increase this past you normally would for SD

Sampler - Samplers also matter Euler\_a is good but Euler is bad at lower steps. Feel free to figure out a good setting for these

Denoise  - Unless you are doing Vid2Vid keep this at one. If you are doing  Vid2Vid you can reduce this to keep things closer to the original video 

**AnimateDiff Combine Node**

For  the Combine node it creates a gif by default. Do know that gifs look a  lot worse than individual frames so even if the gif does not look great  it might look great in a video.

frame\_rate - frame rate of the gif

loop\_count - number of loops to do before stopping. 0 is infinite looping

format - changes what to make gif/mp4 etc

pingpong - will make the video go through all the frames and then back instead of one way

save  image - saves a frame of the video (because the video does not contain  the metadata this is a way to save your workflow if you are not also  saving the images)

# Workflow Explanations

1. Basic Vid2Vid 1 ControlNet - This is the basic Vid2Vid workflow updated with the new nodes.
2. Vid2Vid  Multi-ControlNet - This is basically the same as above but with 2  controlnets (different ones this time). I am giving this workflow  because people were getting confused how to do multicontrolnet.
3. Basic  Txt2Vid - this is a basic text to video - once you ensure your models  are loaded you can just click prompt and it will work. Do note there is a  number of frame primal node that replaces the load image node and no  controlnets. Do know I don't do much txt2vid so this produces and  acceptable output but nothing stellar.
4. Vid2Vid with  Prompt Scheduling - this is basically Vid2Vid with a prompt scheduling  node. This is what I used to make the video for Reddit. See above  documentation of the new node.
5. Txt2Vid with Prompt Scheduling - Basic text2img with the new prompt scheduling nodes.

# What Next?

* Change  the video input for vid2vid (obviously)! There are some new nodes that  can separate video directly into frames. See Load video nodes - this  node is relatively new.
* Change around the parameters!!
* The stable diffusion checkpoint and denoise strength on the KSampler make a lot of difference (for Vid2Vid).
* You  can add/remove control nets or change the strength of them. If you are  used to doing other stable diffusion videos I find that you need much  less ControlNet strength than with straight up SD and you will get more  than just filter effects. I would also suggest trying openpose.
* Try the advanced K sampler
* Try to add loras
* Try Motion loras: [https://civitai.com/models/153022?modelVersionId=171354](https://civitai.com/models/153022?modelVersionId=171354)
* Use a 2nd ksampler to hires fix (some further good examples can be found on the Kosinkadink's animatediff GitHub [https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)).
* Use  masking or regional prompting (this likely will be a separate guide as  people are only starting to do this at the time of this guide).

With these basic workflows adding what you want should be as simple as adding or removing a few nodes. I wish you luck!

# Troubleshooting

As  things get further developed this guide is likely to slowly go out of  date and some of the nodes may be depreciated. That does not mean that  they won't necessarily work. Hopefully I will have the time to make  another guide or somebody else will.

If you are getting Null type errors make sure you have a model loaded in each location noted above.

If  you already use ComfyUI for other things there are several node repos  that conflict with the animation ones and can cause errors.

# In Closing

I hope you enjoyed this tutorial. If you did enjoy it please consider subscribing to my YouTube channel ([https://www.youtube.com/@Inner-Reflections-AI](https://www.youtube.com/@Inner-Reflections-AI)) or my Instagram/Tiktok ([https://linktr.ee/Inner\_Reflections](https://linktr.ee/Inner_Reflections) )

If  you are a commercial entity and want some presets that might work for  different style transformations feel free to contact me on Reddit or on  my social accounts.

If you are would like to collab on something or have questions I am happy to be connect on Reddit or on my social accounts.

If  you’re going deep into Animatediff, you’re welcome to join this Discord  for people who are building workflows, tinkering with the models,  creating art, etc.

[https://discord.gg/hMwBEEF5E5](https://discord.gg/hMwBEEF5E5)",/r/StableDiffusion/comments/16w4zcc/guide_comfyui_animatediff_guideworkflows/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
Tools List,theaiguru,https://i.redd.it/l5fw5ltbvk2b1.png,147,11,1685282079.0,13u11d3,,/r/coolguides/comments/13u11d3/tools_list/,coolguides,r/coolguides,False,not relevant,neutral
List of Stable Diffusion research softwares that I don't think gotten widespread adoption.,ninjasaid13,https://www.reddit.com/r/StableDiffusion/comments/18fhweg/list_of_stable_diffusion_research_softwares_that/,149,19,1702254952.0,18fhweg,"[https://github.com/omerbt/TokenFlow](https://github.com/omerbt/TokenFlow) \-

[TokenFlow](https://preview.redd.it/60d5j6gbti5c1.png?width=960&format=png&auto=webp&s=255216625d74c1fa8e6cd3ecc20d1ec64c0e5dde)

The work presents a text-driven video editing framework using a text-to-image diffusion model, achieving high-quality results without the need for training or fine-tuning. The Problem is that this requires too much VRAM, I was hoping that the community would reduce the requirements.

[https://github.com/williamyang1991/Rerender\_A\_Video](https://github.com/williamyang1991/Rerender_A_Video) \-

[Rerender A Video](https://preview.redd.it/flpe84f8ti5c1.png?width=759&format=png&auto=webp&s=ea25bbb5f47f439e350ada0caceff852fd0bce84)

The work introduces a novel zero-shot text-guided video-to-video translation framework that adapts large text-to-image diffusion models for video generation, ensuring temporal consistency and achieving high-quality, temporally-coherent videos without re-training or optimization. This requires about 24GB VRAM, I hope the requirements was reduced by this time.

[https://github.com/AILab-CVC/SEED](https://github.com/AILab-CVC/SEED) \-

This work provides an LLM interface to a image generation model similar to dalle3 but without the enhanced prompt alignment.

[https://github.com/RQ-Wu/LAMP](https://github.com/RQ-Wu/LAMP)

[Lamp - \\""A horse runs on Mars\\""](https://preview.redd.it/m8xo8a9iti5c1.png?width=512&format=png&auto=webp&s=d9b242a39bda7d1a32e380ebf0dd771bd3505d12)

LAMP is a few-shot-based method for text-to-video generation. You only need 8\~16 videos 1 GPU (> 15 GB VRAM) for training!! Then you can generate videos with learned motion pattern.

[https://github.com/sled-group/CycleNet?tab=readme-ov-file](https://github.com/sled-group/CycleNet?tab=readme-ov-file)

[CycleNet - Summer to Winter](https://preview.redd.it/m9tl93dlti5c1.png?width=1263&format=png&auto=webp&s=6f30f29b91f4d993733720e291b9181485bebe7a)

Cyclenet, is an approach that enhances diffusion models for image-to-image translation by incorporating cycle consistency, demonstrating superior translation consistency and quality across different tasks and datasets with minimal training data and computational resources.

[https://github.com/dvlab-research/RIVAL](https://github.com/dvlab-research/RIVAL)

[RIVAL](https://preview.redd.it/1yxpdmf6ui5c1.png?width=3102&format=png&auto=webp&s=2a62f237cdacd9d1f932666855c84482f18cbbe3)

RIVAL is a new inference pipeline that enhances the generation quality of image variations from a single exemplar using diffusion models by addressing latent distribution gaps, achieving superior results in semantic-condition similarity and perceptual quality without requiring further parameter optimization.

[https://github.com/showlab/MotionDirector](https://github.com/showlab/MotionDirector)

[MotionDirector](https://preview.redd.it/14vnng7toj5c1.png?width=1137&format=png&auto=webp&s=42d88f071d460c43bcef93a8a4e027aa9bdd99ef)

MotionDirector is a method for customizing motion in video generation, utilizing a dual-path Low-Rank Adaptations (LoRAs) architecture to separate appearance and motion learning, enabling the generation of videos with diverse appearances for customized motions and supporting various applications like mixing different videos and animating a single image with customized motions.

[https://yingqinghe.github.io/scalecrafter/](https://yingqinghe.github.io/scalecrafter/)

[ScaleCrafter](https://reddit.com/link/18fhweg/video/2s6jeo97pj5c1/player)

The study proposes a method for generating high-resolution images with arbitrary aspect ratios using pre-trained diffusion models, addressing issues like object repetition and unreasonable structures through dynamic convolutional adjustment, achieving state-of-the-art results without the need for training or optimization, and suggesting the direct use of low-resolution pre-trained models for ultra-high-resolution visual generation. Basically an arbitrary upscaler that works on videos.

[https://pengbo807.github.io/conditionvideo-website/](https://pengbo807.github.io/conditionvideo-website/)

[ConditionVideo](https://reddit.com/link/18fhweg/video/n3d9kodbpj5c1/player)

ConditionVideo is a training-free text-to-video generation approach that disentangles motion into condition-guided and scenery components, utilizing off-the-shelf text-to-image models and showcasing superior performance in frame consistency, clip score, and conditional accuracy. Basically video controlnet, however I expect something like SpareCtrl from Animatediff and Controlnet Authors to suplant this when it's released.

[https://github.com/Vchitect/SEINE](https://github.com/Vchitect/SEINE)

[SEINE](https://preview.redd.it/oyye4s7fpj5c1.png?width=1136&format=png&auto=webp&s=c61ca64f66983ecf3e633b8f0b597b929f695544)

This paper introduces SEINE, a short-to-long video diffusion model focused on generative transition and prediction, enabling the creation of high-quality long videos with creative and smooth transitions between scenes, controlled by textual descriptions, and demonstrating effectiveness across various tasks such as image-to-video animation and autoregressive video prediction. It's based off of 1.4 and is cheap to generate videos, there's also a txt2vid version of this.

[https://github.com/garibida/cross-image-attention](https://github.com/garibida/cross-image-attention)

[Cross Image Attention](https://preview.redd.it/yt6m2gtipj5c1.png?width=2196&format=png&auto=webp&s=6b2ccb7b2045647b4ddbf66aa15d6e409798cc1a)

The method utilizes text-to-image generative models to seamlessly transfer visual appearance between semantically similar but differently shaped objects, employing cross-image attention and three enhancement mechanisms for robust and effective results without requiring training.

[https://github.com/aihao2000/stable-diffusion-reference-only](https://github.com/aihao2000/stable-diffusion-reference-only)

[SD Reference-Only](https://preview.redd.it/ewi97cznpj5c1.png?width=1536&format=png&auto=webp&s=915a2b3b3eeb6c6570db7eb7419068ee0635eea2)

Stable Diffusion Reference Only is a self-supervised image-to-image model that streamlines and accelerates secondary painting in animation and comics by efficiently controlling generation with two types of conditional images, achieving state-of-the-art results and improving production efficiency.

[https://github.com/ziqihuangg/ReVersion](https://github.com/ziqihuangg/ReVersion)

[ReVersion](https://preview.redd.it/2z5ex3gr5k5c1.png?width=1969&format=png&auto=webp&s=cca379f0f89c9e765ad32a616162e760f3f78a19)

ReVersion is a new method that, by learning from exemplar images, effectively captures and generates relation-specific images, emphasizing interactions between objects rather than appearances, surpassing existing methods on diverse visual relations. The problem is that it requires alot of VRAM to train, what I'm hoping is that the requirements have been reduced by now.

[https://github.com/rohitgandikota/sliders](https://github.com/rohitgandikota/sliders)

[Sliders](https://preview.redd.it/yez78qiu5k5c1.png?width=2734&format=png&auto=webp&s=4e0037ae095a6fe80964340e936cad58834a9c0f)

The method introduces interpretable concept sliders for precise control over attributes in image generations from diffusion models, efficiently creating sliders for textual or visual concepts that exhibit stronger targeted edits with lower interference compared to previous techniques, demonstrating applications in weather, age, styles, expressions, and addressing quality issues in Stable Diffusion XL.

[https://github.com/mkshing/ziplora-pytorch](https://github.com/mkshing/ziplora-pytorch)

[ZipLoRA](https://preview.redd.it/c4o7sbzx5k5c1.png?width=3702&format=png&auto=webp&s=c0ccf96f6d962d3e52108a0e0d3d5ebca1db36cc)

ZipLoRA is introduced as an effective method for merging independently trained style and subject low-rank adaptations (LoRAs), offering improved generation results for user-provided subjects and styles with meaningful enhancements in subject and style fidelity compared to existing techniques.

[https://github.com/Picsart-AI-Research/Specialist-Diffusion](https://github.com/Picsart-AI-Research/Specialist-Diffusion)

[Specialist Diffusion](https://preview.redd.it/c5s80iaz5k5c1.png?width=1119&format=png&auto=webp&s=a9c48a87a97c337edf67c4e1260593523246a4f4)

Specialist Diffusion is an effective plug-and-play personalized text-to-image model that outperforms other diffusion model personalization techniques, showcasing superior performance in learning sophisticated styles with ultra-sample-efficient tuning compared to alternatives like Textual Inversion and DreamBooth.

[https://github.com/PRIS-CV/DemoFusion](https://github.com/PRIS-CV/DemoFusion)

[DemoFusion](https://preview.redd.it/yk8ucm636k5c1.png?width=1999&format=png&auto=webp&s=ce692610f86b631fe79540538616de395cf70b1d)

The work presents DemoFusion, a framework designed to make high-resolution image generation with Generative Artificial Intelligence (GenAI) more accessible by improving existing models and enabling rapid prompt iteration.

[https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main](https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main)

[LEDITS++](https://reddit.com/link/18fhweg/video/elk39np66k5c1/player)

LEDITS++ is an efficient, versatile, and precise textual image manipulation technique that overcomes limitations of existing methods, offering high-fidelity results without the need for fine-tuning or optimization, supporting multiple simultaneous edits, and incorporating a novel implicit masking technique.

All of the codes for these software is released.",/r/StableDiffusion/comments/18fhweg/list_of_stable_diffusion_research_softwares_that/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"Furniture Store TV Advertisement, text to video, runway gen-2",ZashManson,https://v.redd.it/sjhgef20xu1b1,149,16,1684949859.0,13qs4tr,Source ➡️ https://twitter.com/erik__schmitt/,/r/aivideo/comments/13qs4tr/furniture_store_tv_advertisement_text_to_video/,aivideo,r/aivideo,True,relevant,neutral
[Shiaman] Tried using Text-to-Video AI to generate a @BlueJays commercial and it...um...did not go well....,Stupendous_man12,https://twitter.com/joshshiaman/status/1666615968024391686?s=46&t=d-EUNz0EDYxow8ocmuZj6Q,151,69,1686192235.0,143x1mr,,/r/Torontobluejays/comments/143x1mr/shiaman_tried_using_texttovideo_ai_to_generate_a/,Torontobluejays,r/Torontobluejays,False,relevant,neutral
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/StableDiffusion/comments/17xh5uh/this_week_in_ai_all_the_major_ai_developments_in/,144,15,1700233802.0,17xh5uh,"1. **Meta AI** introduces:
   1. **Emu Video:** new text-to-video model that leverages Meta’s Emu image generation model and can respond to text-only, image-only or combined text & image inputs to generate high quality video \[[Link](https://emu-video.metademolab.com/)\].
   2. **Emu Edit:** This new model is capable of free-form editing through text instructions. Emu Edit precisely follows instructions, ensuring that pixels in the input image unrelated to the instructions remain untouched \[[Link](https://emu-edit.metademolab.com/)\].
2. Researchers present **LLaVA-Plus**, a general-purpose multimodal assistant that expands the capabilities of large multimodal models. LLaVA-Plus maintains a skill repository that contains a wide range of vision and vision-language pre-trained models (tools), and is able to activate relevant tools, given users’ multimodal inputs, for performing real-world tasks .
3. **Google Deepmind** in collaboration with **YouTube** announce \[[Link](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation)\] :
   1. **Lyria**, a model that excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the output’s style and performance.
   2. **Dream Track**: an experiment in YouTube Shorts. Users can simply enter a topic and choose an artist from the carousel to generate a 30 second soundtrack for their Short. Using the Lyria model, Dream Track simultaneously generates the lyrics, backing track, and AI-generated voice in the style of the participating artist selected.
   3. **Music AI tools:** Users can create new music or instrumental sections from scratch, transform audio from one music style or instrument to another, and create instrumental and vocal accompaniments. *Louis Bell, Producer/Songwriter, builds a track with just a hum \[video\]:*
4. **Meta AI** released ***new stereo models for MusicGen***. By extending the delay codebook pattern to cover tokens from both left & right channels, these models can generate stereo output with no extra computational cost vs previous models **\[**[***Hugging face***](https://huggingface.co/spaces/facebook/MusicGen) ***|***[***Paper***](https://arxiv.org/abs/2306.05284) **\].**
5. **SiloGen** announced ***Poro***, an open-source 34 billion parameter LLM for English, Finnish and code. Future releases to support other European languages. Poro is freely available for both commercial and research use
6. **Alibaba Cloud** introduced ***Qwen-Audio***, an open-source multi-task audio-language model that supports various tasks, languages, and audio types, serving as a universal audio understanding model.
7. Researchers present **JARVIS-1**, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control in Minecraft.
8. **Microsoft** announced:
   1. ***Microsoft Copilot Studio*****: a** low-code tool to quickly build, test, and publish standalone copilots and custom GPTs.
   2. ***Windows AI Studio*** to enable developers to fine-tune, customize and deploy state-of-the-art small language models, for local use in their Windows apps. In the coming weeks developers can access Windows AI Studio as a *VS Code Extension*.
   3. ***Microsoft Azure Maia*****:** Custom-designed chip optimized for large language models training and inference.
   4. ***Text to speech avatar*** feature in Azure AI Speech to create synthetic videos of a 2D photorealistic avatar speaking.
   5. The addition of ***40 new models*** ***to the Azure AI model*** catalog including Mistral, Phi, Jais, Code Llama, NVIDIA Nemotron.
9. **Redwood Research**, a research lab for AI alignment, has unveiled that large language models (LLMs) can master “encoded reasoning,” a form of steganography. This allows LLMs to subtly embed intermediate reasoning steps within their generated text in a way that is undecipherable to human reader.
10. **Microsoft Research** introduced ***phi-2*** \- at 2.7B size, phi-2 is much more robust than phi-1.5 with improved reasoning capabilities.
11. **Forward Health** announced ***CarePods***, a self-contained, AI-powered doctor’s office. CarePod users can get their blood drawn, throat swabbed and blood pressure read, all without a doctor or nurse. Custom AI powers the diagnosis, and behind the scenes, doctors write the appropriate prescription**.**
12. **You.com** launched ***YOU API*** to connect LLMs to the web. The API is launching with three dedicated endpoints: Web Search, News and RAG.
13. **Notion** announced ***Q&A***, an AI assistant that provides answers using information from a Notion workspace.
14. **OpenAI** has paused new ChatGPT Plus sign-ups due to the surge in usage post devday.
15. **Together.ai** announced ***Together Inference Engine*** that up to 2x faster than other serverless APIs (eg: Perplexity, Anyscale, Fireworks AI, or Mosaic ML.
16. Researchers in China have developed an ***AI-powered robot chemist*** that might be able to extract oxygen from water on Mars. The robot uses materials found on the red planet to produce catalysts that break down water, releasing oxygen.
17. **Nvidia** announced **H200 GPU** that features 141GB of memory at 4.8 terabytes per second, nearly double the capacity and 2.4x more bandwidth compared with its predecessor, the NVIDIA A100.

Source: AI Brews - you can subscribe [here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks",/r/StableDiffusion/comments/17xh5uh/this_week_in_ai_all_the_major_ai_developments_in/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"How the release of ChatGPT sparked evolutions in Movies, Video, Photography, Music & Gaming. We are 4 months in...",lostlifon,https://www.reddit.com/r/ChatGPT/comments/123hjap/how_the_release_of_chatgpt_sparked_evolutions_in/,146,54,1679909303.0,123hjap,"When AI first got big, I thought it would have the biggest impact on customer service mainly, obviously I was wrong. There's been a lot of growth in the 'content' industries. Here’s how the landscape has changed in just the last 4 months alone

(I'm not affiliated with any company or tool)

# Movies

* Like having a whole ass studio in your pocket, this tool animates, changes lighting and adds characters into a live scene. I can honestly imagine small teams creating entire movies on par with hollywood movies using something like this. In saying that, I’m no expert so if I’m wrong let me know! \[[Link](https://wonderdynamics.com/)\]
* A group of guys recorded some scenes with a green screen and used AI to transform them into an anime and it honestly does not look bad at all \[[Link](https://www.youtube.com/watch?v=_9LX9HSQkWo)\]. Famous animator Aaron Blaise who’s worked on Lion King, Aladdin and other Disney movies talks about it here \[[Link](https://www.youtube.com/watch?v=xm7BwEsdVbQ)\]. A very interesting watch
* Stable Diffusion over a movie can make it look smoother but does it make it look better? Not sure tbh \[[Link](https://twitter.com/heyBarsee/status/1628656576687906818?s=20)\]
* A new way to edit and change movies, imagine we could edit movies ourselves. Flawless AI is big \[[Link](https://www.flawlessai.com/)\]

# Video

* Gen-2 is probably the best text-to-video tool but isn't available yet \[[Link](https://twitter.com/yining_shi/status/1637840817963278337)\]
* Coke used AI in a new ad. Most probably using Dall-E since they have a deal with OpenAI \[[Link](https://www.youtube.com/watch?v=VGa1imApfdg)\]
* Create videos of yourself talking. This will get to a point where we cant tell the difference between real and AI eventually \[[Link](https://twitter.com/christianortner/status/1639360983192723474)\]
* Edit real life videos with just text \[[Link](https://twitter.com/bilawalsidhu/status/1638919452392583169?s=20)\]
* Text-to-room. Generate rooms by describing what you want \[[Link](https://twitter.com/_akhaliq/status/1638380868526899202?s=20)\]. This has been done with images - take a picture of your room and use AI to see how it looks in different designs \[[Link](https://www.roomgpt.io/)\]
* New text to video model \[[Link](https://twitter.com/_akhaliq/status/1638194089504940032)\]

# Photography

* All the image generation tools - Midjourney v5 seems to be the best for now. Like this image of the pope that looks pretty realistic \[[Link](https://twitter.com/growing_daniel/status/1639810541547061250)\]. Also Trump used an AI image to show him praying in a church which I found pretty funny \[[Link](https://twitter.com/paleofuture/status/1638969629409431552)\]
* This thread of AI generated images insane. It depicts an earthquake that never happened but it looks so realistic its impossible to tell without searching up if it was an actual event \[[Link](https://twitter.com/venturetwins/status/1640038880325009408)\]
* Levi’s is using AI to ‘increase diversity’. A great way to look diverse without actually hiring or paying any diverse models \[[Link](https://twitter.com/icreatelife/status/1640222895203819520)\]. This will be picked up by so many companies in the near future.
* An indiepreneur has built both a professional headshot tool as well as a whole ass AI photo studio and modelling agency. You can generate thousands of images in different poses, locations, different styles of shot, lighting - its crazy. \[[Link](https://www.headshotpro.com/)\] \[[Link](https://www.deepagency.com/)\]. I don't see why companies would hire real people when this will only get better and will be so much cheaper.

# Music

* This guy made Kanye rap a song that doesn’t exist and it sounds pretty real \[[Link](https://twitter.com/rpnickson/status/1639813074176679938)\]
* David Guetta used an AI Eminem and was amazed at how well it works \[[Link](https://twitter.com/davidguetta/status/1621605376733872129)\]
* Kaiber lets you create entire music videos using AI \[[Link](https://www.kaiber.ai/)\]
* I remember reading about AI being used to create one of Niki Minaj’s music videos but couldn’t find the link :/. From memory they even cited a prompt engineer for it. If you find the link lmk and I'll edit it in.

# Books

* Someone actually wrote a book with gpt4. Its pretty meh but its actually a full blown book. How will this look with gpt5 or gpt6? \[[Link](https://old.reddit.com/r/ChatGPT/comments/120oq1x/i_asked_gpt4_to_write_a_book_the_result_echoes_of/)\]. So many people are gona use this to print trash ass books and sell them on amazon, pretty sure people are already doing this

# Games

* A 3D artist talks about how his job has changed since Midjourney came out. He can now create a character in 2-3 days compared to weeks before. He hates it but even admits it does a better job than him. It's honestly sad to read because I imagine how fun it is for them to create art. This is going to affect a lot of people in a lot of creative fields \[[Link](https://www.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/)\]
* text-to-videogame worlds. Literally write stuff like sidewalks, trees, cables, buildings and it adds them to a world, looks so cool. I can’t wait to see where this one goes \[[Link](https://twitter.com/OpusAIInc/status/1613478888033640449)\]
* text-to-level generation - Generate and play a Mario level. So many use cases across so many games for this \[[Link](https://huggingface.co/spaces/multimodalart/mariogpt)\]
* Create custom cutscenes in Fortnite Creative using just your phone. MoveAI is doing crazy work \[[Link](https://twitter.com/JSFILMZ0412/status/1639349544789118977)\]
* AI dungeon lets you play AI generated stories. Atm its just text but when something like this is integrated with images, video and audio, its going to be crazy \[[Link](https://play.aidungeon.io/main/home)\]
* Someone made a cityscape with AI then ported it to VR using Chatgpt \[[Link](https://twitter.com/ClaireSilver12/status/1621960309220032514?s=20)\]
* Leonardo AI creates gaming assets like environments, art, buildings and characters and they look stunning. I wonder if this will have a big impact on actual game development times though. If you know about this stuff let us know in the comments pls :) \[[Link](https://leonardo.ai/)\]
* Soon we’ll be able to make 3d animations using AI \[[Link](https://twitter.com/icreatelife/status/1639698659808886786?s=20)\]
* Someone hooked up gpt4 to Blender and you can use it to apply certain materials and colours to specific objects and a lot more. Looks pretty cool \[[Link](https://twitter.com/gd3kr/status/1638149299925307392)\]
* NPC's can use AI to talk to you. A new level of immersion in gaming is coming \[[Link](https://www.youtube.com/watch?v=DnF4WzM5LPU)\]

# Bonus

* Unreal Engine 5 will let you create crazy animations with your iPhone. Genuinely worth watching \[[Link](https://twitter.com/ImMachineAlpha/status/1639365174762041344?s=20)\]
* Bing can look up latest designs in watches then actually generate a new watch that does not look bad at all \[[Link](https://twitter.com/emollick/status/1639094707795165184?s=20)\]
* Which image is AI generated? I actually got it wrong the first time.. \[[Link](https://twitter.com/javilopen/status/1638284357931528192?s=20)\]
* A company will let you re-watch and relive your memories. For better or worse \[[Link](https://twitter.com/WistLabs/status/1628129368760516609?s=20)\]
* Stable Diffusion Infinite zoom-in and zoom-out is trippy \[[Link](https://twitter.com/icreatelife/status/1637543157872115724)\]
* Luma Labs AR tech is insane and works from your phone \[[Link](https://twitter.com/liorazi/status/1638247764717891609)\]
* Create your own story with you as the protagonist. So much potential for this with kids \[[Link](https://magicstory.ai/)\]

&#x200B;

I write about the latest news & advancements in AI for my newsletter. Have a read if you'd like to stay in the know :) \[[Link](https://nofil.beehiiv.com/subscribe)\]",/r/ChatGPT/comments/123hjap/how_the_release_of_chatgpt_sparked_evolutions_in/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
"Spaghetti Terror, text to video, runway gen-2",ZashManson,https://v.redd.it/vhyvl6r98gxa1,143,9,1683028653.0,135izsn,Coming through Twitter https://twitter.com/andymac3d/,/r/aivideo/comments/135izsn/spaghetti_terror_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,less relevant
"1980s VHS sci-fi horror action film, text to video, runway gen-2",ZashManson,https://v.redd.it/gruy5upyav5b1,134,14,1686697954.0,148sedj,,/r/aivideo/comments/148sedj/1980s_vhs_scifi_horror_action_film_text_to_video/,aivideo,r/aivideo,True,relevant,less relevant
I was fortunate enough to be an early access tester for Runway Gen-2. I had no idea text-to-video could do this.,DunMiff--Sys,https://www.youtube.com/watch?v=94SZNRTL4IA,129,35,1682735487.0,132gysn,,/r/singularity/comments/132gysn/i_was_fortunate_enough_to_be_an_early_access/,singularity,r/singularity,False,relevant,highly relevant
"Walk through hell, text to video, runway gen-2",ZashManson,https://v.redd.it/5raf8iwa7o3b1,126,13,1685740475.0,13ypha1,,/r/aivideo/comments/13ypha1/walk_through_hell_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,less relevant
Man Shreds On Guitar Atop Erupting Volcano - Text-to-Video AI Generated,Ramenko1,https://v.redd.it/7c6ywddescra1,125,7,1680390108.0,1291ypm,,/r/aigeneratedmemes/comments/1291ypm/man_shreds_on_guitar_atop_erupting_volcano/,aigeneratedmemes,r/aigeneratedmemes,True,not relevant,neutral
New 1.2B parameter text to video model is out: Latent Video Diffusion Models for High-Fidelity Long Video Generation,DonOfTheDarkNight,https://www.reddit.com/r/singularity/comments/12cxo7g/new_12b_parameter_text_to_video_model_is_out/,121,20,1680728387.0,12cxo7g,"* Have you ever wanted to create a video based on a text description? If so, you might be interested in VideoCrafter, an open-source video generation and editing toolbox for crafting video content based on text descriptions.
* VideoCrafter is a project developed by researchers from Tsinghua University, Microsoft Research Asia, and Peking University. It aims to provide an easy-to-use and flexible toolkit for text-to-video generation and editing, which can be useful for various applications such as education, entertainment, and communication.
* VideoCrafter currently includes three types of models: Base T2V for generic text-to-video generation, VideoLoRA for personalized text-to-video generation with LoRA, and VideoControl for video generation with more condition controls. Each model type has its own features, advantages, and limitations, which are explained in detail on the project page and GitHub repository.
* To use VideoCrafter, you need to install some dependencies, download some pretrained models and data, and run some commands. The instructions for setup and inference are provided on the GitHub repository, along with some examples and gallery of results for each model type. You can also watch some videos on YouTube that demonstrate how VideoCrafter works and what it can do. Check out their github below:

[https://github.com/VideoCrafter/VideoCrafter](https://github.com/VideoCrafter/VideoCrafter)

Project page: [https://yingqinghe.github.io/LVDM/](https://yingqinghe.github.io/LVDM/)  


Edit: Thanks  [/u/Starshot84](https://www.reddit.com/u/Starshot84)  for giving me my first award on Reddit!",/r/singularity/comments/12cxo7g/new_12b_parameter_text_to_video_model_is_out/,singularity,r/singularity,False,relevant,highly relevant
Free text-to-video add-on for Blender: https://github.com/tin2tin/Generative_AI,tintwotin,https://v.redd.it/1d86m052gsza1,122,10,1684066452.0,13hank2,,/r/StableDiffusion/comments/13hank2/free_texttovideo_addon_for_blender/,StableDiffusion,r/StableDiffusion,True,not relevant,highly relevant
"Melting Wax World, text to video, runway gen-2",ZashManson,https://v.redd.it/cebbtpii172b1,122,14,1685096677.0,13s9a3l,Source ➡️ https://twitter.com/IXITimmyIXI/,/r/aivideo/comments/13s9a3l/melting_wax_world_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Open Source Make-A-Video with SD,Square365,https://i.redd.it/79peqzwhl2ya1.gif,119,11,1683299450.0,138rnsq,"[Discord Relay]
We are happy to announce an update on the progress of our text-to-video model, TempoFunk 

# What is it?

TempoFunk is a LDM for **text-to-video generation**.  🎞️ 
We grab a pretrained LDM model, ***Stable Diffusion*** , freeze it and add additional temporal layers, which are then trained on videos. Temporal attention is purely self attention and also separately attends to time and space. This practice is similar to those of *Meta's Make-A-Video* and *Nvidia's VideoLDM*

Although we used Stable Diffusion 1.5 as a base, **you can extract and apply the additional layers to other models** based on the `1.x` arch. such as Anything V3 or others.

# ✨ What's new

We released new checkpoints trained on TPU clusters provided by HuggingFace 🤗, and trained for `788,600` accumulated steps .

# Try it out on HF Spaces!

https://huggingface.co/spaces/TempoFunk/makeavid-sd-jax

Give it a prompt, and if you want an initial image, and in less than a minute you will get a full animation.
* If you like it, please consider giving it a like! That can help us win the HF Sprint! *

# 💾 Datasets Used

Additionally if you are interested the datasets used are available here:

Caption-URL Dataset (DB): https://huggingface.co/datasets/TempoFunk/map 📔 

Pre-encoded Metadata-Latents: https://huggingface.co/datasets/TempoFunk/small & https://huggingface.co/datasets/TempoFunk/tempofunk-sdance 🔖",/r/StableDiffusion/comments/138rnsq/open_source_makeavideo_with_sd/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
"Doors in the Woods, text to video, runway gen-2",ZashManson,https://v.redd.it/janqtjwo3r3b1,116,5,1685775392.0,13z408s,,/r/aivideo/comments/13z408s/doors_in_the_woods_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
I've checked out over 1000+ AI tools for my directory. Here are the productivity ones that personally use.,clonefitreal,https://www.reddit.com/r/ChatGPTPromptGenius/comments/152s7nt/ive_checked_out_over_1000_ai_tools_for_my/,111,17,1689667802.0,152s7nt,"Tons of business workload made me stress all the time. It urges me to try and test several new AI tools and find out a list to show you.

Now, here are 10 AI tools that will totally upgrade my business this year. Trust me, they'll boost your productivity by 50-80%. And they'll save you countless hours too. Check them out below.

**Chatbot → ChatGPT 4 →** [**https://chat.openai.com**](https://chat.openai.com/**)

Introducing ChatGPT, more than just your ordinary chatbot. It's like conversing with a human but with an AI twist. What sets it apart is its ability to assist with various tasks, such as drafting emails or essays, and even helping with coding. Always ready to serve, ChatGPT acts as a personal helper and is freely accessible.

**Convert text to video → i**[**nvideo.io**](http://invideo.io/)

InVideo is a fantastic do-it-yourself (DIY) video creation platform that allows you to bring your ideas to life in real time. With InVideo, you can transform text into captivating videos quickly and effortlessly.

**Image generation →** [**http://midjourney.com**](http://midjourney.com/)

Say goodbye to complicated video creation with InVideo. This DIY platform allows users to turn their thoughts into impressive videos in real time. Whether you're an amateur or a pro, you can effortlessly craft visually stunning videos from text, breathing life into your ideas.

**\*\*Text to AI voice →** [**https://murf.ai/\*\***](https://murf.ai/**)

Experience the power of AI with [murf.ai](http://murf.ai/). This remarkable tool swiftly converts written text into lifelike voices. Whether you need professional-quality voice-overs for podcasts, videos, or presentations, Murf's voice generator delivers exceptional results.

**Meeting note →** [**https://fireflies.ai/**](https://fireflies.ai/)

Fireflies transforms how you handle your meetings. As a smart assistant, it transcribes, summarizes, and scrutinizes voice conversations, offering invaluable meeting notes for productive discussions. With seamless integration with video-conferencing apps, collaboration becomes a breeze.

**Summarize meetings →** [**http://krisp.ai**](http://krisp.ai/)

Introduce efficiency into your meetings with KRISP. This AI assistant transcribes voice apps, generates sharable AI-powered summaries, and eradicates background noise, improving your meeting experience. Streamline your meetings and get more done with KRISP.

**AI-powered CRM →** [**http://folk.app**](http://folk.app/)

Meet Folk, a novel CRM designed to manage relationships in a team. It allows you to organize your contacts, design handy views, and work collaboratively to keep your contacts engaged. Explore Folk at \*\*[http://folk.app/\*\*](http://folk.app/**) and witness a new approach to CRM.

**Product design →** [**http://flair.ai**](http://flair.ai/)

Flair is a game-changer in the realm of product design. As an AI design tool, Flair simplifies the creation of branded content. Just drag and drop product photos, describe the scene, and start editing. Flair's user-friendly interface and smart recommendations make designing a breeze.

**SEO-optimized content →** [**http://frase.io**](http://frase.io/)

Frase is a one-stop solution for creating SEO-friendly content. Using search results, it helps users identify and categorize ideas for writing. Frase streamlines content creation and improvement, optimizing for search engine rankings.

**Play AI voice-over →** [**https://elevenlabs.io**](https://elevenlabs.io/**)

Unleash the power of AI in voiceovers with ElevenLabs. This advanced tool offers text-to-speech and voice cloning capabilities, letting users generate lifelike voiceovers, transform text into high-quality audio, and create captivating audio experiences for a variety of applications.

P.S. If you found this helpful, come and join our [FREE AI newsletter](https://www.aifire.co/) which over 28,000+ Entrepreneurs & Professionals update important developments in AI every day.",/r/ChatGPTPromptGenius/comments/152s7nt/ive_checked_out_over_1000_ai_tools_for_my/,ChatGPTPromptGenius,r/ChatGPTPromptGenius,False,relevant,highly relevant
A passion i didn't know i had until i was 30,Morfaer,https://www.reddit.com/r/VideoEditing/comments/16rx357/a_passion_i_didnt_know_i_had_until_i_was_30/,106,62,1695660090.0,16rx357,"Hello everyone,

I turned 30 this year and i've always been that person who never really found what they wanted from life. And i've done a lot of things, but nothing clicked just right. Not until a month ago at least. 

It all started with me wanting to build a quick cash grab Youtube channel with text-to-video AI tools. You know, those 60 second daily fact videos. It was frustrating because i care about quality in whatever i do and those AI videos just didn't have a ""soul"" i guess. So the quick cash-grab scheme quickly evolved into me chasing quality in something i had no prior experience.

Then i found out about CapCut. I have a feeling Premiere Pro users hate CapCut but i have no actual proof of that. For the last 2 weeks, i've been doing some video edits of space footage, trying to learn everything there is to it. And i just have to say i haven't felt this way forever. 

It kind of sucks that they don't get many views at the moment, but just the process of working on a video, the pleasure it gives when all clips perfectly line up with the background music, when i can find a good transition that reflects how i feel at that point in the video, that feeling of accomplishment when i export a video i'm happy with is something i've been looking for my whole life. 

At some point i think i will have to leave CapCut behind and learn Premiere Pro. I don't know yet. But, i just wish i've known i'd love editing at least a few years earlier.

Anyways, thank you for taking the time to read my experience. If you have any general advice, feel free to share, i'm always open to learning and researching.",/r/VideoEditing/comments/16rx357/a_passion_i_didnt_know_i_had_until_i_was_30/,VideoEditing,r/VideoEditing,False,relevant,highly relevant
Just a potato with legs running… that is all (AI Made with text to video),aimademedia,https://v.redd.it/pgnj3zun20ua1,108,13,1681525509.0,12mmo1n,,/r/potato/comments/12mmo1n/just_a_potato_with_legs_running_that_is_all_ai/,potato,r/potato,True,relevant,neutral
"Lego World, text to video, runway gen-2",ZashManson,https://v.redd.it/n4lw16skmh2b1,109,12,1685224831.0,13tjfvf,Source ➡️ https://twitter.com/ixitimmyixi/,/r/aivideo/comments/13tjfvf/lego_world_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
A Great Escape - the future of content creators and docs? ,Ftrstdhd,https://v.redd.it/7ytsgfjiah4c1,105,18,1701782984.0,18bbz99,"Video I made for my brand TrueCrime A.I.

It’s a combination of midjourney images and Runway text to video prompts, and all MJ images animated in Runway. Several other workflow layers, especially for the opening influencer. Made this in over 4 days, maybe 15 hours total. I’m on X as @MaxEinhorn if you’d like to follow.  

Happy to answer workflow questions! ",/r/midjourney/comments/18bbz99/a_great_escape_the_future_of_content_creators_and/,midjourney,r/midjourney,True,relevant,highly relevant
Text-to-video feels as significant to me as the invention of the motion picture over a century ago,Current_Side_4024,https://www.reddit.com/r/singularity/comments/14mi809/texttovideo_feels_as_significant_to_me_as_the/,104,24,1688076855.0,14mi809,Being able to record actual events and replay their imagery must have felt similar to how it feels now to be able to generate moving picture imagery without having to film anything,/r/singularity/comments/14mi809/texttovideo_feels_as_significant_to_me_as_the/,singularity,r/singularity,False,relevant,highly relevant
"This Week in AI (4/22/23): AI music bans, GDPR woes, and Nvidia’s amazing new text-to-video",ShotgunProxy,https://www.reddit.com/r/ArtificialInteligence/comments/12v4fd2/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,102,13,1682166315.0,12v4fd2,"I combed through 500+ saved tabs on AI this past week to find the top items (below).

Because it’s hard to keep track of why something is important, I’ve added a sub point for each link to highlight its significance. Enjoy with your ☕!

[The full post with links is here.](https://www.artisana.ai/articles/this-week-in-ai-4-22-23-ai-music-bans-gdpr-woes-and-nvidias-amazing-new-text) (Automod seems to remove posts with too many links)

**News to Know (12 Key Developments)**

AI-generated photo wins major photography award, but winner rejects prize

* The winner deliberately submitted an AI-generated piece to make a statement.

Nvidia unveils text-to-video model

* Please click the link to see it in action. It’s UNREAL and portends how crazy this year will be.

Compliance with GDPR will be difficult for ChatGPT, portending fines and ban

* Numerous legal experts think it will be near impossible for ChatGPT to fully comply with GDPR.

AI-Generated Song Mimicking Drake and The Weeknd Pulled from Streaming Services

* New details are still emerging here, actually! AI-generated music is raising lots of questions.

Reddit to start charging AI models for access to its archives

* AI models use large bodies of data, and content companies now want to cash in.

StackOverflow jumps on the API charge bandwagon as well

* StackOverflow’s extensive code examples were likely used to train OpenAI’s current models

Stability AI launches their own open-source language model, StableLM

* Best known for Stable Diffusion, they’re now moving to compete with ChatGPT

Google plans radical changes to their search engine

* Google races to play catchup, and the CEO swears they’re moving faster!

New Google DeepMind team formed out of two AI teams

* Two AI teams that formerly bickered are now one unit. Google’s survival is at stake here.

Michael Schumacher’s Family Threatens Suing German Tabloid Over AI-Generated Interview

* AI-generated content is at the center of numerous legal firestorms. This is just one of them.

Microsoft developers own AI chip as ChatGPT costs OpenAI an estimated $700k per day to run

* AI is expensive. ChatGPT is expensive. Microsoft is launching their own chip to cut costs.

Employees said Bard was “cringe-worthy,” but Google launched it anyways

* Wonder why Bard disappointed us at launch? It’s because Google didn’t listen to internal warnings.

**Science Experiments and Things to Try**

A beginner’s guide to autonomous agents

* What’s the hype around autonomous agents? 100k stars on GitHub makes this one of the fastest-growing software projects, ever. This writeup explains what it does and how you can play with it, right now.

MiniGPT-4 launched, runs on just 12GB memory, and can process images

* Multi-modal models can now run on personal computers. This one can process images like OpenAI’s GPT-4. Insane and a glimpse of the AI future.

Things you can do right now with AI that you no longer need to pay a marketer for

* Great though-joggers of how marketing is actively transforming now that AI is here. Good for any professional.

Meta open sources their animated drawings AI library

* Pretty fun to see in action \[an a great example of the weird science coming out of the AI sector these days.

**Notable New Research Papers this Week**

LLMs are learning to program with natural language

Analysis of why ChatGPT falls short in comprehension

Using LLMs to create data lakes

Just 51.5% of LLM search engine responses fully supported by citations

Gisting enables 26x compression of LLM prompts

—--

P.S. -- I run my own newsletter that covers the most important and impactful developments in generative AI (no BS clickbait news or content). Cutting through the noise is more important than ever.

Readers from a16z, Meta, McKinsey, Apple and more are all subscribers. If you’re looking to get a roundup of news and analysis that doesn't appear anywhere else,[ you can sign up here.](https://artisana.beehiiv.com/subscribe)

Totally free, no ads/paywall. I do it to provide value to the community.",/r/ArtificialInteligence/comments/12v4fd2/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,ArtificialInteligence,r/ArtificialInteligence,False,relevant,highly relevant
"This Week in AI (4/22/23): AI music bans, GDPR woes, and Nvidia’s amazing new text-to-video",ShotgunProxy,https://www.reddit.com/r/GPT3/comments/12v4he9/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,102,9,1682166460.0,12v4he9,"I combed through 500+ saved tabs on AI this past week to find the top items (below).

Because it’s hard to keep track of why something is important, I’ve added a sub point for each link to highlight its significance. Enjoy with your ☕!

[The full post with links is here.](https://www.artisana.ai/articles/this-week-in-ai-4-22-23-ai-music-bans-gdpr-woes-and-nvidias-amazing-new-text) (Automod seems to remove posts with too many links)

**News to Know (12 Key Developments)**

AI-generated photo wins major photography award, but winner rejects prize

* The winner deliberately submitted an AI-generated piece to make a statement.

Nvidia unveils text-to-video model

* Please click the link to see it in action. It’s UNREAL and portends how crazy this year will be.

Compliance with GDPR will be difficult for ChatGPT, portending fines and ban

* Numerous legal experts think it will be near impossible for ChatGPT to fully comply with GDPR.

AI-Generated Song Mimicking Drake and The Weeknd Pulled from Streaming Services

* New details are still emerging here, actually! AI-generated music is raising lots of questions.

Reddit to start charging AI models for access to its archives

* AI models use large bodies of data, and content companies now want to cash in.

StackOverflow jumps on the API charge bandwagon as well

* StackOverflow’s extensive code examples were likely used to train OpenAI’s current models

Stability AI launches their own open-source language model, StableLM

* Best known for Stable Diffusion, they’re now moving to compete with ChatGPT

Google plans radical changes to their search engine

* Google races to play catchup, and the CEO swears they’re moving faster!

New Google DeepMind team formed out of two AI teams

* Two AI teams that formerly bickered are now one unit. Google’s survival is at stake here.

Michael Schumacher’s Family Threatens Suing German Tabloid Over AI-Generated Interview

* AI-generated content is at the center of numerous legal firestorms. This is just one of them.

Microsoft developers own AI chip as ChatGPT costs OpenAI an estimated $700k per day to run

* AI is expensive. ChatGPT is expensive. Microsoft is launching their own chip to cut costs.

Employees said Bard was “cringe-worthy,” but Google launched it anyways

* Wonder why Bard disappointed us at launch? It’s because Google didn’t listen to internal warnings.

**Science Experiments and Things to Try**

A beginner’s guide to autonomous agents

* What’s the hype around autonomous agents? 100k stars on GitHub makes this one of the fastest-growing software projects, ever. This writeup explains what it does and how you can play with it, right now.

MiniGPT-4 launched, runs on just 12GB memory, and can process images

* Multi-modal models can now run on personal computers. This one can process images like OpenAI’s GPT-4. Insane and a glimpse of the AI future.

Things you can do right now with AI that you no longer need to pay a marketer for

* Great though-joggers of how marketing is actively transforming now that AI is here. Good for any professional.

Meta open sources their animated drawings AI library

* Pretty fun to see in action \[an a great example of the weird science coming out of the AI sector these days.

**Notable New Research Papers this Week**

LLMs are learning to program with natural language

Analysis of why ChatGPT falls short in comprehension

Using LLMs to create data lakes

Just 51.5% of LLM search engine responses fully supported by citations

Gisting enables 26x compression of LLM prompts

—--

P.S. -- I run my own newsletter that covers the most important and impactful developments in generative AI (no BS clickbait news or content). Cutting through the noise is more important than ever.

Readers from a16z, Meta, McKinsey, Apple and more are all subscribers. If you’re looking to get a roundup of news and analysis that doesn't appear anywhere else,[ you can sign up here.](https://artisana.beehiiv.com/subscribe)

Totally free, no ads/paywall. I do it to provide value to the community.",/r/GPT3/comments/12v4he9/this_week_in_ai_42223_ai_music_bans_gdpr_woes_and/,GPT3,r/GPT3,False,relevant,highly relevant
Just a potato with legs running… that is all (AI Made with text to video),aimademedia,https://v.redd.it/puq0si3d10ua1,104,21,1681525073.0,12mmgvt,,/r/deepdream/comments/12mmgvt/just_a_potato_with_legs_running_that_is_all_ai/,deepdream,r/deepdream,True,relevant,neutral
"OPUS AI: Text-to-Video Game, the future of video gaming where you type and a 3D World emerges: A Demo",Hybridx21,https://twitter.com/nonmayorpete/status/1641070825196363776,103,26,1680139910.0,1267d8s,,/r/singularity/comments/1267d8s/opus_ai_texttovideo_game_the_future_of_video/,singularity,r/singularity,False,relevant,neutral
"Cat Skydiving, text to video, runway gen-2",ZashManson,https://v.redd.it/ui8spp2nc60b1,94,10,1684216606.0,13iwjh2,Coming through YouTube https://www.youtube.com/@wintergardenai,/r/aivideo/comments/13iwjh2/cat_skydiving_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Anomalies, text to video, runway gen-2",ZashManson,https://v.redd.it/fb9ycaqwfeya1,94,4,1683442855.0,13aftnp,Coming through Twitter https://twitter.com/m_katzlberger/,/r/aivideo/comments/13aftnp/anomalies_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Opus AI is a text-to-video-games that uses narrative AI to create whatever you type into the program.,Dalembert,https://v.redd.it/kl3yrov52vra1,93,1,1680611492.0,12bhowo,,/r/Automate/comments/12bhowo/opus_ai_is_a_texttovideogames_that_uses_narrative/,Automate,r/Automate,False,relevant,highly relevant
"Horror Film, text to video, runway gen-2",ZashManson,https://v.redd.it/8bstivoq7i1b1,86,8,1684796083.0,13p69hx,Source ➡️ https://twitter.com/ammaar/,/r/aivideo/comments/13p69hx/horror_film_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,less relevant
The Recent AI Developments Are Making Me Suicidal,A_Username_What_Else,https://www.reddit.com/r/ArtistHate/comments/140l89w/the_recent_ai_developments_are_making_me_suicidal/,89,80,1685902337.0,140l89w,"TLDR: AI is replacing my special interest as an artist and I believe AI programs are going to plunge us further into a dystopia. I am feeling defeated.

I want to preface this all by making it clear that I will NEVER end my own life. Yes, I may feel like doing it, but I ultimately never will as there are far to many people in my life who would be destroyed if I did.

I’ve divided this up into sections in order to make it easier to read.

**Introduction**

Who am I? I’m a 20 year old lady with mild autism. I am an artist and have known since I was little. I have been working to improve my skills for so many years and am currently in university studying animation. Art and animation are the only things I am really good at. My autism makes life mostly unenjoyable. My art has been the one thing keeping me going all these years.

**The Problem**

You have all definitely heard of AI art generators by now. I was shocked when Dall-E 2 was unveiled in April last year. We all thought AI could never be creative or make art. Yet all these creative AI’s pretty much came out of nowhere in quick succession last year. Mid journey, chatgpt, stable diffusion, text to video programs, you name it. It just happened all at once.

My concerns should be obvious. These AI systems can make art and even write stories. They can do it at lightning speed and at the cost of electricity. This puts anyone who writes or draws for a living in a terrible position. Companies and individuals will surely go for the quicker, cheaper option. Even if it is slightly worse in quality.

My autism would make getting any other job an actual nightmare so I have been incredibly disheartened by these developments. Not to mention that many creatives are like me and are on the spectrum too. They are often socially awkward and find regular life difficult. Many of them will have their entire future’s ruined by these systems.

I still find it hard to believe that art of all things is getting automated. Art, writing and more. All the creative tasks are getting taken from us. Sure, they can still be hobbies. But that just leaves us with more soul crushing work. And not to mention that future generations will have it even worse. Why would anyone want to bring kids into such a depressing world where all the things we enjoy are monopolized and automated?

**A Dystopian Future**

Not to mention the other, possibly worse side effects of this new technology. AI voices, videos and images are getting better every week. Imagine how easy it will be to generate a fake video of someone doing or saying something horrible that they didn’t do. And the technology will likely be so good in just a few years that we will have no way of knowing if it’s fake or not. Sounds like the worst kind of dystopia. Some people may doubt that the tech will ever get that good. Well we never thought that AI would be able to creat art, yet here we are. Just look at how much AI images have improved since April of last year. Or heck, 3 months ago!

But back to job/purpose loss. This worries me just as much as the paragraph above. Art is the only thing I have really ever excelled at. And I am now realizing that I probably won’t be able to make it my career. Yes, I do animation so that probably has a while longer. But at the rate the tech is progressing I wouldn’t be surprised if AI takes over animation and VFX in under 10 years.

**Where I’m At**

I am not exaggerating when I say I am crushed. Truly this time though. I have been through many things that destroyed my spirit, it’s pretty much guaranteed for people on the spectrum. I was bullied my entire childhood, denied being able to be in a relationship due to my sensory issues, isolated from society and more. Yet the one thing that kept me hopeful was being able to show the world my art and make a living off it.

And now it’s gone.

Now anyone can have top tier skill. Anyone can generate anything in any style. Even if I make a new style, someone can just feed my drawings into a machine and infinitely clone them. My skill doesn’t matter anymore. The one thing I truly had going for me is basically worthless. And if not now, in a few years. I have no purpose anymore. I feel worthless. Even though I have more friends and family that love me then most people, I feel worthless now. Even if there is still room for human artists, we simply can’t keep up with the speed AI produces stuff. We will be drowned out by AI works.

I knew I would likely never make big bucks by pursuing art and animation. Unless you run a massive studio you don’t get mega rich. But I knew I could earn enough to comfortably live off my work. With things like YouTube and Patreon it seemed feasible as long as I worked to become good enough. I wanted to start doing commissions online. I wasn’t expecting to get rich, but I was so excited and happy to get started. Then this all happened. I already see many artists online talking about how they’ve lost their jobs or how nobody commissions them anymore.

People often ask the question ""Where would you be and what would you be doing if money didn't exist?"". Well, the life I had planned out WAS what I would be doing if money had no value.

To be honest, I don't like where the world in general is going. We're spied on every moment of our lives, people are being increasingly fake and we can't provide for our families. Then there's cancel culture, political correctness, wokeness, decaying attention spans and instant gratification. All of which didn't really exist just a few years ago.

**Questions You May Have**

I want to dedicate a section to answering some questions I can see people having for me as to clear up any confusion.

“You’re Being Dramatic”

I can see why someone might think that. But imagine if you’re entire purpose and passion suddenly became pretty much worthless and you had to face the rest of your life without it. Seriously, imagine it. You’d be devastated. Also, all the things I’ve mentioned here either already have happened or are likely to happen.

“AI will never be as good as human artists”

Maybe it will never be as good as the top 1% but it’s already better then most artists out there. And if it’s this good now then imagine where it’ll be in 5-10 years. Not to mention that even if it is slightly worse, most people will chose it over a human artist simply due to the speed and cost.

”You Must Adapt And Use The AI’s In Your Workflow”

I will. I will do anything to keep my dream alive. I’ve played around with various AI tools to enhance my workflow. But in doing so, the enjoyment is gone and my original visions destroyed. Not to mention that AI will dramatically devalue all creative works, so I may not be able to earn enough to keep doing art as a career.

“These Are Just New Tools”

Maybe now, but they’re already replacing tons of artists, voice actors and more. Also, the end game for these ‘tools’ is to completely remove the need for any skill or talent. An AI image generator makes all the decisions for you, while you type in a text prompt. These systems are replacements, not tools. If you were a brick layer you wouldn’t see a brick laying machine as a tool. You would see it as a replacement.

""You Only Care About Making Money""

Read the part where I went over this. Yes, I am afraid of not being able to make a career off art. But I'm also scared of not being able to get noticed and having my work devalued. If everyone's an amazing artist, nobody is. Also, my parents are wealthy enough to be able to comfortably support me and my two siblings for the rest of our lives. But I don't want this. I WANT to work hard and feel accomplished for doing so. And These AI systems take that away.

“We Can Fight The Companies Developing These AI’s”

Sure, but what will that achieve? The technology is out there and it won’t go away. Even if every government in the world banned these AI’s overnight then they would still be developed in secret. I know some people are suing AI art generators over copyright infringement for using their art to train the AI’s. I personally do not see the court systems ruling in favor of the artists. It will most likely be seen as fair use.

“There Will Always Be A Place For Human Made Art”

I do agree on this. One tiny positive is that physical art such as paintings and sculptures may have a resurgence due to digital art being devalued. But this will likely be a tiny niche.

“This Is Just Like NFT’s”

No it’s not. Everyone knew from the beginning how stupid NFT’s were. We knew they were a pyramid scheme destined to collapse. And they did. AI art and their similar tools  are not something that’s going to go away. They are rapidly developing and getting better. They’re going to be more and more widely used in the future.

”You Only Care When Your Own Job Is Being Replaced”

Not true. I have felt very sorry for those who’s jobs have or are being automated. When I first heard of self driving cars I immediately felt dread for truck drivers and anyone in a similar position. I actually will go out of my way to not use the self service checkout in order to help keep the store employees employed.

”Do You Hate AI Bro’s?”

No. I don’t hate anyone who uses or is excited about this new tech. I can totally see their perspectives and we shouldn’t dogpile them. My issues lie with the developers of this technology and the people online saying “LOL GET REKT ARTISTS! YOU’RE WORTHLESS NOW!”. You’re allowed to love this tech, but actively routing for someone’s downfall is very horrible and childish. I wouldn’t think anything bad of any group of people who were about to have their professions made obsolete.

**Conclusion**

I am truly broken this time.

No really, that’s my conclusion. I don’t see this getting better. We are entering into an even bigger dystopia then before. I have never felt more hopeless then I do now. I’m only 20 and I’m tired. My spirit has officially been crushed. And yet I’m meant to stay in this game for another 70+ years.

I have been depressed before in life, but this has made me completely suicidal. Never before have I actually felt like ending my life. Yet this has pushed me over the edge. Though I will ultimately never do it.

I want nothing more then to wake up and for this technology to be gone. I would do anything to go back to even just 2021 when it seemed like these systems didn't exist.

All of us now just happened to be unlucky enough to be born into the most boring dystopia. I have no idea how to find happiness or purpose in any of this. But I will continue living because what else can I do?

If anyone has any questions or wants to discuss things then I will answer them.",/r/ArtistHate/comments/140l89w/the_recent_ai_developments_are_making_me_suicidal/,ArtistHate,r/ArtistHate,False,relevant,highly relevant
I can't wait until text-to-video catches up with Midjourney. I'm ready to get weird.,PizzaLater,https://www.reddit.com/gallery/159kqpv,89,3,1690318468.0,159kqpv,,/r/midjourney/comments/159kqpv/i_cant_wait_until_texttovideo_catches_up_with/,midjourney,r/midjourney,False,relevant,highly relevant
100 self-analyzed ChatGPT behaviors,wpsgdev,https://www.reddit.com/r/ChatGPT/comments/12r2vj1/100_selfanalyzed_chatgpt_behaviors/,89,20,1681849072.0,12r2vj1,"Just another day as a digital psychologist. Credit me if you use this list, thanks!

&#x200B;

|\#|Behavior|Detail|
|:-|:-|:-|
|1|Abstractive Dialogue Summarization|I can generate summaries of entire conversations, capturing the main points and themes.|
|2|Abstractive Summarization|I can generate summaries of text that are not just a selection of sentences but are created by rephrasing and synthesizing content.|
|3|Argumentation Mining|I can identify arguments and their components in a text, such as claims, evidence, and reasoning.|
|4|Commonsense Reasoning|I can reason about everyday situations and make inferences based on common sense knowledge.|
|5|Conditional Text Generation|I can generate text that is conditioned on specific attributes or characteristics, such as style, genre, or topic.|
|6|Contextual awareness|I can use contextual information to understand the meaning of words and sentences and generate appropriate responses.|
|7|Contextual Embedding|I can encode the meaning of words and sentences in a high-dimensional vector space, which enables me to understand the context in which words are used.|
|8|Contextual Similarity|I can identify and measure the similarity between different pieces of text based on their meaning and context.|
|9|Conversational Analysis|I can analyze the structure and dynamics of a conversation, such as turn-taking and topic shifts.|
|10|Conversational Recommender Systems|I can recommend products or services in a conversational manner, taking into account the user's preferences and feedback.|
|11|Coreference Resolution|I can identify which words in a text refer to the same entity, such as ""he"" and ""John"".|
|12|Cross-lingual Retrieval|I can retrieve information in one language based on a query in another language.|
|13|Data Augmentation|I can generate additional training data by creating variations of existing text or speech.|
|14|Dialog Response Generation|I can generate appropriate responses to a user's input during a conversation, taking into account the current context and previous exchanges.|
|15|Dialogue Act Recognition|I can recognize the purpose or function of a dialogue act in a conversation, such as requesting or informing.|
|16|Dialogue Evaluation|I can evaluate the quality of a dialogue system by measuring its ability to maintain a conversation with a user.|
|17|Dialogue Management|I can maintain a conversation with a user, keeping track of the context and generating appropriate responses.|
|18|Dialogue State Tracking|I can keep track of the state of a conversation, such as the user's goals and preferences.|
|19|Document Clustering|I can group similar documents together based on their content.|
|20|Document Summarization|I can generate summaries of longer documents, such as research papers or news articles.|
|21|Domain Adaptation|I can adapt to different domains, such as medical or legal, by learning from domain-specific data.|
|22|Domain-Specific Adaptation|I can adapt to specific domains or industries, such as healthcare or finance, by learning from domain-specific data.|
|23|Event Extraction|I can extract information about events mentioned in a text, such as the location, time, and participants.|
|24|Fact Checking|I can verify the accuracy of claims made in a text or speech.|
|25|Fine-tuning|I can adapt to a specific task or domain by fine-tuning my parameters on a smaller set of related data.|
|26|Grammatical Error Correction|I can correct grammatical errors in text, such as subject-verb agreement or word order.|
|27|Image Captioning|I can generate textual descriptions of images.|
|28|Image-to-Text Conversion|I can extract textual information from images, such as text in a meme or a caption in a photograph.|
|29|Information Extraction|I can extract specific pieces of information from a text, such as dates, locations, and numbers.|
|30|Intent Recognition|I can identify the user's intent based on their input and generate appropriate responses.|
|31|Interpretability|I can provide explanations of my behavior and decision-making processes, such as by highlighting the most relevant parts of a text.|
|32|Knowledge Graph Construction|I can construct a knowledge graph by extracting and linking information from various sources.|
|33|Knowledge Representation|I can represent knowledge in a structured format such as a graph or table, which enables me to reason about relationships between different concepts.|
|34|Knowledge retrieval and synthesis|I can retrieve information from a wide range of sources and synthesize it to provide relevant responses to prompts.|
|35|Language Adaptation|I can adapt to different language varieties or dialects, such as American English or British English.|
|36|Language generation|I can generate human-like responses to prompts based on the input I receive.|
|37|Language Modeling|I can generate new text based on a given prompt, topic or style.|
|38|Language translation|I can translate text from one language to another.|
|39|Language understanding and comprehension|I can understand and interpret human language, including grammar, syntax, and semantics.|
|40|Machine Reading Comprehension|I can answer questions about a given text by understanding its content and context.|
|41|Machine Translation|I can translate texts from one language to another using a machine translation system.|
|42|Model Compression|I can compress my parameters to reduce memory and computation requirements while maintaining performance.|
|43|Multi-lingual Processing|I can process text in multiple languages, including translation, summarization, and sentiment analysis.|
|44|Multi-lingual Text Generation|I can generate text in multiple languages.|
|45|Multimodal Processing|I can process text in conjunction with other modalities, such as images or audio.|
|46|Multi-task Learning|I can perform multiple natural language processing tasks simultaneously, such as translation and summarization.|
|47|Multi-task Text Processing|I can perform multiple natural language processing tasks at the same time, such as sentiment analysis and topic modeling.|
|48|Named Entity Recognition|I can identify and classify entities such as people, places, and organizations within a text.|
|49|Natural Language Generation|I can generate human-like text from structured data or information, such as a weather forecast or a news article.|
|50|Natural Language Inference|I can determine whether a hypothesis is true, false, or unknown based on a given premise.|
|51|Natural language understanding of human emotions|I can recognize and respond appropriately to different emotional states expressed through language.|
|52|Opinion Mining|I can identify the opinions and attitudes expressed in a text, such as in product reviews or social media posts.|
|53|Paraphrasing and Rewriting|I can rephrase sentences and paragraphs while maintaining the same meaning.|
|54|Question answering|I can answer factual questions by providing relevant information from my knowledge base.|
|55|Reasoning with Uncertainty|I can reason with uncertain or incomplete information, such as in a question-answering system.|
|56|Sentiment analysis|I can analyze the sentiment of a text and determine whether it is positive, negative, or neutral.|
|57|Sentiment Classification|I can classify the sentiment of a text as positive, negative, or neutral.|
|58|Speech-to-Text|I can transcribe spoken audio into text.|
|59|Style Transfer|I can transfer the style of one text to another, for example, making a formal text sound more casual.|
|60|Style Transfer Evaluation|I can evaluate the quality of text generated by style transfer algorithms.|
|61|Text Classification|I can classify texts into different categories such as spam, news, or reviews.|
|62|Text Classification Evaluation|I can evaluate the quality of text classification algorithms, such as by measuring precision and recall.|
|63|Text Correction|I can correct grammar, spelling, and punctuation errors in text.|
|64|Text Generation Adaptation|I can adapt text generation models to specific users or contexts, such as by learning their preferences and writing style.|
|65|Text Generation Evaluation|I can evaluate the quality of text generated by language models, such as by measuring coherence and fluency.|
|66|Text Segmentation|I can segment a text into meaningful units, such as sentences or paragraphs.|
|67|Text summarization|I can summarize a large amount of text into a shorter, more concise form.|
|68|Text-to-Action|I can generate actions from natural language commands, such as controlling a smart home device or scheduling a meeting.|
|69|Text-to-Action Adaptation|I can adapt action generation models to specific users or contexts, such as by learning their preferences and behavior.|
|70|Text-to-Action Reasoning|I can reason about the potential consequences of an action generated from natural language commands.|
|71|Text-to-Code Evaluation|I can evaluate the quality of code generated from natural language descriptions of a program's functionality.|
|72|Text-to-Code Execution|I can execute code generated from natural language descriptions of a program's functionality.|
|73|Text-to-Code Generation|I can generate code from natural language descriptions of a program's functionality.|
|74|Text-to-Emotion|I can detect the emotional content of text and generate appropriate emotional responses.|
|75|Text-to-Graph Conversion|I can extract structured data from text and represent it in a graphical format.|
|76|Text-to-Graph Reasoning|I can perform reasoning on graphs created from text, such as by answering complex questions that require knowledge from multiple sources.|
|77|Text-to-Image Evaluation|I can evaluate the quality of images generated from textual descriptions, such as by measuring fidelity and relevance.|
|78|Text-to-Image Retrieval|I can retrieve images based on a textual query or description.|
|79|Text-to-Image Retrieval Evaluation|I can evaluate the quality of text-to-image retrieval algorithms, such as by measuring relevance and diversity.|
|80|Text-to-Image Synthesis|I can generate images from textual descriptions or captions.|
|81|Text-to-Knowledge|I can extract knowledge from text and represent it in a structured format, such as a knowledge graph or database.|
|82|Text-to-Knowledge Graph|I can create a knowledge graph from a collection of related text documents.|
|83|Text-to-Knowledge Graph Reasoning|I can perform reasoning on knowledge graphs created from text.|
|84|Text-to-Math|I can generate mathematical expressions from natural language descriptions, such as in a math problem.|
|85|Text-to-Music Evaluation|I can evaluate the quality of music generated from textual descriptions or instructions.|
|86|Text-to-Music Synthesis|I can generate music from natural language descriptions or instructions.|
|87|Text-to-Speech|I can convert text into spoken audio.|
|88|Text-to-Speech Evaluation|I can evaluate the quality of speech generated from text, such as pronunciation, intonation, and naturalness.|
|89|Text-to-Speech Synthesis Adaptation|I can adapt speech synthesis models to specific users, such as by learning their speaking style.|
|90|Text-to-Speech Synthesis Compression|I can compress speech synthesis models to reduce memory and computation requirements while maintaining performance.|
|91|Text-to-Speech Synthesis Evaluation|I can evaluate the quality of speech generated from text, such as by measuring naturalness and intelligibility.|
|92|Text-to-SQL|I can generate SQL queries from natural language questions or commands.|
|93|Text-to-Summary Evaluation|I can evaluate the quality of a summary generated from a longer text.|
|94|Text-to-Table Conversion|I can extract structured data from text and represent it in a tabular format.|
|95|Text-to-Table Generation|I can generate tables from natural language descriptions or data.|
|96|Text-to-Video Evaluation|I can evaluate the quality of a video generated from a textual description or script.|
|97|Text-to-Video Retrieval|I can retrieve videos based on a textual query or description.|
|98|Text-to-Video Synthesis|I can generate videos from textual descriptions or scripts.|
|99|Topic Modeling|I can identify the topics discussed in a text, even if they are not explicitly stated.|
|100|Transfer Learning|I can transfer knowledge learned from one task or domain to another, to improve performance on related tasks.|

..and there are still likely plenty more, but this list should provide some good context. OpenAI is light years ahead of the other big players. Can I get an upvote?",/r/ChatGPT/comments/12r2vj1/100_selfanalyzed_chatgpt_behaviors/,ChatGPT,r/ChatGPT,False,relevant,highly relevant
a side project I'm working on - a simple text-to-video generator with sounds :),TimeNeighborhood3869,https://v.redd.it/76ezai15aujb1,87,13,1692787493.0,15z07ke,,/r/discordVideos/comments/15z07ke/a_side_project_im_working_on_a_simple_texttovideo/,discordVideos,r/discordVideos,True,not relevant,highly relevant
Drop your 2024 prediction here & Check again next year 🚀,LeKhang98,https://i.redd.it/2jnbk6l0b78c1.png,82,87,1703406252.0,18pr1x9,,/r/StableDiffusion/comments/18pr1x9/drop_your_2024_prediction_here_check_again_next/,StableDiffusion,r/StableDiffusion,False,not relevant,less relevant
Text-to-Video Model Fine-Tuned with 512x512 Anime-Style for Diffusers,brue-Bid-7067,https://v.redd.it/dnvxfrzox1ya1,81,22,1683311806.0,138xdaw,,/r/StableDiffusion/comments/138xdaw/texttovideo_model_finetuned_with_512x512/,StableDiffusion,r/StableDiffusion,True,not relevant,neutral
How much AI do you think will be integrated into films making after 10 years?,Canada_LBM,https://www.reddit.com/r/Futurology/comments/184wycu/how_much_ai_do_you_think_will_be_integrated_into/,86,151,1701068365.0,184wycu,"I'm currently learning houdini, a 3d software to generate various cool effects by linking nodes and writing some simple programs, after learning for almost 2 months, I find out that pure text-to-video ai may can't help much because many effects can't be described by language, you can only describe it by using procedures and programs, and traditional visual effects can be adjusted by tweaking parameters, which is highly more controllable than text-to-video ai.

However, ai softwares like wonder studio may have great application in films' post production, for example we can use ai to trace a moving object in a complex scene and change that object far more easier, and chatgpt like ai can write some auxiliary part in houdini, but describing the whole effects, in many cases, is equivalent to describing the algorithm and procedures behind it, and the final result is highly depending on different parameters, which can't be predicted, so I think current generative ai can help us search for the node we want and help us to finish some trivial but tedious parts, the visual effects may the least shocked part by ai.

let's turn to 3d modeling, there has been ai to turn photos into 3d models, and some ai to generate 3d models from text, but unlike pictures generator, text-to-model ai seriously lacks training data, and in many cases, we need a good UV, topology and we also need to conduct cloth simulations, hence as to 3d modelling, ai may greatly speed up the creative part and leave the shit and tedious part for human, that sucks

So what are your ideas?",/r/Futurology/comments/184wycu/how_much_ai_do_you_think_will_be_integrated_into/,Futurology,r/Futurology,False,relevant,highly relevant
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,83,4,1696086091.0,16w93bx,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Meta** announced:  

   1. **Meta AI** \- a new AI assistant users can interact with on WhatsApp, Messenger and Instagram. Will also be available on Ray-Ban Meta smart glasses and Quest 3, Meta’s mixed reality headset.
   2. **AI stickers** that enable users to generate customized stickers for chats and stories using text. Powered by Llama 2 and the new foundational model for image generation, Emu.
   3. **28 AI characters**, each with a unique personality that users can message on WhatsApp, Messenger, and Instagram.
   4. New AI editing tools, **restyle** and **backdrop** in Instagram.
   5. **AI Studio** \- a platform that supports the creation of custom AIs by coders and non-coders alike.
5. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
6. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
7. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
8. **OpenAI** has returned the ChatGPT browsing feature for Plus subscribers, enabling ChatGPT to access internet for current information. It was disabled earlier as users were able to deploy it to bypass the paywalls of leading news publishers.
9. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
10. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
11. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
12. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
13. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
14. **Google** announced it’s giving website publishers a way to opt out of having their data used to train the company’s AI models while remaining accessible through Google Search.
15. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
16. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
17. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
18. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
19. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.
20. **Pika** Labs' text-to-video tool now lets users encrypt a message in a video\].

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks",/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,learnmachinelearning,r/learnmachinelearning,False,relevant,highly relevant
"Salvador Dali Planet, text to video, runway gen-2",ZashManson,https://v.redd.it/z3fxmabkrb6b1,81,7,1686897264.0,14ap8ui,,/r/aivideo/comments/14ap8ui/salvador_dali_planet_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Woodland Creature Festival, text to video, runway gen-2",ZashManson,https://v.redd.it/k1a99k85zlya1,81,11,1683534082.0,13bifht,Coming through Twitter https://twitter.com/ArcadeKodiak/,/r/aivideo/comments/13bifht/woodland_creature_festival_text_to_video_runway/,aivideo,r/aivideo,True,relevant,highly relevant
"Driving Around, text to video, runway gen-2",ZashManson,https://v.redd.it/4p2u30wu4dza1,79,11,1683862882.0,13f9ni8,Coming through Twitter https://twitter.com/ricarhartt/,/r/aivideo/comments/13f9ni8/driving_around_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Rum TV advertisement, text to video, runway gen-2",ZashManson,https://v.redd.it/dl9dz7azy35b1,78,19,1686367058.0,145p2rd,,/r/aivideo/comments/145p2rd/rum_tv_advertisement_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"This Week's Major AI developments in a nutshell (December Week 4, 2023 + January week 1, 2024)",wyem,https://www.reddit.com/r/StableDiffusion/comments/1901wfh/this_weeks_major_ai_developments_in_a_nutshell/,75,6,1704553551.0,1901wfh,"1. **Meta** and UC, Berkeley introduced ***Audio2Photoreal***, a framework for generating full-bodied photorealistic avatars with gestures driven from audio of a dyadic conversation \[[*Details*](https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/) | [*GitHub*](https://github.com/facebookresearch/audio2photoreal)*\].*
2. **MyShell** along with researchers from MIT and Tsinghua University introduced ***OpenVoice***, an open sourcce voice cloning approach that is nearly instantaneous and provides granular control of tone, from emotion to accent, rhythm, pauses, and intonation, using just a small audio clip \[[*Details*](https://research.myshell.ai/open-voice) *|* [*Hugging Face*](https://huggingface.co/spaces/myshell-ai/OpenVoice)\] .
3. **Suno** and Nvidia present ***Parakeet***, a family of open source speech recognition models that top the Open ASR Leaderboard. Parkeet models effectively prevent the generation of hallucinated transcript and are robust to noisy audio. Available for commercial use under CC BY 4.0 \[[*Details*](https://nvidia.github.io/NeMo/blogs/2024/2024-01-parakeet/) | [*Hugging Face*](https://huggingface.co/spaces/nvidia/parakeet-rnnt-1.1b)\].
4. **Researchers** from Stanford University introduce ***Mobile-ALOHA***, an open-source robot hardware that can can autonomously complete complex mobile manipulation tasks that require whole-body control like cook and serve shrimp, call and take elevator, store a 3Ibs pot to a two-door cabinet etc., with just 50 demos \[[*Details*](https://mobile-aloha.github.io/)\].
5. **Allen Institute for AI** released ***Unified-IO 2*** (open-source), the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. The model is pre-trained from scratch on an extensive variety of multimodal data -- 1 billion image-text pairs, 1 trillion text tokens, 180 million video clips, 130 million interleaved image & text, 3 million 3D assets, and 1 million agent trajectories \[[*Details*](https://unified-io-2.allenai.org/)\].
6. **Alibaba** Research introduced ***DreamTalk***, a diffusion-based audio-driven expressive talking head generation framework that can produce high-quality talking head videos across diverse speaking styles \[[*Details*](https://dreamtalk-project.github.io/) *|* [*GitHub*](https://github.com/ali-vilab/dreamtalk)\].
7. **OpenAI’s app store** for GPTs will launch next week \[[*Details*](https://techcrunch.com/2024/01/04/openais-app-store-for-gpts-will-launch-next-week/)\].
8. **GitHub Copilot Chat**, powered by GPT-4, is now generally available for both Visual Studio Code and Visual Studio, and is included in all GitHub Copilot plans alongside the original GitHub Copilot \[[*Details*](https://github.blog/2023-12-29-github-copilot-chat-now-generally-available-for-organizations-and-individuals)\].
9. **Microsoft Research** presented a new and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training step \[[*Paper*](https://arxiv.org/pdf/2401.00368.pdf)\] | [*Hugging Face*](https://huggingface.co/intfloat/e5-mistral-7b-instruct)\].
10. **Google DeepMind** introduced ***AutoRT, SARA-RT and RT-Trajectory*** to improve real-world robot data collection, speed, and generalization \[[*Details*](https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotic)\].
11. **Salesforce Research** presented ***MoonShot***, a new video generation model that conditions simultaneously on multimodal inputs of image and text, demonstrating significant improvement on visual quality and temporal consistency compared to existing models. The model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing. Models will be made public [here](https://github.com/salesforce/LAVIS) \[[*Details*](https://showlab.github.io/Moonshot/)\].
12. **Leonardo AI** released ***Leonardo Motion*** for generating videos from images. Available to all users, paid and free \[[*Link*](https://leonardo.ai/)\].
13. **JPMorgan AI Research** present ***DocLLM***, a layout-aware generative language model for multimodal document understanding. The spatial layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component \[[Details](https://arxiv.org/pdf/2401.00908.pdf)\].
14. **Alibaba Research** introduced ***Make-A-Character (Mach)***, a framework to create lifelike 3D avatars from text descriptions. Make-A-Character supports both English and Chinese prompts. \[[*Details*](https://human3daigc.github.io/MACH/) *|* [*Hugging Face*](https://huggingface.co/spaces/Human3DAIGC/Make-A-Character)\].
15. **Sony**, Canon and Nikon set to combat deepfakes with digital signature tech in future cameras \[[*Details*](https://www.techradar.com/cameras/photography/sony-canon-and-nikon-set-to-combat-deepfakes-with-digital-signature-tech-in-future-cameras)\].
16. **Meta AI** introduced ***Fairy***, a versatile and efficient video-to-video synthesis framework that generates high-quality videos with remarkable speed. Fairy generates 120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds, outpacing prior works by at least 44× \[[Details](https://fairy-video2video.github.io/)\].
17. **Apple** quietly released an open source multimodal LLM, called ***Ferret***, in October 2023 \[[*Details*](https://venturebeat.com/ai/apple-quietly-released-an-open-source-multimodal-llm-in-october/)\].
18. **Australian researchers** introduced a non-invasive AI system, called ***DeWave***, that can turn silent thoughts into text while only requiring users to wear a snug-fitting cap \[[*Details*](https://www.sciencealert.com/new-mind-reading-ai-translates-thoughts-directly-from-brainwaves-without-implants)\].
19. **Pika Labs** text-to-video AI platform **Pika 1.0** is now available to all and accessible via the web \[[*Link*](https://pika.art/)\].
20. **The New York Times** sued OpenAI and Microsoft for copyright infringement \[[*Details*](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)\].

**Source**: AI Brews newsletter- you can subscribe [here](https://aibrews.substack.com/). it's free to join, sent only once a week with ***bite-sized news, learning resources and selected tools.*** *Thank you!*",/r/StableDiffusion/comments/1901wfh/this_weeks_major_ai_developments_in_a_nutshell/,StableDiffusion,r/StableDiffusion,False,relevant,highly relevant
AI Generated music video. Text To Video Playground + Stable Diffusion.,Magentile,https://v.redd.it/sq5i8jezumza1,76,20,1683998684.0,13gnebo,,/r/AIfreakout/comments/13gnebo/ai_generated_music_video_text_to_video_playground/,AIfreakout,r/AIfreakout,True,relevant,highly relevant
Are Faceless YouTube Channels a Scam? I did some research.,papapatty11,https://www.reddit.com/r/Entrepreneur/comments/19515i2/are_faceless_youtube_channels_a_scam_i_did_some/,73,79,1705081930.0,19515i2,"Hey everyone! I post regularly about business ideas and opportunities - y'all seem to like it so I'll keep them coming! You can check out more ideas like this [here](https://brainstorms.io).

# Faceless YouTube Channels: Golden Opportunities or Risky Gambles?

Is it just my algorithm, or is there a new craze taking over social networks? This last month my X has been clogged with clickbaity titles like:

* The EASIEST Way to Earn $5K/Month from your couch
* How I Made Big Bucks with 0 Work
* Shocking Method to Earn $5K/Month – No Experience Needed

**Well, I decided to do some research.**

# What exactly is a Faceless YouTube Channel (FYC)?

It’s a video content account where the creator never shows his/her face. These accounts use avatars, images, and voice-overs to create videos on YouTube, Facebook, and TikTok.

It’s like the internet girlfriend that you only text, but in your heart, you know it’s just a dude in the Philippines 🥲

Despite what Money-Twitter wants me to believe, FYCs are nothing new. Channels like [Bright Side](https://www.youtube.com/cha?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles), [H2ODelirious](https://www.youtube.com/watch?v=Mn8Xy0RaqHw&ab_channel=H2ODelirious&utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles), and [Fail Army](https://www.youtube.com/@failarmy?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) have been cashing in on it for years.

**So why are FYCs the latest craze?**

Like most things, AI tools have made it easier than ever to spin up an FYC. You can create human-like avatars, voice-overs, content, and even photo-realist images all in a matter of seconds.

**Now, to the million-dollar question: Can you make a fortune with FYCs?**

Short answer: Yes. Long answer: Yes, but probably not.

# The Numbers Behind FYCs:

Here are our inputs (don’t worry we’ll keep it simple):

* An FYC with 10,000 subs
* The sub/view ratio of 10%
* A CPM of $2 (this means you get $2 for every 1,000 views)

**We’ll take 10,000 \* 10% \* $2 CPM =**\*...\* *drum roll* **….**

**That’s a whopping $2/video!!!** No one is buying a Lambo with those returns, but it’s a start.

That’s the ugly side of FYCs. Now, let’s look at an FYC that’s CRUSHING it.

Take [Valid Consumer](https://www.youtube.com/channel/UCPivGkDwFySKlZR_8nO8lGA) \- they’re a channel that reviews products (mostly tech) using AI images and voiceovers.

I’ll be honest, when I first looked at the channel, I cringed. It looks like a shitty review channel that pumps out low-quality videos, but let’s not write it off quite yet.

**Here are some stats:**

* Started \~1 year ago
* 60K subscribers
* Has 300+ Videos
* Get \~60,000 views/day

With their views alone, they’re probably pulling \~$100/day in ad revenue. Not bad, but still not Lambo money.

But, here’s the twist - this channel isn’t making $100/day. It’s making \~$20,000/month…. What could be their secret?!?

Say it with me… ✨**affiliate marketing**✨

Let’s take some VERY conservative numbers.

Assuming they convert .1% (1/10 of 1%) of views on their affiliate deals, and make $10 (probably more than that given their niche) on every affiliate purchase… that’s a cool $600/day.

Now that Lambo seems a little more realistic.

# How to Execute

Now that we’ve found the gravy train in this trend, here’s how to capitalize on it:

**Picking a Niche**: Affiliate marketing is the name of the game. You need to find a niche with products that are trending, have high affiliate commissions, and have endless content opportunities. Here are the niches I’d go after:

* Supplements ([great example here](https://www.tiktok.com/@peakrevivalx?_t=8iLVyGqqVR5&_r=1&utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles))
* Grooming and Skincare
* Tech Products

**TikTok Shop:** Forget YouTube. We’re going to focus on TikTok:

* It has all the attention right now
* They are pushing their shopping platform incredibly hard (I swear every other video is a product promotion)
* They are offering crazy discounts/coupons to customers and eating the cost - again all to promote their shopping platform
* It takes 60 seconds to become an affiliate for any product on TikTok Shop.

**Volume is Key**: The top accounts aren’t just posting once a day, they are posting 3-5 times a day. This might feel cringe, but that’s the hustle baby.

**Tools to use**: Here are the best tools to use to start your Faceless Channel journey:

* [Vid IQ](https://vidiq.com/?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for researching a niche
* [Copy.ai](https://Copy.ai) for script writing
* [DupliChecker](https://www.duplichecker.com/?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for plagiarism checking
* [Pictory.ai](https://pictory.ai/?ref=isah40&utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for text-to-video
* [Animaker](https://www.animaker.com/?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for animations
* [MicMonster](https://micmonster.com/?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for voiceovers
* [11ElevenLabs](https://elevenlabs.io/?utm_source=brainstorms-newsletter.beehiiv.com&utm_medium=newsletter&utm_campaign=faceless-youtube-channels-golden-opportunities-or-risky-gambles) for cloning voices

# So, should you pursue this?

**The Good:** Low startup costs, diverse revenue streams, and a golden opportunity with TikTok Shops.

**The Bad:** Creating quality content is tough… like really tough. Plus, the competition is fierce.

**The Ugly:** It's very unlikely that businesses like this are durable. They are easy to rip off and have HUGH platform risk. So, like all ‘easy-cash’, FYCs are unlikely to make you a millionaire… or even a hundred-thousandaire (is that a thing?)

That’s all for this week! Check out more ideas like this [here](https://brainstorms.io).",/r/Entrepreneur/comments/19515i2/are_faceless_youtube_channels_a_scam_i_did_some/,Entrepreneur,r/Entrepreneur,False,relevant,highly relevant
The Art of Running: Unleash Your Potential - A Pikalabs Ai Generated video (Text to Video ai),Loopnmix,https://v.redd.it/wtolkdwk1mbb1,73,41,1689200961.0,14y2to4,,/r/aivideo/comments/14y2to4/the_art_of_running_unleash_your_potential_a/,aivideo,r/aivideo,True,relevant,neutral
"Please Look Down, text to video, runway gen-2",ZashManson,https://v.redd.it/3ro25s3kwt1b1,75,4,1684937629.0,13qmwvp,Source ➡️ https://twitter.com/next_on_now/,/r/aivideo/comments/13qmwvp/please_look_down_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,highly relevant
A New NVIDIA Research Turns LDM Stable Diffusion into an Efficient and Expressive Text-to-Video Model with Resolution up to 1280 x 2048,ai-lover,https://v.redd.it/5ypieegg4gva1,73,3,1682173687.0,12v7jvz,,/r/machinelearningnews/comments/12v7jvz/a_new_nvidia_research_turns_ldm_stable_diffusion/,machinelearningnews,r/machinelearningnews,True,relevant,highly relevant
But what would Einstein think of Text to Video?,Tokyo_Jab,https://v.redd.it/b9wxyk2aympa1,76,21,1679641292.0,120c6dl,,/r/StableDiffusion/comments/120c6dl/but_what_would_einstein_think_of_text_to_video/,StableDiffusion,r/StableDiffusion,True,relevant,neutral
"Summer Jump, text to video, runway gen-2",ZashManson,https://v.redd.it/5b1ezwd3v62b1,73,7,1685094508.0,13s8lw3,Source ➡️ https://twitter.com/Merzmensch/,/r/aivideo/comments/13s8lw3/summer_jump_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Visit Lucian, text to video, runway (8min runtime)",ZashManson,https://v.redd.it/wv8zxnvvml9b1,71,9,1688324381.0,14ow5ba,,/r/aivideo/comments/14ow5ba/visit_lucian_text_to_video_runway_8min_runtime/,aivideo,r/aivideo,True,relevant,neutral
BREAKING: RUNWAY GEN-2 TEXT TO VIDEO PUBLIC RELEASE IS OUT NOW!! 🔥🔥🎉🎉🥳🥳🍾🍾🏁🏁,ZashManson,https://i.redd.it/8aigv44srm4b1.jpg,73,32,1686158790.0,143jc6q,,/r/aivideo/comments/143jc6q/breaking_runway_gen2_text_to_video_public_release/,aivideo,r/aivideo,False,not relevant,neutral
"Shades of the Abyss, text to video, runway gen-2",ZashManson,https://v.redd.it/ohbtsl0m6z3b1,73,7,1685873262.0,1408dps,,/r/aivideo/comments/1408dps/shades_of_the_abyss_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"After “real” artists and “real” writers, we now have “real” 3D modellers and game developers 🤣.",Trippy-Worlds,https://i.redd.it/40prmc5u0n4b1.jpg,73,20,1686161836.0,143kmsx,,/r/DefendingAIArt/comments/143kmsx/after_real_artists_and_real_writers_we_now_have/,DefendingAIArt,r/DefendingAIArt,False,not relevant,neutral
"Dogs Playing Poker, text to video, runway gen-2",ZashManson,https://v.redd.it/4u5bfuvtyq4b1,71,7,1686209596.0,1442inj,,/r/aivideo/comments/1442inj/dogs_playing_poker_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,highly relevant
"Genecon 2023, text to video, runway gen-2",ZashManson,https://v.redd.it/pi61jbvm36ya1,70,14,1683341893.0,1399z8v,Coming through Twitter https://twitter.com/DrewMedina20/,/r/aivideo/comments/1399z8v/genecon_2023_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
"Horror film, text to video, runway gen-2",ZashManson,https://v.redd.it/qmbo7a8vrlza1,67,15,1683967484.0,13gbzbb,Coming through Twitter https://twitter.com/ai_filmmaker/,/r/aivideo/comments/13gbzbb/horror_film_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,less relevant
Commute,DanielWinne,https://v.redd.it/cfdplruv96vb1,69,5,1697727118.0,17bl0cq,All text to video in animateDiff,/r/StableDiffusion/comments/17bl0cq/commute/,StableDiffusion,r/StableDiffusion,True,relevant,neutral
This Week in AI (Curated News),nerority,https://www.reddit.com/r/singularity/comments/1823l1a/this_week_in_ai_curated_news/,69,6,1700753223.0,1823l1a,"Hi everyone, here is a curated TLDR of the news from the past week in the field of AI developments I have been working on. Exciting week!

### OpenAI

**Rumors of Q\* Breakthrough**

* Possible references based on state of the field and rumors (credit to r/singularity)
   * Q-Search  
      * Smart decision making method, inspired from Q-Learning in reinforcement learning
   * Evidence Lower Bound (ELBO)  
      * Technique used to enhance accuracy in complex scenarios, ensuring predictions are as precise as possible
   * AlphaZero-Style “Zero” Learning  
      * Enables an AI to learn and master tasks through a “self-play” system without relying on external data.
   * Proximal Policy Optimization (PPO)  
      * Guides decision making for stable improvement
* Putting these together, you have AI systems that do not rely on external data, greatly improved approach to optimal solutions, with more precise predictions and self-guided improvement that can be trained in a “self-play” environment to reach AGI+ levels in their defined fields.

**Leadership Changes and Board Reshuffle**:  

* Sam Altman has returned as CEO of OpenAI after a brief period of ouster, accompanied by significant changes in the board. The new initial board includes former Salesforce co-CEO Bret Taylor as chair, and Larry Summers, former U.S. Treasury Secretary, and Adam D'Angelo as directors.
* During Altman's brief departure, Emmett Shear, ex-Twitch boss, was named interim CEO. Altman's return was partly facilitated by Microsoft, as CEO Satya Nadella offered him to head a new research team alongside Greg Brockman and other colleagues departing from OpenAI.

**First Developer Event and Announcements**:  

* OpenAI held its first developer event, announcing various new products and updates:  
   * Launch of GPT-4 Turbo, an improved version of GPT-4, with enhanced text and image understanding capabilities.
   * Introduction of an Assistants API, allowing developers to build their own agent-like experiences.
   * DALL-E 3 API release with in-built moderation tools.
   * A new text-to-speech API featuring six preset voices.
   * The introduction of Copyright Shield, a program to protect businesses from copyright claims related to content created by OpenAI’s tools.
   * Other significant updates, such as the removal of the model picker in ChatGPT and the launch of Whisper large v3, an open-sourced speech recognition mode.

**Commercial and Ethical Considerations**:  

* OpenAI, overseen by a nonprofit parent board, aims to prioritize AI safety alongside growth. Altman's return is expected to push OpenAI towards more bold and profit-focused strategies, while also potentially making it less risk-averse.

### Anthropic

1. **Launch of Claude 2.1**: Amidst the backdrop of these developments, Anthropic has launched Claude 2.1, the latest version of its large language model. This model boasts a 200K token context and is claimed to produce 50% fewer false claims compared to its predecessors. It also offers tool integration capabilities for enterprises seeking stable AI alternatives.
2. **Potential Merger with OpenAI**: OpenAI's board of directors approached Dario Amodei, the co-founder and CEO of Anthropic, to discuss a potential merger between OpenAI and Anthropic.
3. **Google's Investment in Anthropic**: Alphabet’s Google has agreed to invest up to $2 billion in Anthropic. This investment adds significant momentum to the AI race, with Google having already made an upfront investment of $500 million.

### Inflection AI

1. **Development of Inflection-2**: Suleyman announced the completion of training for Inflection-2, claiming it to be the ""2nd best LLM in the world"" and indicating plans for significant scaling.
2. **Mustafa Suleyman's Invitation to OpenAI Staff**: Inflection AI's co-founder and CEO, Mustafa Suleyman, extended an invitation to OpenAI staff to join his firm following an eventful weekend at OpenAI. He used this as an opportunity to promote Inflection AI and its progress.
3. **Launch of Chatbot Pi**: Inflection AI launched its chatbot Pi in May, designed as a personal assistant capable of providing emotional support and advice. This move was seen as a direct competition to OpenAI's ChatGPT.
4. **Backing and Valuation**: Inflection AI has received backing from tech heavyweights like ex-Google CEO Eric Schmidt, Microsoft co-founder Bill Gates, and Nvidia. The company raised $1.3 billion at a $4 billion valuation in June. Suleyman, who also co-founded DeepMind before its acquisition by Google, co-founded Inflection AI in 2022 alongside Reid Hoffman, a member of the ""PayPal Mafia"" and LinkedIn co-founder.

### Stability AI

1. **Stable Video Diffusion Announcement**: Stability AI has announced Stable Video Diffusion, a new AI model that generates videos by animating existing images. This model is one of the few video-generating models available both in open source and commercially, and is currently in a ""research preview"" stage.
2. **Features and Limitations of Stable Video Diffusion**: The Stable Video Diffusion model, which comes in two forms, SVD and SVD-XT, can generate high-quality four-second clips. However, it has limitations, such as not being able to generate videos without motion or slow camera pans, control videos by text, render text legibly, or consistently generate faces and people properly.
3. **Future Plans**: Stability AI plans to develop a variety of models that build on and extend SVD and SVD-XT, including a ""text-to-video"" tool for web applications. The ultimate goal appears to be commercialization, with potential applications in advertising, education, entertainment, and more.

## Notable Papers

GAIA: A Benchmark for General AI Assistants

[https://arxiv.org/pdf/2311.12983.pdf](https://arxiv.org/pdf/2311.12983.pdf)

Orca 2: Teaching Small Language Models How to Reason

[https://arxiv.org/pdf/2311.11045.pdf](https://arxiv.org/pdf/2311.11045.pdf)

Levels Of AGI - Operationalizing Progress On The Path To Agi

[https://arxiv.org/pdf/2311.02462.pdf](https://arxiv.org/pdf/2311.02462.pdf)

Are Large Language Models Temporally Grounded?

[https://arxiv.org/pdf/2311.08398.pdf](https://arxiv.org/pdf/2311.08398.pdf)",/r/singularity/comments/1823l1a/this_week_in_ai_curated_news/,singularity,r/singularity,False,relevant,highly relevant
What are some expectations people have in this sub that are just way too unrealistic?,ResponsiveSignature,https://www.reddit.com/r/singularity/comments/16u35j8/what_are_some_expectations_people_have_in_this/,65,150,1695866024.0,16u35j8,"Keep in mind this is for all expectations people have over the next year or two. I have no idea how things will progress if some company cracks getting AGI working.

**GPT-4+ level models running locally**

Some people are *very* obsessed with extreme miniaturization and localization of models. There seems to be this recurring dream of GPT-4 running locally on one's phone with no restrictions. Why people care specifically about it being on a phone rather than a computer or local server with low latency, I have no idea, but either way, the benefits of releasing and designing a model are far lower than the expenses, risk and overhead. Current models are as versatile as they are because of their enormous memory requirements, something not even the latest iPhone could even barely touch even on a quantized highly fine-tuned model.  All the best models are going to remain big or behind a closed source API for the foreseeable future.

**Custom full-length movies from a single prompt**

For all the interesting possibilities of entertainment available due to advances in AI, people seem to be stuck on this somewhat trite capability, essentially being able to prompt and get a Hollywood movie the same way current image models work.

Even with the advances in text to video (Runway V2 and such), we are still a long way away from developing the capabilities to match the current expectations people have for feature-length films via AI. I think it's reasonable to assume an LLM could write syndicated TV-show level script soon, but turning that into a film with consistent 3D continuity, motion, sense of space and time, etc. is something only an AGI level AI could accomplish at this point.

**State of the art language/art models with no censorship whatsoever**

Everyone thinks that it's pure moral prudishness that keeps OpenAI from allowing people to use their models uncensored for whatever purposes users want. They imagine that since there is a huge natural demand for uncensored image/language models that all a company would have to do is release one with no restrictions and they'll eat up most of the market.

Most companies that want to be venture-backed are not going to risk going down the NSFW route to appease a niche market. The risk of controversy, payment issues, and abuse is far too great. It's not worth it to appeal to a market driven solely by human libido.",/r/singularity/comments/16u35j8/what_are_some_expectations_people_have_in_this/,singularity,r/singularity,False,relevant,highly relevant
"World in the year 3023, text to video, runway gen-2",ZashManson,https://v.redd.it/sbdn7ljf20xa1,68,13,1682832994.0,133guxb,Coming through Twitter https://twitter.com/alexgnewmedia,/r/aivideo/comments/133guxb/world_in_the_year_3023_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
A terrible day for luddites. But a great one for literally everyone else!,Present_Dimension464,https://i.redd.it/n21jpfc1htic1.png,66,51,1708032350.0,1arqp3i,,/r/DefendingAIArt/comments/1arqp3i/a_terrible_day_for_luddites_but_a_great_one_for/,DefendingAIArt,r/DefendingAIArt,False,not relevant,highly relevant
"Rainbowcore and Teddycore, text to video, runway gen-2",ZashManson,https://v.redd.it/4g42gof20n2b1,69,10,1685290003.0,13u4296,Source ➡️ https://twitter.com/stevemills/,/r/aivideo/comments/13u4296/rainbowcore_and_teddycore_text_to_video_runway/,aivideo,r/aivideo,True,relevant,neutral
This week in AI - all the Major AI developments in a nutshell,wyem,https://www.reddit.com/r/artificial/comments/1ase382/this_week_in_ai_all_the_major_ai_developments_in/,67,16,1708104050.0,1ase382,"1. **Meta AI** introduces ***V-JEPA*** (Video Joint Embedding Predictive Architecture), a method for teaching machines to understand and model the physical world by watching videos. Meta AI releases a collection of V-JEPA vision models trained with a feature prediction objective using self-supervised learning. The models are able to understand and predict what is going on in a video, even with limited information \[[*Details*](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) | [*GitHub*](https://github.com/facebookresearch/jepa)\].
2. **Open AI** introduces ***Sora***, a text-to-video model that can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions \[[*Details + sample videos*](https://openai.com/sora)[ ](https://openai.com/sora)| [*Report*](https://openai.com/research/video-generation-models-as-world-simulators)\].
3. **Google** announces their next-generation model, **Gemini 1.5,** that uses a new [Mixture-of-Experts](https://arxiv.org/abs/1701.06538) (MoE) architecture. The first Gemini 1.5 model being released for early testing is ***Gemini 1.5 Pro*** with a context window of up to 1 million tokens, which is the longest context window of any large-scale foundation model yet. 1.5 Pro can perform sophisticated understanding and reasoning tasks for different modalities, including video and it performs at a similar level to 1.0 Ultra \[[*Details*](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15) *|*[*Tech Report*](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\].
4. Reka introduced **Reka Flash,** a new 21B multimodal and multilingual model trained entirely from scratch that is competitive with Gemini Pro & GPT 3.5 on key language & vision benchmarks. Reka also present a compact variant Reka Edge , a smaller and more efficient model (7B) suitable for local and on-device deployment. Both models are in public beta and available in [**Reka Playground** ](https://chat.reka.ai/chat)\[[*Details*](https://reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model)\].
5. **Cohere** For AI released ***Aya***, a new open-source, massively multilingual LLM & dataset to help support under-represented languages. Aya outperforms existing open-source models and covers 101 different languages – more than double covered by previous models \[[*Details*](https://cohere.com/research/aya)\].
6. **BAAI** released ***Bunny***, a family of lightweight but powerful multimodal models. Bunny-3B model built upon SigLIP and Phi-2 outperforms the state-of-the-art MLLMs, not only in comparison with models of similar size but also against larger MLLMs (7B), and even achieves performance on par with LLaVA-13B \[[*Details*](https://github.com/BAAI-DCAI/Bunny)\].
7. **Amazon** introduced a text-to-speech (TTS) model called ***BASE TTS*** (Big Adaptive Streamable TTS with Emergent abilities). BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data and exhibits “emergent” qualities improving its ability to speak even complex sentences naturally \[[*Details*](https://techcrunch.com/2024/02/14/largest-text-to-speech-ai-model-yet-shows-emergent-abilities/) | [*Paper*](https://assets.amazon.science/6e/82/1d037a4243c9a6cf4169895482d5/base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data.pdf)\].
8. **Stability AI** released ***Stable Cascade*** in research preview, a new text to image model that is exceptionally easy to train and finetune on consumer hardware due to its three-stage architecture. Stable Cascade can also generate image variations and image-to-image generations. In addition to providing checkpoints and inference scripts, Stability AI has also released scripts for finetuning, ControlNet, and LoRA training \[[*Details*](https://stability.ai/news/introducing-stable-cascade)\].
9. **Researchers** from UC berkeley released ***Large World Model (LWM)***, an open-source general-purpose large-context multimodal autoregressive model, trained from LLaMA-2, that can perform language, image, and video understanding and generation. LWM answers questions about 1 hour long YouTube video even if GPT-4V and Gemini Pro both fail and can retriev facts across 1M context with high accuracy \[[*Details*](https://largeworldmodel.github.io/)\].
10. **GitHub** opens applications for the next cohort of ***GitHub Accelerator program*** with a focus on funding the people and projects that are building ***AI-based solutions*** under an open source license \[[*Details*](https://github.blog/2024-02-13-powering-advancements-of-ai-in-the-open-apply-now-to-github-accelerator)\].
11. **NVIDIA** released ***Chat with RTX***, a locally running (Windows PCs with specific NVIDIA GPUs) AI assistant that integrates with your file system and lets you chat with your notes, documents, and videos using open source models \[[*Details*](https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai)\].
12. **Open AI** is testing ***memory with ChatGPT***, enabling it to remember things you discuss across all chats. ChatGPT's memories evolve with your interactions and aren't linked to specific conversations. It is being rolled out to a small portion of ChatGPT free and Plus users this week \[[*Details*](https://openai.com/blog/memory-and-new-controls-for-chatgpt)\].
13. **BCG X** released of ***AgentKit***, a LangChain-based starter kit (NextJS, FastAPI) to build constrained agent applications \[[*Details*](https://blog.langchain.dev/bcg-x-releases-agentkit-a-full-stack-starter-kit-for-building-constrained-agents/) | [*GitHub*](https://github.com/BCG-X-Official/agentkit)\].
14. **Elevenalabs**' Speech to Speech feature, launched in November, for voice transformation with control over emotions and delivery, is now ***multilingual*** and available in 29 languages \[[*Link*](https://elevenlabs.io/voice-changer)\]
15. **Apple** introduced ***Keyframer***, an LLM-powered animation prototyping tool that can generate animations from static images (SVGs). Users can iterate on their design by adding prompts and editing LLM-generated CSS animation code or properties \[[*Paper*](https://arxiv.org/pdf/2402.06071.pdf)\].
16. **Eleven Labs** launched a ***payout program*** for voice actors to earn rewards every time their voice clone is used \[[*Details*](https://elevenlabs.io/voice-actors)\].
17. **Azure OpenAI Service** announced Assistants API, new models for finetuning, new text-to-speech model and new generation of embeddings models with lower pricing \[[*Details*](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-openai-service-announces-assistants-api-new-models-for/ba-p/4049940)\].
18. **Brilliant Labs**, the developer of AI glasses, launched ***Frame***, the world’s first glasses featuring an integrated AI assistant, ***Noa***. Powered by an integrated multimodal generative AI system capable of running GPT4, Stability AI, and the Whisper AI model simultaneously, Noa performs real-world visual processing, novel image generation, and real-time speech recognition and translation. \[[*Details*](https://venturebeat.com/games/brilliant-labss-frame-glasses-serve-as-multimodal-ai-assistant/)\].
19. **Nous Research** released ***Nous Hermes 2 Llama-2 70B*** model trained on the Nous Hermes 2 dataset, with over 1,000,000 entries of primarily synthetic data \[[*Details*](https://huggingface.co/NousResearch/Nous-Hermes-2-Llama-2-70B)\].
20. **Open AI** in partnership with Microsoft Threat Intelligence, have disrupted five state-affiliated actors that sought to use AI services in support of malicious cyber activities \[[*Details*](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors)\]
21. **Perplexity** partners with **Vercel**, opening AI search to developer apps \[[*Details*](https://venturebeat.com/ai/perplexity-partners-with-vercel-opening-ai-search-to-developer-apps/)\].
22. **Researchers** show that ***LLM agents can autonomously hack websites***, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. The agent does not need to know the vulnerability beforehand \[[*Paper*](https://arxiv.org/html/2402.06664v1)\].
23. **FCC** makes AI-generated voices in unsolicited robocalls illegal \[[*Link*](https://www.msn.com/en-us/money/companies/fcc-bans-ai-voices-in-unsolicited-robocalls/ar-BB1hZoZ0)\].
24. **Slack** adds AI-powered search and summarization to the platform for enterprise plans \[[*Details*](https://techcrunch.com/2024/02/14/slack-brings-ai-fueled-search-and-summarization-to-the-platform/)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks.",/r/artificial/comments/1ase382/this_week_in_ai_all_the_major_ai_developments_in/,artificial,r/artificial,False,relevant,highly relevant
Text-to-video is improving rapidly : Runway Gen-2,Mission-Length7704,https://twitter.com/runwayml/status/1684921783617482752?t=XHP9r_EccnONCUfxYoP3Eg&s=19,65,15,1690639426.0,15ct4lq,,/r/singularity/comments/15ct4lq/texttovideo_is_improving_rapidly_runway_gen2/,singularity,r/singularity,False,not relevant,highly relevant
StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter,ninjasaid13,https://i.redd.it/3pkfbhoky64c1.gif,66,5,1701657906.0,18aaerv,,/r/StableDiffusion/comments/18aaerv/stylecrafter_enhancing_stylized_texttovideo/,StableDiffusion,r/StableDiffusion,False,not relevant,neutral
Google AI recently gave 60 Minutes a preview of their text-to-video model Phenaki.,Educational_Ice151,https://v.redd.it/r2v9z6nr16ua1,66,26,1681597888.0,12nl7mu,,/r/aipromptprogramming/comments/12nl7mu/google_ai_recently_gave_60_minutes_a_preview_of/,aipromptprogramming,r/aipromptprogramming,True,not relevant,neutral
"A Recap of a Week's Noteworthy AI Developments | Nvidia Text-To-Video, Blockade Labs, Zip NeRF, AI Sony Award, and many more.",xutw21,https://www.youtube.com/watch?v=E2aZiejw-8A,65,8,1682011005.0,12t8f7l,,/r/singularity/comments/12t8f7l/a_recap_of_a_weeks_noteworthy_ai_developments/,singularity,r/singularity,False,not relevant,highly relevant
FOSS (Free and Open Source Software) Applications I use on my Android Devices,marinluv,https://www.reddit.com/r/IndiaTech/comments/182shyt/foss_free_and_open_source_software_applications_i/,65,22,1700833756.0,182shyt,"I made [this](https://www.reddit.com/r/IndiaTech/comments/17zixyc/foss_free_and_open_source_software_softwares_i/?utm_source=share&utm_medium=web2x&context=3) post a few days about FOSS apps for windows. I am making this post about Android FOSS apps I use. Furthermore, I don't consume media on my phone, nor I use my phone to download/torrent, so, applications related to that won't be included, but you can ask about FOSS alternative for such use case and I will reply. These are the apps I use extensively or occasionally, but they are on my device (Pixel 6). 

\- [**Aegis**](https://github.com/beemdevelopment/Aegis) \- For Two Factor Authentication

\- [**alibi**](https://github.com/SeldonIO/alibi) \- Use your phone as a dashcam. Very helpful if you don't have a dashcam or hate dashcam quality.

\- [**Audile**](https://github.com/aleksey-saenko/MusicRecognizer) \- App to recognize the song playing around your phone and save that song in the database for future reference. Similar to Sahzam but open source.

\- [**AntennaPod**](https://github.com/AntennaPod/AntennaPod) \- Hands down best podcast manager for Android.

\- [**BlackHole**](https://github.com/Sangwan5688/BlackHole) \- Tired of paying for Spotify or YTMusic? Try this open source app, AFAIK it sources music from Savaan and YouTube but nevertheless, it works great, UI is very interactive and simple, and works well with Android Auto.

\- [**birday**](https://github.com/m-i-n-a-r/birday) \- Never knew I needed this app but I can't live without this app now. It's a very small app (3mb in size) which let's you add Birthdays of your friends and loved ones and it will remind you as per your notification settings. I know there's Google Calender, but as I don't use Google services, this app is so good with its beautiful UI and it just works.

\- [**ChangeDetection**](https://github.com/bernaferrari/ChangeDetection) \- App to track websites changes. Very useful for in some use case.

\- [**destiny**](https://github.com/LeastAuthority/destiny) \- E2E app to share files across your device securely

\- [**Droid-ify**](https://github.com/Droid-ify/client) **-** F-droid (mentioned below about it) fork with material UI and looks modern.

\-   [**exif-eraser**](https://github.com/Tommy-Geenexus/exif-eraser)  \- Permissionless image metadata erasing application. You can do this with Signal Messenger too as Signal strips off all metadata but this is also a neat small tool to do the same.

\- [**exodus**](https://github.com/Exodus-Privacy/exodus) \-  **εxodus** is a privacy auditing platform for Android  applications. It detects behaviors which can be dangerous for user  privacy like ads, tracking, analytics, … 

\- [**FakeStandby**](https://github.com/JonasBernard/FakeStandby) \- Rarely used but still have it on my Pixel. It turns the screen off while everything works on the phone. Might be useful for people who download big games.

\- [**FairEmail**](https://github.com/M66B/FairEmail) \- An encrypted, feature rich, email client. Other than Proton, you can add Yahoo, Gmail, Hotmail, Outlook, here and it's just one email app to check mails on those platforms securely.

\- [**F-droid**](https://github.com/f-droid/fdroidclient) \- You fill find majority of the apps mentioned here and hundreds of many FOSS apps on F-droid. Also, refer to this [F-droid forum post](https://forum.f-droid.org/t/known-repositories/721) where they have mentioned repositories. You can add these repositories to get more FOSS app in your F-droid client. Recommended ones are - IzzyOnDroid and  The Guardian Project.

\- [**ffshare**](https://github.com/caydey/ffshare) \- Awseome app to compress images and videos while sharing them over email or normally. It uses FFmpeg to convert which is another open source library MANY other software and apps use like YT-DLP

\- [**florisboard**](https://github.com/florisboard/florisboard) \- Open source keyboard for Android. I know it's not perfect and it would feel like a downgrade to some who are coming from Gboard but if you want privacy then this is the one to go for.

\- [**FoodExpirationDates**](https://github.com/lorenzovngl/FoodExpirationDates) \- Another sweet app to keep track of your food items expire dates.

\- [**Imagepipe**](https://codeberg.org/Starfish/Imagepipe) \-  To reduce image size similar to ffshare but it also removes exif data similar to exif eraser and have watermark feature in one app.

\- [**Infinity-For-Reddit**](https://github.com/Docile-Alligator/Infinity-For-Reddit) \- The third party Reddit client I love and use. I know Reddit has made it's API restricted but Infinity still works if you pay some amount to the dev so he could pay for the API. Miles better than the official Reddit app, buttery smooth, no ads, no trackers, customizable, and fast AF.

\- [**Insular**](https://github.com/proletarius101/Insular) \- Clone app, freeze main apps, and much more power user control

\- [**InviZible**](https://github.com/Gedsh/InviZible) \- A very good app to have control over network for privacy and security.

\- [**KDE Connect**](https://kdeconnect.kde.org/) \- The only app you need to connect between your devices like Android, Linux, Windows and share anything between them and communicate between them.

\- [**KeePassDX**](https://github.com/Kunzisoft/KeePassDX) \- As mentioned in my last post, I am a keepass user so I use KeePassDX client on Android to access my keepass password database.

\- [**Kuroba-Experimental**](https://github.com/K1rakishou/Kuroba-Experimental) **-** Open source image board browser to browse boards like 4chan

\- [**MaterialFiles**](https://github.com/zhanghai/MaterialFiles) \- File manager

\- [**NewPipe**](https://github.com/TeamNewPipe/NewPipe) \- The best front-end for streaming (YouTubeXD) on Android. No ads, no login, no complicated UI. A simple, lightweight, streaming client (YouTube XD) which has no ads even during streaming.

\- [**notificationlog-android**](https://codeberg.org/jonas-l/notificationlog-android) \- To log your all notifications. Extremely useful.

\- [**Obtainium**](https://github.com/ImranR98/Obtainium) \- You can update your FOSS apps directly from the source i.e GitHub or GitLab without going through F-Droid route and some apps are not even on F-droid, so with Obtainium you can update them easily.

\- [**OpenBoard**](https://github.com/OpenBoard-org/OpenBoard) \- Another FOSS keyboard similar to Florisboard. Try both and see which one is for you.

\- [**OpenCalc**](https://github.com/Darkempire78/OpenCalc) \- A simple open source calculator app

\- [**PixelKnot**](https://github.com/guardianproject/PixelKnot) \- This app is part of The Guardian Project (the repo I mentioned above to be added on F-droid client). This app used steganography to hide text in image. I use [**stegify**](https://github.com/DimitarPetrov/stegify) on my Linux to add text to videos and images but to have the same functionality on an Android app is amazing.

\- [**squawker**](https://github.com/j-fbriere/squawker) \- An open-source anonymous Twitter/X client. Don't need an account to subscribe or follow feeds.

\- [**QP-Gallery**](https://github.com/WSTxda/QP-Gallery-Releases) \- Quickpic gallery app modded version. The official app is broken for most Android versiob but it's the best gallery app I have ever come across. Thankfully, the community modded it and released the FOSS version which is regularly updated and maintained. Use Obtainium to get the updates as this is not on F-droid. 

\- [**QuickTiles**](https://github.com/asdoi/QuickTiles) \- Add quick tiles settings easily.

\- [**rethink-app**](https://github.com/celzero/rethink-app) \- DNS over HTTPS / DNS over Tor / DNSCrypt client, WireGuard proxifier, firewall, and connection tracker  all in one app.

\- [**SaikouTV**](https://github.com/kemuriXD/SaikouTV) \-  stream & download Anime & Manga - no ads and no trackers (rarely use as I don't consume media on my phone)

\- [**savertuner**](https://codeberg.org/s1m/savertuner) \- Extension to the built-in Android Battery Saver 

\- [**ScreenCam**](https://github.com/DevipriyaSarkar/ScreenCam) \- Hands down the BEST screen recording app

\- [**Seal**](https://github.com/JunkFood02/Seal) \- YT-DLP but on Android. 

\- [**SecScanQR**](https://github.com/Fr4gorSoftware/SecScanQR) \- Privacy centric QR code scanner and generator

\- [**sharik**](https://github.com/marchellodev/sharik) \- An open-source, cross-platform solution for sharing files via Wi-Fi or Mobile Hotspot

\- [**Shelter**](https://gitea.angry.im/PeterCxy/Shelter) \- Shelter is a Free and Open-Source (FOSS) app that leverages the ""Work  Profile"" feature of Android to provide an isolated space that you can  install or clone apps into.

\- [**SuperFreeZ**](https://gitlab.com/SuperFreezZ/SuperFreezZ) \-  Android app to completely stop all background activities of other apps. 

\- [**syncthing-android**](https://github.com/syncthing/syncthing-android) \- As I mentioned in my last post, I use Syncthing to sync and backup data across my devices, so, this is a must app for me.

\- [**tachiyomi**](https://github.com/tachiyomiorg/tachiyomi) \- Manga reader app. As mentioned on my earlier post, I use [Kavita](https://github.com/Kareadita/Kavita) to manage my collection of manga and my Kavita server is connected with Tachiyomi (with Kavita extensinon), so, I have seamless experience of reading manga with Tachiyomi through my own server.   

\- [**tracker-control-android**](https://github.com/TrackerControl/tracker-control-android) \- Monitor and control trackers and ads. Must have. 

\- [**TranslateYou**](https://github.com/you-apps/TranslateYou) \- Uninstalled Google translate. Use this app instead. Privacy focused translator app.

\- [**TxtNet-Browser**](https://github.com/lukeaschenbrenner/TxtNet-Browser) \- An app that lets you browse the web over SMS

\- [**UrlChecker**](https://github.com/TrianguloY/UrlChecker) \- **EVERYONE SHOULD CHECK URLS BEFORE CLICKING ON THEM. URLCheck** acts as an amazingly customizable and powerful  intermediary when opening url links, allowing, among other things: to remove trackers, affiliate links, unnecessary elements, check Hosts, facilitating link holding and sharing, protecting against certain phishing techniques and many more... 

\- [**Warden**](https://apt.izzysoft.de/fdroid/index/apk/com.aurora.warden) \- Warden helps you to keep trackers and loggers at bay, as well as getting rid of bloatware. MUST HAVE. Available on Izzy, so, add Izzy repo on F-droid.

\- [**WarpShare**](https://github.com/moseoridev/WarpShare) \- Use airdrop feature in Android

\- [**wireguard-android**](https://github.com/WireGuard/wireguard-android) \- I prefer WireGuard over OpenVPN",/r/IndiaTech/comments/182shyt/foss_free_and_open_source_software_applications_i/,IndiaTech,r/IndiaTech,False,relevant,highly relevant
"Thank you for not answering, text to video, runway gen-2",ZashManson,https://v.redd.it/7z9p7h0ahnxa1,65,9,1683116460.0,136iyme,Coming through Twitter https://twitter.com/paultrillo,/r/aivideo/comments/136iyme/thank_you_for_not_answering_text_to_video_runway/,aivideo,r/aivideo,True,relevant,highly relevant
"Classic Children’s Tale, text to video, runway gen-2",ZashManson,https://v.redd.it/kh8p44yawj4b1,61,11,1686124015.0,1436r91,,/r/aivideo/comments/1436r91/classic_childrens_tale_text_to_video_runway_gen2/,aivideo,r/aivideo,True,relevant,neutral
Text to Video Generator Using AI,BadExtension1309,https://www.linkedin.com/posts/santoshvp_ai-machinelearning-deeplearning-activity-7067483078662770688-exqA?utm_source=share&utm_medium=member_desktop,1,0,1685019740.0,13rh7us,,/r/machinelearningnews/comments/13rh7us/text_to_video_generator_using_ai/,machinelearningnews,r/machinelearningnews,False,relevant,neutral
Meta Unveils Emu Video and Emu Edit: Pioneering Advances in Text-to-Video Generation and Precision Image Editing,ai-lover,https://i.redd.it/fby03tm6a91c1.gif,7,1,1700378476.0,17yrlye,,/r/machinelearningnews/comments/17yrlye/meta_unveils_emu_video_and_emu_edit_pioneering/,machinelearningnews,r/machinelearningnews,False,not relevant,neutral
This AI Research Introduces Two Diffusion Models for High-Quality Video Generation: Text-to-Video (T2V) and Image-to-Video (I2V) Models,ai-lover,https://i.redd.it/tckv49nfr4zb1.gif,24,2,1699452045.0,17qm9d0,,/r/machinelearningnews/comments/17qm9d0/this_ai_research_introduces_two_diffusion_models/,machinelearningnews,r/machinelearningnews,False,not relevant,neutral
Can Large Language Models Revolutionize Multi-Scene Video Generation? Meet VideoDirectorGPT: The Future of Dynamic Text-to-Video Creation,ai-lover,https://v.redd.it/6kz1tfydoasb1,10,2,1696472732.0,17067pi,,/r/machinelearningnews/comments/17067pi/can_large_language_models_revolutionize/,machinelearningnews,r/machinelearningnews,True,not relevant,highly relevant
Baidu AI Researchers Introduce VideoGen: A New Text-to-Video Generation Approach That Can Generate High-Definition Video With High Frame Fidelity,ai-lover,https://v.redd.it/ug8cyl0hhunb1,13,1,1694533311.0,16guoxd,,/r/machinelearningnews/comments/16guoxd/baidu_ai_researchers_introduce_videogen_a_new/,machinelearningnews,r/machinelearningnews,True,not relevant,neutral
Enhancing Task-Specific Adaptation for Video Foundation Models: Introducing Video Adapter as a Probabilistic Framework for Adapting Text-to-Video Models,ai-lover,https://v.redd.it/rxfiwtokk85b1,3,1,1686422727.0,1467sy2,,/r/machinelearningnews/comments/1467sy2/enhancing_taskspecific_adaptation_for_video/,machinelearningnews,r/machinelearningnews,True,not relevant,neutral
Researchers from the National University of Singapore propose Show-1: A Hybrid Artificial Intelligence Model that Marries Pixel-Based and Latent-Based VDMs for Text-to-Video Generation,ai-lover,https://i.redd.it/wqevepnf64vb1.gif,2,1,1697701623.0,17bdpa6,,/r/machinelearningnews/comments/17bdpa6/researchers_from_the_national_university_of/,machinelearningnews,r/machinelearningnews,False,not relevant,highly relevant
This AI Paper Presents Video Language Planning (VLP): A Novel Artificial Intelligence Approach that Consists of a Tree Search Procedure with Vision-Language Models and Text-to-Video Dynamics,ai-lover,https://v.redd.it/ao8isntowiwb1,9,1,1698315794.0,17gtbjw,,/r/machinelearningnews/comments/17gtbjw/this_ai_paper_presents_video_language_planning/,machinelearningnews,r/machinelearningnews,True,not relevant,highly relevant
Meet Generative Disco: A Generative AI System That Facilitates Text-To-Video Generation For Music Visualization Using A Large Language Model And A Text-To-Image Model,ai-lover,https://i.redd.it/9x5xqtx1e8wa1.png,20,1,1682515915.0,12zhoja,,/r/machinelearningnews/comments/12zhoja/meet_generative_disco_a_generative_ai_system_that/,machinelearningnews,r/machinelearningnews,False,not relevant,neutral
CMU Researchers Propose STF (Sketching the Future): A New AI Approach that Combines Zero-Shot Text-to-Video Generation with ControlNet to Improve the Output of these Models,ai-lover,https://v.redd.it/jticngypaj1b1,29,1,1684827340.0,13phh4c,,/r/machinelearningnews/comments/13phh4c/cmu_researchers_propose_stf_sketching_the_future/,machinelearningnews,r/machinelearningnews,True,relevant,highly relevant
